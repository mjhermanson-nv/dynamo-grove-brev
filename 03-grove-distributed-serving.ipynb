{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8762f4a3",
   "metadata": {},
   "source": [
    "# Lab 3: Distributed Dynamo with Multi-GPU/Multi-Node Serving\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you will:\n",
    "- Deploy distributed Dynamo across multiple GPUs or nodes\n",
    "- Understand multi-GPU and multi-node serving architectures\n",
    "- Enable distributed KV cache awareness and transfer via NIXL\n",
    "- Monitor distributed components with Grafana\n",
    "- Learn when to add optional infrastructure (NATS/etcd) for large-scale deployments\n",
    "\n",
    "**Prerequisites**: Complete Lab 1 (Dynamo Deployment) and Lab 2 (Monitoring)\n",
    "\n",
    "**What You'll Learn:**\n",
    "- How Dynamo discovers and coordinates workers across GPUs/nodes\n",
    "- KV cache sharing between workers for improved performance\n",
    "- Two deployment modes: Standard (built-in) vs Advanced (NATS/etcd)\n",
    "- Monitoring distributed inference metrics\n",
    "\n",
    "**Note**: Distributed Dynamo is designed for multi-node Kubernetes clusters or single nodes with multiple GPUs. While we'll deploy it on a single node for learning purposes, maximum benefits are realized when scaling across multiple nodes with high cache hit workloads.\n",
    "\n",
    "## Duration: ~45 minutes (standard path) / ~75 minutes (with optional NATS/etcd)\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Understanding Distributed Dynamo Architecture\n",
    "\n",
    "### What is Grove vs Dynamo?\n",
    "\n",
    "**Dynamo** is NVIDIA's inference serving framework (the Python code, Router, Frontend, Workers).\n",
    "\n",
    "**Grove** is the Kubernetes Operator that orchestrates Dynamo deployments (handling CRDs like `DynamoGraphDeployment`, pod gangs, startup order).\n",
    "\n",
    "**Distributed Dynamo** (orchestrated by Grove) enables:\n",
    "- **Multi-node deployments** across Kubernetes clusters or multi-GPU single nodes\n",
    "- **KV-aware routing** where the Router knows which worker has which cache blocks\n",
    "- **Distributed KV cache transfer** between workers via NIXL (NVIDIA Inference Transfer Library)\n",
    "- **Coordination and discovery** using either:\n",
    "  - **K8s-native (v0.8.0+)**: EndpointSlices + TCP (simpler, recommended for most use cases)\n",
    "  - **NATS/etcd (optional)**: For extreme scale (very large clusters, multi-region, complex topologies)\n",
    "\n",
    "### Architecture: K8s-Native (Recommended for Most Users)\n",
    "\n",
    "```\n",
    "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "               â”‚  Cloud Load Balancer       â”‚\n",
    "               â”‚  or Ingress Controller     â”‚\n",
    "               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚                â”‚\n",
    "    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "    â”‚Frontend 1â”‚    â”‚Frontend 2â”‚    â”‚Frontend 3â”‚\n",
    "    â”‚ (Node 1) â”‚    â”‚ (Node 2) â”‚    â”‚ (Node 3) â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                â”‚                â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚ Kubernetes            â”‚\n",
    "              â”‚  - EndpointSlices     â”‚\n",
    "              â”‚    (Discovery)        â”‚\n",
    "              â”‚  - TCP (Transport)    â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚                â”‚\n",
    "    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Worker 1 â”‚    â”‚ Worker 2 â”‚    â”‚ Worker 3 â”‚\n",
    "    â”‚ (Node 4) â”‚    â”‚ (Node 5) â”‚    â”‚ (Node 6) â”‚\n",
    "    â”‚  +GPU    â”‚    â”‚  +GPU    â”‚    â”‚  +GPU    â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                â”‚                â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  NIXL (KV Cache       â”‚\n",
    "              â”‚   Data Transfer)      â”‚\n",
    "              â”‚  RDMA/TCP/SSD         â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Benefits:\n",
    "- Built-in: Uses Kubernetes service discovery (no extra components)\n",
    "- Fast: Direct TCP connections between components\n",
    "- Reliable: Fewer moving parts to maintain\n",
    "- Recommended for most deployments\n",
    "\n",
    "Note: Workers require GPU nodes. Frontends don't require GPUs\n",
    "      and can run on any node. The diagram shows them on separate\n",
    "      nodes, but they can share nodes in smaller clusters.\n",
    "\n",
    "**For NATS/etcd architecture (extreme scale deployments)**, see Appendix A.\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Kubernetes-native Discovery (v0.8.0+)**: Built-in service discovery:\n",
    "- Uses EndpointSlices (standard Kubernetes API)\n",
    "- Workers register with K8s API server automatically\n",
    "- Frontends watch EndpointSlices for worker availability\n",
    "- No additional infrastructure required\n",
    "\n",
    "**TCP Transport (v0.8.0+ default)**: Direct worker communication:\n",
    "- Frontends connect to workers via TCP\n",
    "- Lower latency than pub/sub patterns\n",
    "- Simpler debugging with standard networking tools\n",
    "\n",
    "**NIXL (NVIDIA Inference Transfer Library)**: Handles actual KV cache data transfer:\n",
    "- Uses high-speed transports (RDMA, TCP, or CPU/SSD offload)\n",
    "- Transfers gigabytes of tensor data between workers\n",
    "- Direct worker-to-worker communication\n",
    "- Works with both K8s-native and NATS/etcd modes\n",
    "\n",
    "**KV-Aware Routing**: The Router knows which worker has which cache blocks:\n",
    "- In K8s-native mode: Routing metadata shared via API or direct communication\n",
    "- In NATS mode: NATS shares metadata about cache state\n",
    "- Enables intelligent request routing to workers with relevant cached data\n",
    "- Dramatically reduces prefill latency when cache hits occur\n",
    "\n",
    "**Optional NATS/etcd (for extreme scale)**: Advanced coordination:\n",
    "- **NATS**: Pub/sub messaging for metadata (cache events, routing tables)\n",
    "- **etcd**: Distributed configuration and service discovery\n",
    "- **When to use**: Very large clusters, multi-region, custom routing policies\n",
    "- **Note**: NATS does NOT transfer KV cache data (NIXL does that)\n",
    "- Router directs requests to workers with relevant cached prefixes\n",
    "- Improves cache hit rates even on single node with multiple GPUs\n",
    "- Workers transfer actual cache data via NIXL when needed\n",
    "\n",
    "### Understanding Multi-GPU/Multi-Node Benefits\n",
    "\n",
    "**In this lab (single node, 2 GPUs):**\n",
    "- Each GPU runs a separate worker\n",
    "- Router can direct requests to the worker with the best KV cache match\n",
    "- NIXL can transfer cache data between workers on the same node\n",
    "\n",
    "**In production (multi-node):**\n",
    "- Scale workers across multiple nodes\n",
    "- Scale frontends for high availability (multiple frontend replicas)\n",
    "- NIXL transfers cache data between nodes over the network (RDMA/TCP)\n",
    "- Kubernetes Services automatically load balance traffic across frontend replicas\n",
    "\n",
    "### When to Use Distributed Dynamo\n",
    "\n",
    "| Scenario | Use Distributed Dynamo? | Why |\n",
    "|----------|-----------|-----|\n",
    "| Single GPU | âŒ No | Adds overhead without benefit |\n",
    "| Multiple GPUs, single node | âœ… Yes | KV-aware routing improves cache hits between GPU workers |\n",
    "| 2-3 nodes | âœ… Yes | Cache awareness and coordination provide benefits |\n",
    "| 4+ nodes | âœ… Strongly Yes | Significant performance improvements from distributed cache awareness |\n",
    "| High traffic, repeated queries | âœ… Yes | Cache-aware routing reduces latency |\n",
    "| Low traffic, unique queries | âš ï¸ Maybe | Lower cache hit rates, but coordination still useful |\n",
    "| Dynamo 0.8+ | â„¹ï¸ Info | Can use K8s-native discovery (no NATS/etcd required) for simple deployments |\n",
    "\n",
    "---\n",
    "\n",
    "## Section 2: Environment Setup\n",
    "\n",
    "### Overview\n",
    "\n",
    "This lab uses **K8s-native deployment** - no NATS or etcd installation required. Dynamo uses Kubernetes EndpointSlices for service discovery and TCP for transport.\n",
    "\n",
    "**For NATS/etcd deployment** (extreme scale only), see Appendix A after completing this lab.\n",
    "\n",
    "### Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5fa39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables (use defaults if not already set)\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ğŸŒ² Lab 3: Distributed Dynamo Configuration\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Release Version:  $RELEASE_VERSION\"\n",
    "echo \"  Namespace:        $NAMESPACE\"\n",
    "echo \"  Node IP:          $NODE_IP\"\n",
    "echo \"\"\n",
    "echo \"âœ“ Environment configured for distributed Dynamo setup\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f839ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Deploy Distributed Dynamo Model\n",
    "\n",
    "### Understanding Dynamo's Distributed Architecture\n",
    "\n",
    "Dynamo (orchestrated by Grove) automatically uses NATS and etcd for distributed coordination when they are available in the cluster. The deployment will:\n",
    "\n",
    "**1. Workers register via NATS**: Each worker announces itself and its cache state\n",
    "**2. Frontend discovers workers**: The frontend finds workers through NATS service discovery\n",
    "**3. KV-aware Router**: Routes requests to workers with relevant cached data\n",
    "**4. NIXL handles KV cache data**: Workers transfer actual KV cache tensors via NIXL (RDMA/TCP), not through NATS\n",
    "\n",
    "**Note**: In Dynamo 0.8+, Kubernetes-native discovery (EndpointSlices) is available as an alternative to NATS/etcd for simpler deployments without KV-aware routing.\n",
    "\n",
    "### Step 1: Create Distributed Dynamo Deployment\n",
    "\n",
    "We'll create a deployment with 2 workers to demonstrate distributed architecture and KV-aware routing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5845bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create distributed Dynamo deployment\n",
    "echo \"Creating distributed Dynamo deployment with 2 workers...\"\n",
    "\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: nvidia.com/v1alpha1\n",
    "kind: DynamoGraphDeployment\n",
    "metadata:\n",
    "  name: vllm-distributed-demo\n",
    "  namespace: dynamo\n",
    "spec:\n",
    "  services:\n",
    "    Frontend:\n",
    "      dynamoNamespace: vllm-distributed-demo\n",
    "      componentType: frontend\n",
    "      replicas: 1\n",
    "      extraPodSpec:\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.7.1\n",
    "    VllmWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-distributed-demo\n",
    "      componentType: worker\n",
    "      replicas: 2\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: info\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: /data/huggingface-cache\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.7.1\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --tensor-parallel-size 1\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Distributed Dynamo deployment created\"\n",
    "echo \"  Deployment: vllm-distributed-demo\"\n",
    "echo \"  Workers: 2 (will use NATS for coordination and NIXL for cache transfer)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681fa3e",
   "metadata": {},
   "source": [
    "### Step 2: Create NodePort Service\n",
    "\n",
    "Expose the frontend for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92719737",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create NodePort service\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: vllm-distributed-demo-frontend-np\n",
    "  namespace: dynamo\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    nvidia.com/dynamo-component: Frontend\n",
    "    nvidia.com/dynamo-graph-deployment-name: vllm-distributed-demo\n",
    "  ports:\n",
    "  - port: 8000\n",
    "    targetPort: 8000\n",
    "    nodePort: 30200\n",
    "    protocol: TCP\n",
    "    name: http\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ NodePort service created on port 30200\"\n",
    "echo \"  Access at: http://$NODE_IP:30200\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ef8a6",
   "metadata": {},
   "source": [
    "### Step 3: Wait for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1888ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Wait for pods to be ready\n",
    "echo \"Waiting for distributed Dynamo deployment...\"\n",
    "echo \"This may take 2-3 minutes for model download and initialization...\"\n",
    "echo \"\"\n",
    "\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Wait for pods to be ready\n",
    "kubectl wait --for=condition=ready --timeout=300s \\\n",
    "  pods -l nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo \\\n",
    "  -n $NAMESPACE 2>/dev/null || echo \"Pods are initializing...\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Deployment status:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Distributed Dynamo deployment ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab3d97b",
   "metadata": {},
   "source": [
    "### Step 4: Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ff792",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Test the deployment\n",
    "echo \"Testing inference...\"\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain distributed inference in one sentence\"}],\n",
    "    \"max_tokens\": 50\n",
    "  }' | python3 -m json.tool\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Distributed Dynamo deployment is serving requests\"\n",
    "echo \"  Multiple workers coordinated via K8s-native discovery\"\n",
    "echo \"  NIXL handles KV cache data transfer between workers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f81a0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Monitoring Distributed Components\n",
    "\n",
    "### Step 1: Access Dynamo Metrics in Grafana\n",
    "\n",
    "Monitor your distributed Dynamo deployment using Grafana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b32eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get Grafana URL\n",
    "BREV_ID=$(hostname | cut -d'-' -f2)\n",
    "GRAFANA_URL=\"https://grafana0-${BREV_ID}.brevlab.com/\"\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ğŸ“Š Distributed Dynamo Monitoring\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Grafana URL: $GRAFANA_URL\"\n",
    "echo \"\"\n",
    "echo \"  Dashboard: Dynamo Inference Metrics\"\n",
    "echo \"    â€¢ Request rates across multiple workers\"\n",
    "echo \"    â€¢ Time to First Token (TTFT) distribution\"\n",
    "echo \"    â€¢ Inter-Token Latency (ITL) per worker\"\n",
    "echo \"    â€¢ Worker utilization and queue depths\"\n",
    "echo \"\"\n",
    "echo \"ğŸ”— Open Grafana and search for 'Dynamo Inference'\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b7b4fe",
   "metadata": {},
   "source": [
    "### Step 2: Test Distributed Dynamo with Traffic\n",
    "\n",
    "Generate meaningful traffic to see distributed coordination in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Generate test traffic with concurrent requests\n",
    "echo \"Generating traffic to distributed Dynamo deployment...\"\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Function to send a request\n",
    "send_request() {\n",
    "    local id=$1\n",
    "    curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d \"{\n",
    "        \\\"model\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\",\n",
    "        \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Explain distributed systems in 2 sentences. Request $id\\\"}],\n",
    "        \\\"stream\\\": false,\n",
    "        \\\"max_tokens\\\": 100\n",
    "      }\" > /dev/null 2>&1\n",
    "}\n",
    "\n",
    "# Send 30 requests with 3 concurrent workers\n",
    "echo \"Sending 30 requests with 3 concurrent connections...\"\n",
    "echo \"This will generate metrics for:\"\n",
    "echo \"  - Request throughput across multiple workers\"\n",
    "echo \"  - Worker utilization and load balancing\"\n",
    "echo \"  - KV cache effectiveness and NIXL transfers\"\n",
    "echo \"\"\n",
    "\n",
    "for i in {1..10}; do\n",
    "    send_request $((i*3-2)) &\n",
    "    send_request $((i*3-1)) &\n",
    "    send_request $((i*3)) &\n",
    "    wait\n",
    "    echo \"Batch $i/10 complete (requests $((i*3-2))-$((i*3)))\"\n",
    "    sleep 0.5\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Sent 30 requests with concurrent load\"\n",
    "echo \"\"\n",
    "echo \"Check metrics in Grafana:\"\n",
    "echo \"  - Dynamo Inference Dashboard\"\n",
    "echo \"  - Request throughput, TTFT, ITL across workers\"\n",
    "echo \"  - Worker queue depths and GPU utilization\"\n",
    "echo \"\"\n",
    "echo \"View Grafana: https://grafana0-$(hostname | sed 's/^brev-//').brevlab.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3cde46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Understanding Distributed Dynamo Trade-offs\n",
    "\n",
    "### K8s-Native vs NATS/etcd Comparison (v0.8.0+)\n",
    "\n",
    "| Aspect | K8s-Native | NATS/etcd |\n",
    "|--------|------------|-----------|\n",
    "| **Setup Complexity** | âœ… Simple (no extra infra) | âš ï¸ Complex (2 systems to manage) |\n",
    "| **Latency** | âœ… Lower (direct TCP) | âš ï¸ Slightly higher (pub/sub) |\n",
    "| **Scale Sweet Spot** | Most deployments | Extreme scale |\n",
    "| **Discovery** | EndpointSlices (built-in) | etcd (external) |\n",
    "| **Transport** | TCP | NATS + TCP |\n",
    "| **Ops Burden** | âœ… Low | âš ï¸ Medium-High |\n",
    "| **Multi-Region** | âš ï¸ Limited | âœ… Excellent |\n",
    "| **Custom Routing** | âš ï¸ Basic | âœ… Advanced |\n",
    "| **Cache Coordination** | âœ… Yes (via planner) | âœ… Yes (via NATS) |\n",
    "| **NIXL Support** | âœ… Yes | âœ… Yes |\n",
    "\n",
    "**Recommendation:** Start with K8s-native. Only add NATS/etcd if you need extreme scale or multi-region capabilities.\n",
    "\n",
    "### Single-Node vs Multi-Node\n",
    "\n",
    "**Single Node with Multiple GPUs (Typical Dev Setup)**:\n",
    "```\n",
    "âœ“ KV-aware routing still beneficial (routes to worker with cached data)\n",
    "âœ“ Learning opportunity to understand architecture\n",
    "âœ“ Workers can share cache blocks via NIXL locally\n",
    "âœ“ K8s-native = simpler (no NATS/etcd overhead)\n",
    "âœ— Less dramatic network benefits (same machine)\n",
    "```\n",
    "\n",
    "**Multi-Node (Production)**:\n",
    "```\n",
    "âœ“ KV-aware Router directs requests to nodes with relevant cache\n",
    "âœ“ NIXL transfers cache data efficiently (RDMA/TCP between nodes)\n",
    "âœ“ Improved cache hit rates = lower latency\n",
    "âœ“ Better resource utilization across cluster\n",
    "âœ“ K8s-native recommended for most deployments\n",
    "âœ“ NATS/etcd for extreme scale or multi-region\n",
    "```\n",
    "âœ“ Enables advanced features (cache migration, load balancing)\n",
    "âœ— Network latency between nodes\n",
    "âœ— Increased complexity in debugging\n",
    "```\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "```bash\n",
    "# Display performance comparison\n",
    "cat <<'EOF'\n",
    "\n",
    "Performance Impact of Distributed Dynamo:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Metric              â”‚ Single Node      â”‚ Multi-Node   â”‚\n",
    "â”‚                     â”‚ (Multi-GPU)      â”‚              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Cache Hit Rate      â”‚ +10-20%          â”‚ +20-40%      â”‚\n",
    "â”‚ Latency (P50)       â”‚ +2-5ms           â”‚ +2-5ms       â”‚\n",
    "â”‚ Latency (P99)       â”‚ +5-10ms          â”‚ +5-10ms      â”‚\n",
    "â”‚ Throughput          â”‚ Same to +10%     â”‚ +30-60%      â”‚\n",
    "â”‚ Memory Overhead     â”‚ +100-200MB       â”‚ +100-200MB   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "When Distributed Dynamo Helps Most:\n",
    "  â€¢ Multiple GPUs or nodes with high traffic\n",
    "  â€¢ Repeated queries (high cache hit potential)\n",
    "  â€¢ Long context lengths (expensive to recompute)\n",
    "  â€¢ Batch processing workloads\n",
    "\n",
    "When It May Not Help:\n",
    "  â€¢ Single GPU deployments\n",
    "  â€¢ Unique queries every time (low cache hit rate)\n",
    "  â€¢ Very short context lengths\n",
    "  â€¢ Real-time streaming with completely unique prompts\n",
    "\n",
    "Architecture Notes:\n",
    "  â€¢ Grove = Kubernetes Operator (orchestration)\n",
    "  â€¢ Dynamo = Serving Framework (actual inference)\n",
    "  â€¢ NATS = Metadata/coordination (small messages)\n",
    "  â€¢ NIXL = KV cache data transfer (large tensors via RDMA/TCP)\n",
    "EOF\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Section 6: Advanced Distributed Features\n",
    "\n",
    "### Cache Monitoring\n",
    "\n",
    "Check distributed coordination through worker logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f132d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get cache stats from worker logs\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "WORKER_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker,nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)\n",
    "\n",
    "if [ -n \"$WORKER_POD\" ]; then\n",
    "    echo \"Checking NIXL/NATS activity in worker logs...\"\n",
    "    echo \"\"\n",
    "    kubectl logs -n $NAMESPACE $WORKER_POD --tail=100 | grep -i \"nixl\\|nats\" | tail -10\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"Worker pod: $WORKER_POD\"\n",
    "    echo \"\"\n",
    "    echo \"What to look for:\"\n",
    "    echo \"  - NIXL initialization messages (KV cache transfer setup)\"\n",
    "    echo \"  - NATS connection status (coordination layer)\"\n",
    "    echo \"  - KV cache registration events\"\n",
    "    echo \"  - UCX backend messages (if using RDMA for cache transfer)\"\n",
    "else\n",
    "    echo \"âš ï¸ No worker pods found\"\n",
    "    echo \"Make sure the vllm-distributed-demo deployment is running\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd97e548",
   "metadata": {},
   "source": [
    "**Note**: Cache hit/miss metrics depend on workload patterns. Even on a single node with multiple GPUs, KV-aware routing can improve cache hits by directing requests to the worker that already has relevant cache blocks.\n",
    "\n",
    "### NATS Health Check\n",
    "\n",
    "Verify NATS is functioning correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98eb5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS service health\n",
    "echo \"Checking NATS health...\"\n",
    "kubectl exec -n nats-system nats-0 -- nats-server --version 2>/dev/null || kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"NATS endpoints:\"\n",
    "kubectl get svc -n nats-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf3da9",
   "metadata": {},
   "source": [
    "### etcd Health Check\n",
    "\n",
    "Verify etcd cluster health:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069af6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check etcd health\n",
    "echo \"Checking etcd health...\"\n",
    "ETCD_POD=$(kubectl get pods -n etcd-system -l app.kubernetes.io/name=etcd -o jsonpath='{.items[0].metadata.name}')\n",
    "\n",
    "if [ -n \"$ETCD_POD\" ]; then\n",
    "    kubectl exec -n etcd-system $ETCD_POD -- etcdctl endpoint health 2>/dev/null || echo \"etcd health check requires auth setup\"\n",
    "    echo \"\"\n",
    "    kubectl exec -n etcd-system $ETCD_POD -- etcdctl member list 2>/dev/null || echo \"etcd member list requires auth setup\"\n",
    "else\n",
    "    echo \"âš ï¸ No etcd pods found\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"etcd endpoints:\"\n",
    "kubectl get svc -n etcd-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05191a53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Cleanup\n",
    "\n",
    "### Step 1: Remove Distributed Demo Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete the distributed deployment\n",
    "echo \"Removing distributed Dynamo deployment...\"\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "kubectl delete dynamographdeployment vllm-distributed-demo -n $NAMESPACE\n",
    "kubectl delete svc vllm-distributed-demo-frontend-np -n $NAMESPACE\n",
    "\n",
    "echo \"âœ“ Distributed deployment removed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c05341",
   "metadata": {},
   "source": [
    "### Step 2: Verify Lab 1 Deployment is Still Running\n",
    "\n",
    "Your original Lab 1 deployment should still be running on port 30100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe96e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check Lab 1 deployment status\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Checking Lab 1 deployment...\"\n",
    "kubectl get dynamographdeployment vllm-disagg-router -n $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"Lab 1 pods:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-disagg-router\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Lab 1 deployment is available at: http://$NODE_IP:30100\"\n",
    "echo \"\"\n",
    "echo \"Test it:\"\n",
    "echo \"  curl http://$NODE_IP:30100/v1/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb9fc8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Known Issues (v0.8.0 Distributed Serving)\n",
    "\n",
    "**âš ï¸ Distributed Deployment Issues:**\n",
    "\n",
    "1. **K8s-native Discovery Propagation**: EndpointSlices may take 5-15 seconds to propagate in large clusters (20+ nodes). First requests may fail with \"no available workers\" until discovery completes.\n",
    "\n",
    "2. **NIXL KV Cache Transfer on ARM**: RDMA support on ARM-based nodes (Graviton, Ampere) is experimental in v0.8.0. Fall back to TCP transport if encountering issues.\n",
    "\n",
    "3. **Multi-Frontend KV-Aware Routing**: In some edge cases with >10 frontend replicas, routing metadata may be stale for 1-2 seconds, resulting in suboptimal cache hits (not failures, just less efficient).\n",
    "\n",
    "4. **NATS/etcd Compatibility**: If mixing K8s-native and NATS/etcd modes in the same cluster, ensure workers use consistent discovery method. Mixed modes are not supported.\n",
    "\n",
    "5. **Worker Gang Scheduling**: In v0.8.0, workers must all be ready before accepting traffic. If one worker pod fails, the entire deployment may be blocked. Use `kubectl describe dynamographdeployment` to debug.\n",
    "\n",
    "**Workarounds:**\n",
    "- Discovery delays: Add `--wait-for-workers-timeout=30s` flag to frontend\n",
    "- NIXL issues: Set `NIXL_TRANSPORT=tcp` env var to force TCP mode\n",
    "- Routing staleness: Reduce frontend replicas to 3-5 for optimal cache awareness\n",
    "- Gang scheduling: Use pod disruption budgets and ensure adequate node resources\n",
    "\n",
    "**Migration from v0.7.x:**\n",
    "- K8s-native discovery is backward compatible\n",
    "- Existing NATS/etcd deployments continue to work\n",
    "- Can upgrade in-place, but test K8s-native in staging first\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- âœ… Distributed Dynamo architecture with K8s-native discovery\n",
    "- âœ… Difference between Grove (operator) and Dynamo (serving framework)\n",
    "- âœ… Deploying distributed Dynamo with multiple workers\n",
    "- âœ… Creating a distributed deployment with KV-aware routing\n",
    "- âœ… Monitoring distributed Dynamo metrics with Grafana\n",
    "- âœ… Understanding NIXL for KV cache data transfer between workers\n",
    "- âœ… Trade-offs between single-node and multi-node setups\n",
    "- âœ… When to consider NATS/etcd for extreme scale (Appendix A & B)\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Architecture Clarity**:\n",
    "- **Grove**: Kubernetes Operator (orchestrates Dynamo deployments)\n",
    "- **Dynamo**: Inference serving framework (does the actual work)\n",
    "- **K8s-native Discovery**: EndpointSlices for worker discovery (v0.8.0+)\n",
    "- **NIXL**: Transfers actual KV cache data (gigabytes via RDMA/TCP/SSD)\n",
    "\n",
    "**Distributed Dynamo is Powerful**:\n",
    "- Enables KV-aware routing (even on single node with multiple GPUs)\n",
    "- NIXL transfers cache data efficiently between workers\n",
    "- Improves cache hit rates and throughput\n",
    "- Essential for production scale-out scenarios\n",
    "\n",
    "**Benefits of K8s-Native (v0.8.0+)**:\n",
    "- Simpler deployment (no NATS/etcd required)\n",
    "- Lower latency (direct TCP connections)\n",
    "- Fewer moving parts to maintain\n",
    "- Built-in Kubernetes service discovery\n",
    "\n",
    "**Production Considerations**:\n",
    "- Use distributed Dynamo when scaling beyond single GPU\n",
    "- Monitor worker metrics for health and utilization\n",
    "- Plan for network latency between nodes in multi-node setups\n",
    "- Consider cache hit patterns for your workload\n",
    "- NATS/etcd optional for extreme scale (see Appendix A)\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**When Companies Use Distributed Dynamo**:\n",
    "- Multi-region LLM deployments\n",
    "- High-traffic serving (1000+ RPS)\n",
    "- Multi-GPU and multi-node clusters\n",
    "- Cost optimization (share expensive cache computation)\n",
    "- Enterprise multi-tenant platforms\n",
    "\n",
    "**Deployment Options**:\n",
    "- **Single GPU**: No distributed coordination needed\n",
    "- **Multiple GPUs, single node**: Distributed Dynamo with KV-aware routing beneficial\n",
    "- **Small clusters**: K8s-native discovery (simple, effective)\n",
    "- **Large clusters**: K8s-native for most; NATS/etcd for extreme scale\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Experiment**: Try different worker replica counts to see KV-aware routing\n",
    "- **Monitor**: Watch Dynamo metrics dashboards during traffic\n",
    "- **Scale**: If you have access to multi-node clusters, test distributed benefits\n",
    "- **Learn**: Understand NIXL for KV cache data transfer in Dynamo docs\n",
    "- **Explore**: Review Appendix A & B if you need NATS/etcd for extreme scale\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Deployment Not Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362bbba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check deployment status\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "kubectl describe dynamographdeployment vllm-distributed-demo -n $NAMESPACE\n",
    "\n",
    "# Check pod status\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo\n",
    "\n",
    "# Check worker logs\n",
    "kubectl logs -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker\n",
    "\n",
    "# Common issues:\n",
    "# - Insufficient GPU resources\n",
    "# - Worker gang scheduling waiting for all pods\n",
    "# - Image pull errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283da0e",
   "metadata": {},
   "source": [
    "### Workers Not Discovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c31d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check K8s services and endpoints\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "kubectl get svc -n $NAMESPACE\n",
    "kubectl get endpoints -n $NAMESPACE\n",
    "\n",
    "# Check EndpointSlices (K8s-native discovery)\n",
    "kubectl get endpointslices -n $NAMESPACE\n",
    "\n",
    "# Check worker pods are running\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker\n",
    "\n",
    "# Common issues:\n",
    "# - Workers not fully ready (check 1/1 Running)\n",
    "# - Service selectors not matching pods\n",
    "# - Network policies blocking communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d65831",
   "metadata": {},
   "source": [
    "### No Requests Reaching Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Test frontend endpoint\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "curl -v http://$NODE_IP:30200/v1/models\n",
    "\n",
    "# Check frontend logs\n",
    "kubectl logs -n $NAMESPACE -l nvidia.com/dynamo-component=Frontend\n",
    "\n",
    "# Verify NodePort service exists\n",
    "kubectl get svc vllm-distributed-demo-frontend-np -n $NAMESPACE\n",
    "\n",
    "# Common issues:\n",
    "# - NodePort service not created\n",
    "# - Frontend pod not ready\n",
    "# - Port conflicts on node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f4d8e",
   "metadata": {},
   "source": [
    "### Understanding Cache Sharing with NIXL\n",
    "\n",
    "**NIXL** (NVIDIA Inference Transfer Library) handles KV cache transfer between workers:\n",
    "\n",
    "- Transfers actual KV cache data (gigabytes of tensors)\n",
    "- Uses RDMA, TCP, or CPU/SSD offload  \n",
    "- Direct worker-to-worker communication\n",
    "- Not visible in application logs (happens at library level)\n",
    "\n",
    "**On Single Node**:\n",
    "- Cache transfers via NIXL are faster (local)\n",
    "- Workers coordinate via K8s-native discovery\n",
    "- Benefits still apply with multiple GPU workers\n",
    "\n",
    "**Benefits Require**:\n",
    "- Multiple workers (even on same node)\n",
    "- Repeated queries with shared prefixes\n",
    "- Workload that generates cache hits\n",
    "\n",
    "**For NATS/etcd troubleshooting**, see Appendix B\n",
    "\n",
    "---\n",
    "\n",
    "## Appendix A: NATS/etcd Architecture (Optional - Extreme Scale)\n",
    "\n",
    "This appendix covers the NATS/etcd deployment architecture for extreme scale deployments or multi-region setups. **Most users should use K8s-native deployment** (covered in the main lab).\n",
    "\n",
    "### When You Need NATS/etcd\n",
    "\n",
    "Consider NATS/etcd if you have:\n",
    "- Very large Kubernetes clusters (extreme scale)\n",
    "- Multi-region deployments\n",
    "- Complex custom routing logic\n",
    "- Advanced cache policies and coordination requirements\n",
    "\n",
    "### NATS/etcd Architecture Diagram\n",
    "\n",
    "```\n",
    "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "               â”‚  Cloud Load Balancer       â”‚\n",
    "               â”‚  or Ingress Controller     â”‚\n",
    "               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚                â”‚\n",
    "    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "    â”‚Frontend 1â”‚    â”‚Frontend 2â”‚    â”‚Frontend 3â”‚\n",
    "    â”‚ (Node 1) â”‚    â”‚ (Node 2) â”‚    â”‚ (Node 3) â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                â”‚                â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  NATS Message Bus     â”‚\n",
    "              â”‚  (Metadata, Routing,  â”‚\n",
    "              â”‚   Cache Awareness)    â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  etcd (Coordination)  â”‚\n",
    "              â”‚  (Service Discovery)  â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚                â”‚\n",
    "    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Worker 1 â”‚    â”‚ Worker 2 â”‚    â”‚ Worker 3 â”‚\n",
    "    â”‚ (Node 4) â”‚    â”‚ (Node 5) â”‚    â”‚ (Node 6) â”‚\n",
    "    â”‚  +GPU    â”‚    â”‚  +GPU    â”‚    â”‚  +GPU    â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                â”‚                â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  NIXL (KV Cache       â”‚\n",
    "              â”‚   Data Transfer)      â”‚\n",
    "              â”‚  RDMA/TCP/SSD         â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Components\n",
    "\n",
    "**NATS Message Bus:**\n",
    "- Pub/sub messaging for metadata (cache events, routing tables)\n",
    "- Low-latency coordination between frontends and workers\n",
    "- Does NOT transfer KV cache data (NIXL handles that)\n",
    "\n",
    "**etcd:**\n",
    "- Distributed configuration and service discovery\n",
    "- Leader election and coordination\n",
    "- Cluster state management\n",
    "\n",
    "**NIXL:**\n",
    "- Handles actual KV cache data transfer (same as K8s-native mode)\n",
    "- Uses RDMA/TCP for high-speed transfer\n",
    "- Direct worker-to-worker communication\n",
    "\n",
    "### Deployment Steps (Optional)\n",
    "\n",
    "If you need to deploy NATS/etcd, refer to Section 2a in the main lab (marked as \"Optional - Skip for K8s-Native\"). The steps are preserved but skipped in the standard lab flow.\n",
    "\n",
    "### Trade-offs vs K8s-Native\n",
    "\n",
    "| Aspect | K8s-Native | NATS/etcd |\n",
    "|--------|------------|-----------|\n",
    "| Setup Complexity | Simple | Complex |\n",
    "| Ops Burden | Low | Medium-High |\n",
    "| Max Scale | Standard clusters | Extreme scale |\n",
    "| Multi-Region | Limited | Excellent |\n",
    "| Custom Routing | Basic | Advanced |\n",
    "\n",
    "---\n",
    "\n",
    "## Appendix B: NATS/etcd Deployment Steps (Optional)\n",
    "\n",
    "**âš ï¸ WARNING:** These steps are ONLY for users deploying NATS/etcd for extreme-scale scenarios. Most users should skip this appendix and use K8s-native deployment (covered in the main lab).\n",
    "\n",
    "### When to Use These Steps\n",
    "\n",
    "Deploy NATS/etcd only if you have:\n",
    "- Very large Kubernetes clusters (extreme scale)\n",
    "- Multi-region deployments\n",
    "- Complex custom routing requirements\n",
    "- Advanced cache coordination policies\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Complete Section 2 Step 1 (Environment Setup)\n",
    "- Have cluster-admin access for cluster-wide resources\n",
    "\n",
    "### Step 1: Install NATS Message Bus\n",
    "\n",
    "NATS handles distributed coordination metadata between Dynamo components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for NATS\n",
    "kubectl create namespace nats-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Add NATS Helm repository\n",
    "echo \"Adding NATS Helm repository...\"\n",
    "helm repo add nats https://nats-io.github.io/k8s/helm/charts/\n",
    "helm repo update\n",
    "\n",
    "# Install NATS (with Prometheus exporter)\n",
    "echo \"Installing NATS with metrics exporter...\"\n",
    "helm upgrade --install nats nats/nats \\\n",
    "  --namespace nats-system \\\n",
    "  --set config.jetstream.enabled=true \\\n",
    "  --set config.jetstream.fileStore.pvc.size=1Gi \\\n",
    "  --set promExporter.enabled=true \\\n",
    "  --set promExporter.port=7777 \\\n",
    "  --wait \\\n",
    "  --timeout 5m\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ NATS installed successfully\"\n",
    "echo \"  Connection: nats://nats.nats-system:4222\"\n",
    "echo \"  Metrics: Port 7777\"\n",
    "echo \"\"\n",
    "echo \"Note: NATS handles metadata (cache events, routing tables).\"\n",
    "echo \"      Actual KV cache data transfers via NIXL (RDMA/TCP).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4ca9f",
   "metadata": {},
   "source": [
    "### Step 2: Install etcd Coordination Layer\n",
    "\n",
    "etcd provides distributed coordination for Grove components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f28543",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for etcd\n",
    "kubectl create namespace etcd-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Add Bitnami Helm repository\n",
    "echo \"Adding Bitnami Helm repository...\"\n",
    "helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "helm repo update\n",
    "\n",
    "# Install etcd (using legacy Bitnami mirror)\n",
    "echo \"Installing etcd...\"\n",
    "helm upgrade --install etcd bitnami/etcd \\\n",
    "  --namespace etcd-system \\\n",
    "  --set replicaCount=1 \\\n",
    "  --set auth.rbac.create=false \\\n",
    "  --set image.registry=docker.io \\\n",
    "  --set image.repository=bitnamilegacy/etcd \\\n",
    "  --set persistence.size=1Gi \\\n",
    "  --set preUpgradeHook.enabled=false \\\n",
    "  --wait \\\n",
    "  --timeout 5m\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ etcd installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8646f8a",
   "metadata": {},
   "source": [
    "### Step 3: Verify Infrastructure\n",
    "\n",
    "Check that NATS and etcd are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS pods\n",
    "echo \"Checking NATS deployment...\"\n",
    "kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking NATS service...\"\n",
    "kubectl get svc -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "\n",
    "# Check etcd pods\n",
    "echo \"Checking etcd deployment...\"\n",
    "kubectl get pods -n etcd-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking etcd service...\"\n",
    "kubectl get svc -n etcd-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Infrastructure verified\"\n",
    "echo \"  NATS:  nats://nats.nats-system:4222 (metadata/coordination)\"\n",
    "echo \"  etcd:  http://etcd.etcd-system:2379 (service discovery)\"\n",
    "echo \"  NIXL will handle KV cache data transfer between workers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4517990",
   "metadata": {},
   "source": [
    "### Step 4: Enable Prometheus Monitoring (Optional)\n",
    "\n",
    "Create PodMonitors so Prometheus can scrape NATS and etcd metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create PodMonitor for NATS\n",
    "echo \"Enabling NATS metrics collection...\"\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PodMonitor\n",
    "metadata:\n",
    "  name: nats\n",
    "  namespace: nats-system\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/name: nats\n",
    "  podMetricsEndpoints:\n",
    "  - port: prom-metrics\n",
    "    path: /metrics\n",
    "EOF\n",
    "\n",
    "# Create PodMonitor for etcd\n",
    "echo \"Enabling etcd metrics collection...\"\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PodMonitor\n",
    "metadata:\n",
    "  name: etcd\n",
    "  namespace: etcd-system\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/name: etcd\n",
    "  podMetricsEndpoints:\n",
    "  - port: client\n",
    "    path: /metrics\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Prometheus monitoring enabled\"\n",
    "echo \"  Metrics will be available in Grafana within 2-3 minutes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06788c7e",
   "metadata": {},
   "source": [
    "### Cleanup (NATS/etcd)\n",
    "\n",
    "When you're done with NATS/etcd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove NATS\n",
    "echo \"Removing NATS...\"\n",
    "helm uninstall nats -n nats-system\n",
    "kubectl delete namespace nats-system\n",
    "\n",
    "# Remove etcd  \n",
    "echo \"Removing etcd...\"\n",
    "helm uninstall etcd -n etcd-system\n",
    "kubectl delete namespace etcd-system\n",
    "\n",
    "# Remove PodMonitors\n",
    "kubectl delete podmonitor nats -n nats-system 2>/dev/null || true\n",
    "kubectl delete podmonitor etcd -n etcd-system 2>/dev/null || true\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Infrastructure removed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572495a2",
   "metadata": {},
   "source": [
    "### Configuring Dynamo to Use NATS/etcd\n",
    "\n",
    "After installing NATS/etcd, you need to configure your `DynamoGraphDeployment` to use them. Add these annotations to your deployment spec:\n",
    "\n",
    "```yaml\n",
    "metadata:\n",
    "  annotations:\n",
    "    dynamo.nvidia.com/discovery-backend: \"nats\"  # Use NATS/etcd instead of K8s-native\n",
    "    dynamo.nvidia.com/nats-url: \"nats://nats.nats-system:4222\"\n",
    "    dynamo.nvidia.com/etcd-url: \"http://etcd.etcd-system:2379\"\n",
    "```\n",
    "\n",
    "Refer to Dynamo documentation for complete configuration options.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **Dynamo Deployment Guide**: https://docs.nvidia.com/dynamo/latest/guides/dynamo_deploy/\n",
    "- **Grove Operator Guide**: https://docs.nvidia.com/dynamo/latest/guides/dynamo_deploy/grove.html\n",
    "- **Grove GitHub Repository**: https://github.com/NVIDIA/grove\n",
    "- **NIXL Documentation**: NVIDIA Inference Transfer Library (check Dynamo docs)\n",
    "- **NATS Documentation**: https://docs.nats.io/\n",
    "- **etcd Documentation**: https://etcd.io/docs/\n",
    "- **NVIDIA Dynamo Documentation**: https://docs.nvidia.com/dynamo/latest/\n",
    "- **Distributed Systems Patterns**: Understanding consensus and coordination\n",
    "- **KV Cache Architecture**: Understanding distributed cache strategies\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've completed Lab 3: Distributed Dynamo with Grove Orchestration** ğŸŒ²\n",
    "\n",
    "You now understand the fundamentals of distributed LLM serving, the difference between Grove (operator) and Dynamo (serving framework), and how NATS (metadata) and NIXL (data transfer) work together for distributed coordination!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-kernelspec,-widgets,-language_info"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
