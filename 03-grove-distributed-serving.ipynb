{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2429cb",
   "metadata": {},
   "source": [
    "# Lab 3: Distributed Dynamo with Grove Orchestration\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you will:\n",
    "- Understand Dynamo's distributed serving architecture with Grove orchestration\n",
    "- Deploy NATS and etcd for distributed coordination and KV-aware routing\n",
    "- Enable distributed KV cache awareness and transfer via NIXL\n",
    "- Monitor distributed components with Grafana\n",
    "- Understand when and why to use distributed Dynamo in production\n",
    "\n",
    "**Prerequisites**: Complete Lab 1 (Dynamo Deployment) and Lab 2 (Monitoring)\n",
    "\n",
    "**Note**: Distributed Dynamo is designed for multi-node Kubernetes clusters or single nodes with multiple GPUs. While we'll deploy it on a single node for learning purposes, maximum benefits are realized when scaling across multiple nodes with high cache hit workloads.\n",
    "\n",
    "## Duration: ~45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Understanding Distributed Dynamo Architecture\n",
    "\n",
    "### What is Grove vs Dynamo?\n",
    "\n",
    "**Dynamo** is NVIDIA's inference serving framework (the Python code, Router, Frontend, Workers).\n",
    "\n",
    "**Grove** is the Kubernetes Operator that orchestrates Dynamo deployments (handling CRDs like `DynamoGraphDeployment`, pod gangs, startup order).\n",
    "\n",
    "**Distributed Dynamo** (orchestrated by Grove) enables:\n",
    "- **Multi-node deployments** across Kubernetes clusters or multi-GPU single nodes\n",
    "- **KV-aware routing** where the Router knows which worker has which cache blocks\n",
    "- **Distributed KV cache transfer** between workers via NIXL (NVIDIA Inference Transfer Library)\n",
    "- **Coordination and discovery** using NATS for metadata and etcd for service registration\n",
    "- **Advanced features** like cache migration and load balancing\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "```\n",
    "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "               â”‚  Cloud Load Balancer       â”‚\n",
    "               â”‚  or Ingress Controller     â”‚\n",
    "               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚                â”‚\n",
    "    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "    â”‚Frontend 1â”‚    â”‚Frontend 2â”‚    â”‚Frontend 3â”‚\n",
    "    â”‚ (Node 1) â”‚    â”‚ (Node 2) â”‚    â”‚ (Node 3) â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                â”‚                â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  NATS Message Bus     â”‚\n",
    "              â”‚  (Metadata, Routing,  â”‚\n",
    "              â”‚   Cache Awareness)    â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  etcd (Coordination)  â”‚\n",
    "              â”‚  (Service Discovery)  â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚                â”‚\n",
    "    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Worker 1 â”‚    â”‚ Worker 2 â”‚    â”‚ Worker 3 â”‚\n",
    "    â”‚ (Node 4) â”‚    â”‚ (Node 5) â”‚    â”‚ (Node 6) â”‚\n",
    "    â”‚  +GPU    â”‚    â”‚  +GPU    â”‚    â”‚  +GPU    â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                â”‚                â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  NIXL (KV Cache       â”‚\n",
    "              â”‚   Data Transfer)      â”‚\n",
    "              â”‚  RDMA/TCP/SSD         â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Note: Workers typically run on GPU nodes (4-6), separate from\n",
    "      CPU-only frontend nodes (1-3). In smaller clusters, they\n",
    "      may share nodes with frontends.\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**NATS**: A high-performance message bus that enables:\n",
    "- Metadata sharing (cache events, routing tables)\n",
    "- Low-latency pub/sub messaging for coordination\n",
    "- Resilient delivery guarantees\n",
    "- **Note**: NATS does NOT transfer the actual KV cache data (which is gigabytes of tensors)\n",
    "\n",
    "**etcd**: A distributed key-value store that provides:\n",
    "- Service discovery and registration\n",
    "- Configuration management\n",
    "- Leader election and coordination\n",
    "\n",
    "**NIXL (NVIDIA Inference Transfer Library)**: Handles actual KV cache data transfer:\n",
    "- Uses high-speed transports (RDMA, TCP, or CPU/SSD offload)\n",
    "- Transfers gigabytes of tensor data between workers\n",
    "- Direct worker-to-worker communication (not through NATS)\n",
    "\n",
    "**KV-Aware Routing**: The Router knows which worker has which cache blocks:\n",
    "- NATS shares metadata about cache state\n",
    "- Router directs requests to workers with relevant cached prefixes\n",
    "- Improves cache hit rates even on single node with multiple GPUs\n",
    "- Workers transfer actual cache data via NIXL when needed\n",
    "\n",
    "### How Multiple Frontends Work\n",
    "\n",
    "The architecture diagram shows 2 frontends, but **frontend replicas â‰  one per node**. Here's how it actually works in production:\n",
    "\n",
    "**Frontend Scaling Strategy**:\n",
    "```\n",
    "Small cluster (3 nodes):    2-3 frontend replicas\n",
    "Medium cluster (10 nodes):  3-5 frontend replicas\n",
    "Large cluster (50+ nodes):  5-10 frontend replicas\n",
    "```\n",
    "\n",
    "**Load Balancing via Kubernetes Service**:\n",
    "\n",
    "When you create a Service (NodePort or LoadBalancer), Kubernetes automatically load balances across all frontend pods:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: frontend-service\n",
    "spec:\n",
    "  type: LoadBalancer  # or NodePort\n",
    "  selector:\n",
    "    component: frontend  # Selects ALL frontend pods\n",
    "  ports:\n",
    "  - port: 8000\n",
    "```\n",
    "\n",
    "**How Traffic Flows**:\n",
    "1. **External Load Balancer** (cloud provider or Ingress) receives request\n",
    "2. **Kubernetes Service** load balances to any frontend pod\n",
    "3. **Frontend** sends inference request via NATS\n",
    "4. **NATS** routes to an available worker (KV-aware routing if enabled)\n",
    "5. **Worker** may receive KV cache data from another worker via NIXL\n",
    "6. **Worker** responds via NATS\n",
    "7. **Frontend** returns HTTP response\n",
    "\n",
    "**Key Benefits**:\n",
    "- âœ… **High Availability**: If one frontend crashes, others continue\n",
    "- âœ… **Load Distribution**: Spread HTTP connections across pods\n",
    "- âœ… **Dynamic Discovery**: NATS decouples frontends from workers (Dynamo 0.7.x requires NATS/etcd; 0.8+ supports K8s-native discovery)\n",
    "- âœ… **Flexible Scaling**: Add/remove frontends independently\n",
    "- âœ… **KV-Aware Routing**: Route requests to workers with relevant cached data\n",
    "\n",
    "**Single Node (This Lab)**:\n",
    "Even in a single-node setup with multiple GPUs/workers, KV-aware routing provides benefits! The Router uses NATS to track which worker has which cached prefixes, directing requests to the worker with the best cache hit potential.\n",
    "\n",
    "### When to Use Distributed Dynamo\n",
    "\n",
    "| Scenario | Use Distributed Dynamo? | Why |\n",
    "|----------|-----------|-----|\n",
    "| Single GPU | âŒ No | Adds overhead without benefit |\n",
    "| Multiple GPUs, single node | âœ… Yes | KV-aware routing improves cache hits between GPU workers |\n",
    "| 2-3 nodes | âœ… Yes | Cache awareness and coordination provide benefits |\n",
    "| 4+ nodes | âœ… Strongly Yes | Significant performance improvements from distributed cache awareness |\n",
    "| High traffic, repeated queries | âœ… Yes | Cache-aware routing reduces latency |\n",
    "| Low traffic, unique queries | âš ï¸ Maybe | Lower cache hit rates, but coordination still useful |\n",
    "| Dynamo 0.8+ | â„¹ï¸ Info | Can use K8s-native discovery (no NATS/etcd required) for simple deployments |\n",
    "\n",
    "---\n",
    "\n",
    "## Section 2: Deploy Distributed Infrastructure\n",
    "\n",
    "### Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables (use defaults if not already set)\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.7.1}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ğŸŒ² Lab 3: Distributed Dynamo Configuration\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Release Version:  $RELEASE_VERSION\"\n",
    "echo \"  Namespace:        $NAMESPACE\"\n",
    "echo \"  Node IP:          $NODE_IP\"\n",
    "echo \"\"\n",
    "echo \"âœ“ Environment configured for distributed Dynamo setup\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72888e0",
   "metadata": {},
   "source": [
    "### Step 2: Install NATS Message Bus\n",
    "\n",
    "NATS handles distributed coordination metadata between Dynamo components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498baa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for NATS\n",
    "kubectl create namespace nats-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Add NATS Helm repository\n",
    "echo \"Adding NATS Helm repository...\"\n",
    "helm repo add nats https://nats-io.github.io/k8s/helm/charts/\n",
    "helm repo update\n",
    "\n",
    "# Install NATS (with Prometheus exporter)\n",
    "echo \"Installing NATS with metrics exporter...\"\n",
    "helm upgrade --install nats nats/nats \\\n",
    "  --namespace nats-system \\\n",
    "  --set config.jetstream.enabled=true \\\n",
    "  --set config.jetstream.fileStore.pvc.size=1Gi \\\n",
    "  --set promExporter.enabled=true \\\n",
    "  --set promExporter.port=7777 \\\n",
    "  --wait \\\n",
    "  --timeout 5m\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ NATS installed successfully\"\n",
    "echo \"  Connection: nats://nats.nats-system:4222\"\n",
    "echo \"  Metrics: Port 7777\"\n",
    "echo \"\"\n",
    "echo \"Note: NATS handles metadata (cache events, routing tables).\"\n",
    "echo \"      Actual KV cache data transfers via NIXL (RDMA/TCP).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f7f82",
   "metadata": {},
   "source": [
    "### Step 3: Install etcd Coordination Layer\n",
    "\n",
    "etcd provides distributed coordination for Grove components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for etcd\n",
    "kubectl create namespace etcd-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Add Bitnami Helm repository\n",
    "echo \"Adding Bitnami Helm repository...\"\n",
    "helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "helm repo update\n",
    "\n",
    "# Install etcd (using legacy Bitnami mirror)\n",
    "echo \"Installing etcd...\"\n",
    "helm upgrade --install etcd bitnami/etcd \\\n",
    "  --namespace etcd-system \\\n",
    "  --set replicaCount=1 \\\n",
    "  --set auth.rbac.create=false \\\n",
    "  --set image.registry=docker.io \\\n",
    "  --set image.repository=bitnamilegacy/etcd \\\n",
    "  --set persistence.size=1Gi \\\n",
    "  --set preUpgradeHook.enabled=false \\\n",
    "  --wait \\\n",
    "  --timeout 5m\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ etcd installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4676765f",
   "metadata": {},
   "source": [
    "### Step 4: Verify Grove Infrastructure\n",
    "\n",
    "Check that NATS and etcd are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS pods\n",
    "echo \"Checking NATS deployment...\"\n",
    "kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking NATS service...\"\n",
    "kubectl get svc -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "\n",
    "# Check etcd pods\n",
    "echo \"Checking etcd deployment...\"\n",
    "kubectl get pods -n etcd-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking etcd service...\"\n",
    "kubectl get svc -n etcd-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Grove infrastructure verified\"\n",
    "echo \"  NATS:  nats://nats.nats-system:4222 (metadata/coordination)\"\n",
    "echo \"  etcd:  http://etcd.etcd-system:2379 (service discovery)\"\n",
    "echo \"  NIXL will handle KV cache data transfer between workers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59024dab",
   "metadata": {},
   "source": [
    "### Step 4: Enable Prometheus Monitoring\n",
    "\n",
    "Create PodMonitors so Prometheus can scrape NATS and etcd metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce70ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create PodMonitor for NATS (scrapes the prometheus-nats-exporter sidecar)\n",
    "echo \"Enabling NATS metrics collection...\"\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PodMonitor\n",
    "metadata:\n",
    "  name: nats\n",
    "  namespace: nats-system\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/name: nats\n",
    "  podMetricsEndpoints:\n",
    "  - port: prom-metrics\n",
    "    path: /metrics\n",
    "EOF\n",
    "\n",
    "# Create PodMonitor for etcd\n",
    "echo \"Enabling etcd metrics collection...\"\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PodMonitor\n",
    "metadata:\n",
    "  name: etcd\n",
    "  namespace: etcd-system\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/name: etcd\n",
    "  podMetricsEndpoints:\n",
    "  - port: client\n",
    "    path: /metrics\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Prometheus monitoring enabled for distributed infrastructure\"\n",
    "echo \"  Metrics will be available in Grafana within 2-3 minutes\"\n",
    "echo \"\"\n",
    "echo \"  NATS metrics: Scraped from prometheus-nats-exporter (port: prom-metrics)\"\n",
    "echo \"  etcd metrics: Scraped directly from etcd's /metrics endpoint (port: client)\"\n",
    "echo \"\"\n",
    "echo \"Note: etcd metrics typically appear faster than NATS metrics\"\n",
    "echo \"      NATS metrics show coordination traffic, not KV cache data volume\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e121721",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Deploy Distributed Dynamo Model\n",
    "\n",
    "### Understanding Dynamo's Distributed Architecture\n",
    "\n",
    "Dynamo (orchestrated by Grove) automatically uses NATS and etcd for distributed coordination when they are available in the cluster. The deployment will:\n",
    "\n",
    "**1. Workers register via NATS**: Each worker announces itself and its cache state\n",
    "**2. Frontend discovers workers**: The frontend finds workers through NATS service discovery\n",
    "**3. KV-aware Router**: Routes requests to workers with relevant cached data\n",
    "**4. NIXL handles KV cache data**: Workers transfer actual KV cache tensors via NIXL (RDMA/TCP), not through NATS\n",
    "\n",
    "**Note**: In Dynamo 0.8+, Kubernetes-native discovery (EndpointSlices) is available as an alternative to NATS/etcd for simpler deployments without KV-aware routing.\n",
    "\n",
    "### Step 1: Create Distributed Dynamo Deployment\n",
    "\n",
    "We'll create a deployment with 2 workers to demonstrate distributed architecture and KV-aware routing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create distributed Dynamo deployment\n",
    "echo \"Creating distributed Dynamo deployment with 2 workers...\"\n",
    "\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: nvidia.com/v1alpha1\n",
    "kind: DynamoGraphDeployment\n",
    "metadata:\n",
    "  name: vllm-distributed-demo\n",
    "  namespace: dynamo\n",
    "spec:\n",
    "  services:\n",
    "    Frontend:\n",
    "      dynamoNamespace: vllm-distributed-demo\n",
    "      componentType: frontend\n",
    "      replicas: 1\n",
    "      extraPodSpec:\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.7.1\n",
    "    VllmWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-distributed-demo\n",
    "      componentType: worker\n",
    "      replicas: 2\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: info\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: /data/huggingface-cache\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.7.1\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --tensor-parallel-size 1\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Distributed Dynamo deployment created\"\n",
    "echo \"  Deployment: vllm-distributed-demo\"\n",
    "echo \"  Workers: 2 (will use NATS for coordination and NIXL for cache transfer)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39d1c77",
   "metadata": {},
   "source": [
    "### Step 2: Create NodePort Service\n",
    "\n",
    "Expose the frontend for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create NodePort service\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: vllm-distributed-demo-frontend-np\n",
    "  namespace: dynamo\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    nvidia.com/dynamo-component: Frontend\n",
    "    nvidia.com/dynamo-graph-deployment-name: vllm-distributed-demo\n",
    "  ports:\n",
    "  - port: 8000\n",
    "    targetPort: 8000\n",
    "    nodePort: 30200\n",
    "    protocol: TCP\n",
    "    name: http\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ NodePort service created on port 30200\"\n",
    "echo \"  Access at: http://$NODE_IP:30200\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc6807",
   "metadata": {},
   "source": [
    "### Step 3: Wait for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce7055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Wait for pods to be ready\n",
    "echo \"Waiting for distributed Dynamo deployment...\"\n",
    "echo \"This may take 2-3 minutes for model download and initialization...\"\n",
    "echo \"\"\n",
    "\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Wait for pods to be ready\n",
    "kubectl wait --for=condition=ready --timeout=300s \\\n",
    "  pods -l nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo \\\n",
    "  -n $NAMESPACE 2>/dev/null || echo \"Pods are initializing...\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Deployment status:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Distributed Dynamo deployment ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84b628",
   "metadata": {},
   "source": [
    "### Step 4: Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8fd99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Test the deployment\n",
    "echo \"Testing inference...\"\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain distributed inference in one sentence\"}],\n",
    "    \"max_tokens\": 50\n",
    "  }' | python3 -m json.tool\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Distributed Dynamo deployment is serving requests\"\n",
    "echo \"  Router uses NATS for worker coordination\"\n",
    "echo \"  NIXL handles KV cache data transfer between workers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77dcce",
   "metadata": {},
   "source": [
    "### Step 5: Verify NATS and NIXL Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eca6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check worker logs for NATS connectivity and NIXL initialization\n",
    "echo \"Verifying NATS and NIXL integration...\"\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "WORKER_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker,nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)\n",
    "\n",
    "if [ -n \"$WORKER_POD\" ]; then\n",
    "    echo \"Checking worker: $WORKER_POD\"\n",
    "    kubectl logs -n $NAMESPACE $WORKER_POD 2>&1 | grep -i \"nats\\|nixl\" | head -5\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"âœ“ Workers are using:\"\n",
    "    echo \"  â€¢ NATS for coordination and cache awareness\"\n",
    "    echo \"  â€¢ NIXL for KV cache data transfer (RDMA/TCP/SSD)\"\n",
    "    echo \"\"\n",
    "    echo \"Note: NATS carries metadata (cache events, routing tables).\"\n",
    "    echo \"      NIXL transfers the actual tensor data between workers.\"\n",
    "else\n",
    "    echo \"âš ï¸ No worker pods found. Make sure the deployment is running:\"\n",
    "    kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5519a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Monitoring Distributed Components\n",
    "\n",
    "### Step 1: Access Grafana Dashboards\n",
    "\n",
    "The distributed infrastructure dashboards were created during the oneshot.sh bootstrap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102487fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get Grafana URL\n",
    "BREV_ID=$(hostname | cut -d'-' -f2)\n",
    "GRAFANA_URL=\"https://grafana0-${BREV_ID}.brevlab.com/\"\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ğŸ“Š Distributed Dynamo Monitoring Dashboards\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Grafana URL: $GRAFANA_URL\"\n",
    "echo \"\"\n",
    "echo \"  Available Dashboards:\"\n",
    "echo \"    â€¢ NATS Overview - Message bus metrics (metadata/coordination)\"\n",
    "echo \"    â€¢ etcd Overview - Service discovery metrics\"\n",
    "echo \"    â€¢ Dynamo Inference Metrics - Model serving metrics\"\n",
    "echo \"\"\n",
    "echo \"ğŸ”— Open Grafana and search for these dashboards\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43214545",
   "metadata": {},
   "source": [
    "### Step 2: Understanding NATS Metrics\n",
    "\n",
    "The NATS Overview dashboard shows real-time metrics about the message bus that Dynamo uses for distributed coordination metadata (not KV cache data).\n",
    "\n",
    "#### Connection Metrics (Top Left Panel)\n",
    "\n",
    "**`nats_varz_connections`** - Current active connections to NATS\n",
    "- **What it shows**: Number of Dynamo components connected to NATS\n",
    "- **Expected value**: \n",
    "  - With distributed deployment running: 2-4 connections (workers + frontend)\n",
    "  - Without active deployment: 0\n",
    "- **Why it matters**: Each Dynamo component (frontend, workers) maintains a connection to NATS for coordination\n",
    "\n",
    "#### Message Rate Metrics (Top Center Panels)\n",
    "\n",
    "**`rate(nats_varz_in_msgs[1m])`** - Incoming messages per second\n",
    "- **What it shows**: Coordination messages NATS is receiving (cache events, routing metadata)\n",
    "- **Expected value**: \n",
    "  - Idle: Low (< 1 msg/s for heartbeats)\n",
    "  - During load: 10-100+ msg/s depending on request rate\n",
    "- **Why it matters**: Shows the coordination throughput (NOT KV cache data volume)\n",
    "- **Important**: NATS messages are small metadata packets, not gigabytes of tensor data\n",
    "\n",
    "**`rate(nats_varz_out_msgs[1m])`** - Outgoing messages per second\n",
    "- **What it shows**: How many coordination messages NATS is distributing\n",
    "- **Expected value**: Similar to or slightly higher than incoming rate\n",
    "- **Why it matters**: NATS may send multiple copies of messages to subscribers (pub/sub pattern)\n",
    "\n",
    "#### Resource Metrics (Top Right Panel)\n",
    "\n",
    "**`nats_varz_cpu`** - NATS CPU usage percentage\n",
    "- **What it shows**: CPU usage of the NATS process\n",
    "- **Expected value**: \n",
    "  - Idle: < 1%\n",
    "  - Under load: 5-20%\n",
    "  - High load: > 50% (consider scaling)\n",
    "- **Why it matters**: High CPU might indicate NATS is becoming a bottleneck\n",
    "\n",
    "#### Message Rate Graph (Middle Panel)\n",
    "\n",
    "**Time series visualization** of message rates\n",
    "- **Green line**: Incoming messages (rate(nats_varz_in_msgs[1m]))\n",
    "- **Blue line**: Outgoing messages (rate(nats_varz_out_msgs[1m]))\n",
    "- **What to look for**:\n",
    "  - Spikes during traffic bursts\n",
    "  - Correlation between in/out rates\n",
    "  - Steady state during constant load\n",
    "  - Drops to zero when idle\n",
    "\n",
    "#### Memory Usage (Bottom Left Panel)\n",
    "\n",
    "**`nats_varz_mem`** - NATS memory consumption in MB\n",
    "- **What it shows**: RAM used by the NATS process\n",
    "- **Expected value**: \n",
    "  - Base: 20-50 MB\n",
    "  - With JetStream: 50-200 MB\n",
    "  - Under load: May increase with buffered messages\n",
    "- **Why it matters**: Monitors memory leaks or excessive buffering\n",
    "\n",
    "#### NATS Statistics (Bottom Right Panel)\n",
    "\n",
    "**`nats_varz_subscriptions`** - Active subscriptions\n",
    "- **What it shows**: Number of topics/subjects that components are subscribed to\n",
    "- **Expected value**: 5-20 subscriptions (depending on number of workers and endpoints)\n",
    "- **Why it matters**: Each Dynamo service registers subscriptions for the requests it can handle\n",
    "\n",
    "**`nats_server_total_messages`** - Total messages processed\n",
    "- **What it shows**: Cumulative count of all messages since NATS started\n",
    "- **Expected value**: Increases steadily under load\n",
    "- **Why it matters**: Overall message volume indicator\n",
    "\n",
    "**`nats_server_total_streams`** - JetStream streams\n",
    "- **What it shows**: Number of persistent message streams\n",
    "- **Expected value**: Usually 0-2 for Grove (depends on configuration)\n",
    "- **Why it matters**: JetStream provides message persistence and replay capabilities\n",
    "\n",
    "#### Interpreting the Dashboard\n",
    "\n",
    "**Healthy State**:\n",
    "- âœ… Connections: 2-4 (workers + frontend connected)\n",
    "- âœ… Message rates: Correlated with request traffic (metadata only)\n",
    "- âœ… CPU: < 20%\n",
    "- âœ… Memory: Stable, not growing continuously\n",
    "- âœ… Subscriptions: Non-zero (services registered)\n",
    "\n",
    "**Problem Indicators**:\n",
    "- âš ï¸ Connections: 0 when deployment exists â†’ connectivity issue\n",
    "- âš ï¸ Message rate: Out > In by large margin â†’ message amplification/looping\n",
    "- âš ï¸ CPU: Sustained > 80% â†’ NATS bottleneck\n",
    "- âš ï¸ Memory: Continuously growing â†’ memory leak or message backlog\n",
    "- âš ï¸ Subscriptions: 0 â†’ services not registering with NATS\n",
    "\n",
    "**Important Note**: NATS message volume does NOT reflect KV cache data transfer volume. NIXL handles the heavy tensor data transfer (gigabytes) separately via RDMA/TCP.\n",
    "\n",
    "### Step 3: Understanding etcd Metrics\n",
    "\n",
    "Key etcd metrics to monitor:\n",
    "\n",
    "**Health Metrics**:\n",
    "- `etcd_server_has_leader` - Whether cluster has a leader (should be 1)\n",
    "- `etcd_server_is_leader` - Whether this instance is the leader\n",
    "\n",
    "**Performance Metrics**:\n",
    "- `etcd_mvcc_db_total_size_in_bytes` - Database size\n",
    "- `rate(etcd_server_proposals_committed_total[5m])` - Proposal commit rate\n",
    "\n",
    "**Operation Metrics**:\n",
    "- `etcd_debugging_mvcc_put_total` - Total PUT operations\n",
    "- `etcd_debugging_mvcc_range_total` - Total GET operations\n",
    "\n",
    "### Step 4: Test Distributed Dynamo with Traffic\n",
    "\n",
    "Generate meaningful traffic to see distributed coordination in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f822a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Generate test traffic with concurrent requests\n",
    "echo \"Generating traffic to distributed Dynamo deployment...\"\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Function to send a request\n",
    "send_request() {\n",
    "    local id=$1\n",
    "    curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d \"{\n",
    "        \\\"model\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\",\n",
    "        \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Explain distributed systems in 2 sentences. Request $id\\\"}],\n",
    "        \\\"stream\\\": false,\n",
    "        \\\"max_tokens\\\": 100\n",
    "      }\" > /dev/null 2>&1\n",
    "}\n",
    "\n",
    "# Send 30 requests with 3 concurrent workers\n",
    "echo \"Sending 30 requests with 3 concurrent connections...\"\n",
    "echo \"This will generate metrics for:\"\n",
    "echo \"  - NATS coordination message throughput\"\n",
    "echo \"  - Worker utilization across 2 workers\"\n",
    "echo \"  - KV-aware request routing\"\n",
    "echo \"\"\n",
    "\n",
    "for i in {1..10}; do\n",
    "    send_request $((i*3-2)) &\n",
    "    send_request $((i*3-1)) &\n",
    "    send_request $((i*3)) &\n",
    "    wait\n",
    "    echo \"Batch $i/10 complete (requests $((i*3-2))-$((i*3)))\"\n",
    "    sleep 0.5\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Sent 30 requests with concurrent load\"\n",
    "echo \"\"\n",
    "echo \"Check metrics in Grafana:\"\n",
    "echo \"  - Dynamo Inference: Request throughput, TTFT, ITL across workers\"\n",
    "echo \"  - NATS Overview: Coordination message rates (metadata only)\"\n",
    "echo \"  - etcd Overview: Service discovery operations\"\n",
    "echo \"\"\n",
    "echo \"View Grafana: https://grafana0-$(hostname | sed 's/^brev-//').brevlab.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d8df9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Understanding Distributed Dynamo Trade-offs\n",
    "\n",
    "### Single-Node vs Multi-Node\n",
    "\n",
    "**Single Node with Multiple GPUs (Typical Dev Setup)**:\n",
    "```\n",
    "âœ“ KV-aware routing still beneficial (routes to worker with cached data)\n",
    "âœ“ Learning opportunity to understand architecture\n",
    "âœ“ Workers can share cache blocks via NIXL locally\n",
    "âœ— Less dramatic network benefits (same machine)\n",
    "âœ— Additional resource overhead (NATS + etcd)\n",
    "```\n",
    "\n",
    "**Multi-Node (Production)**:\n",
    "```\n",
    "âœ“ KV-aware Router directs requests to nodes with relevant cache\n",
    "âœ“ NIXL transfers cache data efficiently (RDMA/TCP between nodes)\n",
    "âœ“ Improved cache hit rates = lower latency\n",
    "âœ“ Better resource utilization across cluster\n",
    "âœ“ Enables advanced features (cache migration, load balancing)\n",
    "âœ— Network latency between nodes\n",
    "âœ— Increased complexity in debugging\n",
    "```\n",
    "\n",
    "### Performance Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Display performance comparison\n",
    "cat <<'EOF'\n",
    "\n",
    "Performance Impact of Distributed Dynamo:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Metric              â”‚ Single Node      â”‚ Multi-Node   â”‚\n",
    "â”‚                     â”‚ (Multi-GPU)      â”‚              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Cache Hit Rate      â”‚ +10-20%          â”‚ +20-40%      â”‚\n",
    "â”‚ Latency (P50)       â”‚ +2-5ms           â”‚ +2-5ms       â”‚\n",
    "â”‚ Latency (P99)       â”‚ +5-10ms          â”‚ +5-10ms      â”‚\n",
    "â”‚ Throughput          â”‚ Same to +10%     â”‚ +30-60%      â”‚\n",
    "â”‚ Memory Overhead     â”‚ +100-200MB       â”‚ +100-200MB   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "When Distributed Dynamo Helps Most:\n",
    "  â€¢ Multiple GPUs or nodes with high traffic\n",
    "  â€¢ Repeated queries (high cache hit potential)\n",
    "  â€¢ Long context lengths (expensive to recompute)\n",
    "  â€¢ Batch processing workloads\n",
    "\n",
    "When It May Not Help:\n",
    "  â€¢ Single GPU deployments\n",
    "  â€¢ Unique queries every time (low cache hit rate)\n",
    "  â€¢ Very short context lengths\n",
    "  â€¢ Real-time streaming with completely unique prompts\n",
    "\n",
    "Architecture Notes:\n",
    "  â€¢ Grove = Kubernetes Operator (orchestration)\n",
    "  â€¢ Dynamo = Serving Framework (actual inference)\n",
    "  â€¢ NATS = Metadata/coordination (small messages)\n",
    "  â€¢ NIXL = KV cache data transfer (large tensors via RDMA/TCP)\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89b759",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Advanced Distributed Features\n",
    "\n",
    "### Cache Monitoring\n",
    "\n",
    "Check distributed coordination through worker logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4999b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get cache stats from worker logs\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "WORKER_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker,nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)\n",
    "\n",
    "if [ -n \"$WORKER_POD\" ]; then\n",
    "    echo \"Checking NIXL/NATS activity in worker logs...\"\n",
    "    echo \"\"\n",
    "    kubectl logs -n $NAMESPACE $WORKER_POD --tail=100 | grep -i \"nixl\\|nats\" | tail -10\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"Worker pod: $WORKER_POD\"\n",
    "    echo \"\"\n",
    "    echo \"What to look for:\"\n",
    "    echo \"  - NIXL initialization messages (KV cache transfer setup)\"\n",
    "    echo \"  - NATS connection status (coordination layer)\"\n",
    "    echo \"  - KV cache registration events\"\n",
    "    echo \"  - UCX backend messages (if using RDMA for cache transfer)\"\n",
    "else\n",
    "    echo \"âš ï¸ No worker pods found\"\n",
    "    echo \"Make sure the vllm-distributed-demo deployment is running\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8c31c",
   "metadata": {},
   "source": [
    "**Note**: Cache hit/miss metrics depend on workload patterns. Even on a single node with multiple GPUs, KV-aware routing can improve cache hits by directing requests to the worker that already has relevant cache blocks.\n",
    "\n",
    "### NATS Health Check\n",
    "\n",
    "Verify NATS is functioning correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS service health\n",
    "echo \"Checking NATS health...\"\n",
    "kubectl exec -n nats-system nats-0 -- nats-server --version 2>/dev/null || kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"NATS endpoints:\"\n",
    "kubectl get svc -n nats-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc66cf",
   "metadata": {},
   "source": [
    "### etcd Health Check\n",
    "\n",
    "Verify etcd cluster health:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75991f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check etcd health\n",
    "echo \"Checking etcd health...\"\n",
    "ETCD_POD=$(kubectl get pods -n etcd-system -l app.kubernetes.io/name=etcd -o jsonpath='{.items[0].metadata.name}')\n",
    "\n",
    "if [ -n \"$ETCD_POD\" ]; then\n",
    "    kubectl exec -n etcd-system $ETCD_POD -- etcdctl endpoint health 2>/dev/null || echo \"etcd health check requires auth setup\"\n",
    "    echo \"\"\n",
    "    kubectl exec -n etcd-system $ETCD_POD -- etcdctl member list 2>/dev/null || echo \"etcd member list requires auth setup\"\n",
    "else\n",
    "    echo \"âš ï¸ No etcd pods found\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"etcd endpoints:\"\n",
    "kubectl get svc -n etcd-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b6fd43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Cleanup\n",
    "\n",
    "### Step 1: Remove Distributed Demo Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399379a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete the distributed deployment\n",
    "echo \"Removing distributed Dynamo deployment...\"\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "kubectl delete dynamographdeployment vllm-distributed-demo -n $NAMESPACE\n",
    "kubectl delete svc vllm-distributed-demo-frontend-np -n $NAMESPACE\n",
    "\n",
    "echo \"âœ“ Distributed deployment removed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ce734",
   "metadata": {},
   "source": [
    "### Step 2: Verify Lab 1 Deployment is Still Running\n",
    "\n",
    "Your original Lab 1 deployment should still be running on port 30100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a874d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check Lab 1 deployment status\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Checking Lab 1 deployment...\"\n",
    "kubectl get dynamographdeployment vllm-disagg-router -n $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"Lab 1 pods:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-disagg-router\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Lab 1 deployment is available at: http://$NODE_IP:30100\"\n",
    "echo \"\"\n",
    "echo \"Test it:\"\n",
    "echo \"  curl http://$NODE_IP:30100/v1/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2f41a7",
   "metadata": {},
   "source": [
    "### Step 3: Remove Distributed Infrastructure (Optional)\n",
    "\n",
    "Only remove NATS and etcd if you're done experimenting with distributed Dynamo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181cfaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove NATS\n",
    "echo \"Removing NATS...\"\n",
    "helm uninstall nats -n nats-system\n",
    "kubectl delete namespace nats-system\n",
    "\n",
    "# Remove etcd  \n",
    "echo \"Removing etcd...\"\n",
    "helm uninstall etcd -n etcd-system\n",
    "kubectl delete namespace etcd-system\n",
    "\n",
    "# Remove PodMonitors\n",
    "kubectl delete podmonitor nats -n nats-system 2>/dev/null || true\n",
    "kubectl delete podmonitor etcd -n etcd-system 2>/dev/null || true\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Distributed infrastructure removed\"\n",
    "echo \"\"\n",
    "echo \"Note: You can reinstall NATS/etcd anytime by re-running Section 2 of this lab\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11bc532",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- âœ… Distributed Dynamo architecture and components (NATS, etcd, NIXL)\n",
    "- âœ… Difference between Grove (operator) and Dynamo (serving framework)\n",
    "- âœ… Deploying distributed coordination infrastructure\n",
    "- âœ… Creating a distributed Dynamo deployment with KV-aware routing\n",
    "- âœ… Monitoring NATS and etcd with Grafana\n",
    "- âœ… Understanding NATS (metadata) vs NIXL (KV cache data transfer)\n",
    "- âœ… Trade-offs between single-node and multi-node setups\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Architecture Clarity**:\n",
    "- **Grove**: Kubernetes Operator (orchestrates Dynamo deployments)\n",
    "- **Dynamo**: Inference serving framework (does the actual work)\n",
    "- **NATS**: Handles coordination metadata and cache events (small messages)\n",
    "- **NIXL**: Transfers actual KV cache data (gigabytes via RDMA/TCP/SSD)\n",
    "\n",
    "**Distributed Dynamo is Powerful**:\n",
    "- Enables KV-aware routing (even on single node with multiple GPUs)\n",
    "- NIXL transfers cache data efficiently between workers\n",
    "- Improves cache hit rates and throughput\n",
    "- Essential for production scale-out scenarios\n",
    "\n",
    "**Benefits Even on Single Node with Multiple GPUs**:\n",
    "- KV-aware Router directs requests to workers with relevant cache\n",
    "- Improved cache hit rates compared to random routing\n",
    "- Coordination overhead is minimal with NATS\n",
    "\n",
    "**Production Considerations**:\n",
    "- Use distributed Dynamo when scaling beyond single GPU\n",
    "- Monitor NATS message rates for coordination health (not data volume)\n",
    "- Plan for network latency between nodes in multi-node setups\n",
    "- Consider cache hit patterns for your workload\n",
    "- Dynamo 0.8+ supports K8s-native discovery (optional NATS/etcd)\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**When Companies Use Distributed Dynamo**:\n",
    "- Multi-region LLM deployments\n",
    "- High-traffic serving (1000+ RPS)\n",
    "- Multi-GPU and multi-node clusters\n",
    "- Cost optimization (share expensive cache computation)\n",
    "- Enterprise multi-tenant platforms\n",
    "\n",
    "**Deployment Options**:\n",
    "- **Single GPU**: No distributed coordination needed\n",
    "- **Multiple GPUs, single node**: Distributed Dynamo with KV-aware routing beneficial\n",
    "- **Small clusters (2-5 nodes)**: Distributed Dynamo provides clear benefits\n",
    "- **Large clusters (10+ nodes)**: Distributed Dynamo essential for coordination\n",
    "- **Dynamo 0.8+**: Can use K8s-native discovery for simpler deployments\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Experiment**: Try different worker replica counts to see KV-aware routing\n",
    "- **Monitor**: Watch NATS/etcd dashboards during traffic (coordination metadata)\n",
    "- **Compare**: Deploy same model without NATS/etcd and compare metrics\n",
    "- **Scale**: If you have access to multi-node clusters, test distributed benefits\n",
    "- **Learn**: Understand NIXL for KV cache data transfer in Dynamo docs\n",
    "- **Explore**: Check out Dynamo 0.8+ features (K8s-native discovery)\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### NATS Not Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ddf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS pods\n",
    "kubectl get pods -n nats-system\n",
    "kubectl logs -n nats-system nats-0\n",
    "\n",
    "# Common issues:\n",
    "# - Insufficient resources (need ~256MB RAM)\n",
    "# - Port conflicts (4222 already in use)\n",
    "# - PersistentVolume issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e1086",
   "metadata": {},
   "source": [
    "### etcd Not Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112cfd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check etcd pods\n",
    "kubectl get pods -n etcd-system\n",
    "kubectl logs -n etcd-system etcd-0\n",
    "\n",
    "# Common issues:\n",
    "# - Insufficient resources (need ~512MB RAM)\n",
    "# - Volume mounting issues\n",
    "# - Network policies blocking ports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12848f",
   "metadata": {},
   "source": [
    "### Workers Not Connecting to Distributed Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ad4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check worker logs for NATS/NIXL connection messages\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "kubectl logs -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker | grep -i \"nats\\|nixl\"\n",
    "\n",
    "# Verify NATS/etcd service endpoints are correct\n",
    "kubectl get svc -n nats-system\n",
    "kubectl get svc -n etcd-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc3365",
   "metadata": {},
   "source": [
    "### No Cache Sharing Observed\n",
    "\n",
    "**This is normal behavior!** Understanding what's actually happening:\n",
    "\n",
    "**What NATS Does** (visible in metrics):\n",
    "- Shares metadata about cache state between workers\n",
    "- Enables KV-aware routing (Router knows which worker has which cache blocks)\n",
    "- Low message volume (small coordination packets)\n",
    "\n",
    "**What NIXL Does** (not visible in NATS metrics):\n",
    "- Transfers actual KV cache data (gigabytes of tensors)\n",
    "- Uses RDMA, TCP, or CPU/SSD offload\n",
    "- Direct worker-to-worker communication\n",
    "\n",
    "**On Single Node**:\n",
    "- Workers can still benefit from KV-aware routing\n",
    "- Cache transfers via NIXL are faster (no network)\n",
    "- NATS provides coordination, not data transfer\n",
    "\n",
    "**Benefits Require**:\n",
    "- Multiple workers (even on same node)\n",
    "- Repeated queries with shared prefixes\n",
    "- Workload that generates cache hits\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **Dynamo Deployment Guide**: https://docs.nvidia.com/dynamo/latest/guides/dynamo_deploy/\n",
    "- **Grove Operator Guide**: https://docs.nvidia.com/dynamo/latest/guides/dynamo_deploy/grove.html\n",
    "- **Grove GitHub Repository**: https://github.com/NVIDIA/grove\n",
    "- **NIXL Documentation**: NVIDIA Inference Transfer Library (check Dynamo docs)\n",
    "- **NATS Documentation**: https://docs.nats.io/\n",
    "- **etcd Documentation**: https://etcd.io/docs/\n",
    "- **NVIDIA Dynamo Documentation**: https://docs.nvidia.com/dynamo/latest/\n",
    "- **Distributed Systems Patterns**: Understanding consensus and coordination\n",
    "- **KV Cache Architecture**: Understanding distributed cache strategies\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've completed Lab 3: Distributed Dynamo with Grove Orchestration** ğŸŒ²\n",
    "\n",
    "You now understand the fundamentals of distributed LLM serving, the difference between Grove (operator) and Dynamo (serving framework), and how NATS (metadata) and NIXL (data transfer) work together for distributed coordination!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-kernelspec,-widgets,-language_info"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
