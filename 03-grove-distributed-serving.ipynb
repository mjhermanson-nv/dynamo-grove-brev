{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030997f8",
   "metadata": {},
   "source": [
    "# Lab 3: Distributed Dynamo with Multi-GPU/Multi-Node Serving\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you will:\n",
    "- **Primary Path:** Deploy distributed Dynamo using K8s-native discovery (simplified, v0.8.0)\n",
    "- Understand multi-GPU and multi-node serving architectures\n",
    "- Enable distributed KV cache awareness and transfer via NIXL\n",
    "- Monitor distributed components with Grafana\n",
    "- **Optional Advanced:** Deploy with NATS/etcd for extreme scale (multi-region, 100+ nodes)\n",
    "\n",
    "**Prerequisites**: Complete Lab 1 (Dynamo Deployment) and Lab 2 (Monitoring)\n",
    "\n",
    "**What's New in v0.8.0:**\n",
    "- âœ… K8s-native discovery (EndpointSlices) - no etcd needed\n",
    "- âœ… TCP transport - no NATS needed\n",
    "- âœ… Simpler deployment for 2-50 node clusters\n",
    "- âœ… NATS/etcd now optional for extreme scale only\n",
    "\n",
    "**Note**: Distributed Dynamo is designed for multi-node Kubernetes clusters or single nodes with multiple GPUs. While we'll deploy it on a single node for learning purposes, maximum benefits are realized when scaling across multiple nodes with high cache hit workloads.\n",
    "\n",
    "## Duration: ~45 minutes (K8s-native path) / ~75 minutes (with optional NATS/etcd)\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Understanding Distributed Dynamo Architecture\n",
    "\n",
    "### What is Grove vs Dynamo?\n",
    "\n",
    "**Dynamo** is NVIDIA's inference serving framework (the Python code, Router, Frontend, Workers).\n",
    "\n",
    "**Grove** is the Kubernetes Operator that orchestrates Dynamo deployments (handling CRDs like `DynamoGraphDeployment`, pod gangs, startup order).\n",
    "\n",
    "**Distributed Dynamo** (orchestrated by Grove) enables:\n",
    "- **Multi-node deployments** across Kubernetes clusters or multi-GPU single nodes\n",
    "- **KV-aware routing** where the Router knows which worker has which cache blocks\n",
    "- **Distributed KV cache transfer** between workers via NIXL (NVIDIA Inference Transfer Library)\n",
    "- **Coordination and discovery** using either:\n",
    "  - **K8s-native (v0.8.0+)**: EndpointSlices + TCP (simpler, recommended for most use cases)\n",
    "  - **NATS/etcd (optional)**: For extreme scale (100+ nodes, multi-region, complex topologies)\n",
    "\n",
    "### Architecture: K8s-Native (Recommended for Most Users)\n",
    "\n",
    "```\n",
    "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "               â”‚  Cloud Load Balancer       â”‚\n",
    "               â”‚  or Ingress Controller     â”‚\n",
    "               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚                â”‚\n",
    "    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "    â”‚Frontend 1â”‚    â”‚Frontend 2â”‚    â”‚Frontend 3â”‚\n",
    "    â”‚ (Node 1) â”‚    â”‚ (Node 2) â”‚    â”‚ (Node 3) â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                â”‚                â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚ Kubernetes            â”‚\n",
    "              â”‚  - EndpointSlices     â”‚\n",
    "              â”‚    (Discovery)        â”‚\n",
    "              â”‚  - TCP (Transport)    â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚                â”‚\n",
    "    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Worker 1 â”‚    â”‚ Worker 2 â”‚    â”‚ Worker 3 â”‚\n",
    "    â”‚ (Node 4) â”‚    â”‚ (Node 5) â”‚    â”‚ (Node 6) â”‚\n",
    "    â”‚  +GPU    â”‚    â”‚  +GPU    â”‚    â”‚  +GPU    â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                â”‚                â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  NIXL (KV Cache       â”‚\n",
    "              â”‚   Data Transfer)      â”‚\n",
    "              â”‚  RDMA/TCP/SSD         â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Benefits:\n",
    "- Simpler: No additional infrastructure (NATS/etcd)\n",
    "- Lower latency: Direct TCP connections\n",
    "- Easier ops: Fewer moving parts\n",
    "- Sufficient for 2-50 node clusters\n",
    "\n",
    "Note: Workers typically run on GPU nodes (4-6), separate from\n",
    "      CPU-only frontend nodes (1-3). In smaller clusters, they\n",
    "      may share nodes with frontends.\n",
    "```\n",
    "\n",
    "### Architecture: NATS/etcd (Optional - For Extreme Scale)\n",
    "\n",
    "```\n",
    "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "               â”‚  Cloud Load Balancer       â”‚\n",
    "               â”‚  or Ingress Controller     â”‚\n",
    "               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚                â”‚\n",
    "    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "    â”‚Frontend 1â”‚    â”‚Frontend 2â”‚    â”‚Frontend 3â”‚\n",
    "    â”‚ (Node 1) â”‚    â”‚ (Node 2) â”‚    â”‚ (Node 3) â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                â”‚                â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  NATS Message Bus     â”‚\n",
    "              â”‚  (Metadata, Routing,  â”‚\n",
    "              â”‚   Cache Awareness)    â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  etcd (Coordination)  â”‚\n",
    "              â”‚  (Service Discovery)  â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚                â”‚\n",
    "    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Worker 1 â”‚    â”‚ Worker 2 â”‚    â”‚ Worker 3 â”‚\n",
    "    â”‚ (Node 4) â”‚    â”‚ (Node 5) â”‚    â”‚ (Node 6) â”‚\n",
    "    â”‚  +GPU    â”‚    â”‚  +GPU    â”‚    â”‚  +GPU    â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                â”‚                â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  NIXL (KV Cache       â”‚\n",
    "              â”‚   Data Transfer)      â”‚\n",
    "              â”‚  RDMA/TCP/SSD         â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "When to use NATS/etcd:\n",
    "- 100+ node clusters\n",
    "- Multi-region deployments\n",
    "- Complex custom routing logic\n",
    "- Advanced cache policies\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Kubernetes-native Discovery (v0.8.0+)**: Built-in service discovery:\n",
    "- Uses EndpointSlices (standard Kubernetes API)\n",
    "- Workers register with K8s API server automatically\n",
    "- Frontends watch EndpointSlices for worker availability\n",
    "- No additional infrastructure required\n",
    "\n",
    "**TCP Transport (v0.8.0+ default)**: Direct worker communication:\n",
    "- Frontends connect to workers via TCP\n",
    "- Lower latency than pub/sub patterns\n",
    "- Simpler debugging with standard networking tools\n",
    "\n",
    "**NIXL (NVIDIA Inference Transfer Library)**: Handles actual KV cache data transfer:\n",
    "- Uses high-speed transports (RDMA, TCP, or CPU/SSD offload)\n",
    "- Transfers gigabytes of tensor data between workers\n",
    "- Direct worker-to-worker communication\n",
    "- Works with both K8s-native and NATS/etcd modes\n",
    "\n",
    "**KV-Aware Routing**: The Router knows which worker has which cache blocks:\n",
    "- In K8s-native mode: Routing metadata shared via API or direct communication\n",
    "- In NATS mode: NATS shares metadata about cache state\n",
    "- Enables intelligent request routing to workers with relevant cached data\n",
    "- Dramatically reduces prefill latency when cache hits occur\n",
    "\n",
    "**Optional NATS/etcd (for extreme scale)**: Advanced coordination:\n",
    "- **NATS**: Pub/sub messaging for metadata (cache events, routing tables)\n",
    "- **etcd**: Distributed configuration and service discovery\n",
    "- **When to use**: 100+ nodes, multi-region, custom routing policies\n",
    "- **Note**: NATS does NOT transfer KV cache data (NIXL does that)\n",
    "- Router directs requests to workers with relevant cached prefixes\n",
    "- Improves cache hit rates even on single node with multiple GPUs\n",
    "- Workers transfer actual cache data via NIXL when needed\n",
    "\n",
    "### How Multiple Frontends Work\n",
    "\n",
    "The architecture diagram shows 2 frontends, but **frontend replicas â‰  one per node**. Here's how it actually works in production:\n",
    "\n",
    "**Frontend Scaling Strategy**:\n",
    "```\n",
    "Small cluster (3 nodes):    2-3 frontend replicas\n",
    "Medium cluster (10 nodes):  3-5 frontend replicas\n",
    "Large cluster (50+ nodes):  5-10 frontend replicas\n",
    "```\n",
    "\n",
    "**Load Balancing via Kubernetes Service**:\n",
    "\n",
    "When you create a Service (NodePort or LoadBalancer), Kubernetes automatically load balances across all frontend pods:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: frontend-service\n",
    "spec:\n",
    "  type: LoadBalancer  # or NodePort\n",
    "  selector:\n",
    "    component: frontend  # Selects ALL frontend pods\n",
    "  ports:\n",
    "  - port: 8000\n",
    "```\n",
    "\n",
    "**How Traffic Flows**:\n",
    "1. **External Load Balancer** (cloud provider or Ingress) receives request\n",
    "2. **Kubernetes Service** load balances to any frontend pod\n",
    "3. **Frontend** sends inference request via NATS\n",
    "4. **NATS** routes to an available worker (KV-aware routing if enabled)\n",
    "5. **Worker** may receive KV cache data from another worker via NIXL\n",
    "6. **Worker** responds via NATS\n",
    "7. **Frontend** returns HTTP response\n",
    "\n",
    "**Key Benefits**:\n",
    "- âœ… **High Availability**: If one frontend crashes, others continue\n",
    "- âœ… **Load Distribution**: Spread HTTP connections across pods\n",
    "- âœ… **Dynamic Discovery**: NATS decouples frontends from workers (Dynamo 0.7.x requires NATS/etcd; 0.8+ supports K8s-native discovery)\n",
    "- âœ… **Flexible Scaling**: Add/remove frontends independently\n",
    "- âœ… **KV-Aware Routing**: Route requests to workers with relevant cached data\n",
    "\n",
    "**Single Node (This Lab)**:\n",
    "Even in a single-node setup with multiple GPUs/workers, KV-aware routing provides benefits! The Router uses NATS to track which worker has which cached prefixes, directing requests to the worker with the best cache hit potential.\n",
    "\n",
    "### When to Use Distributed Dynamo\n",
    "\n",
    "| Scenario | Use Distributed Dynamo? | Why |\n",
    "|----------|-----------|-----|\n",
    "| Single GPU | âŒ No | Adds overhead without benefit |\n",
    "| Multiple GPUs, single node | âœ… Yes | KV-aware routing improves cache hits between GPU workers |\n",
    "| 2-3 nodes | âœ… Yes | Cache awareness and coordination provide benefits |\n",
    "| 4+ nodes | âœ… Strongly Yes | Significant performance improvements from distributed cache awareness |\n",
    "| High traffic, repeated queries | âœ… Yes | Cache-aware routing reduces latency |\n",
    "| Low traffic, unique queries | âš ï¸ Maybe | Lower cache hit rates, but coordination still useful |\n",
    "| Dynamo 0.8+ | â„¹ï¸ Info | Can use K8s-native discovery (no NATS/etcd required) for simple deployments |\n",
    "\n",
    "---\n",
    "\n",
    "## Section 2: Deploy Distributed Dynamo (K8s-Native)\n",
    "\n",
    "### âš ï¸ IMPORTANT: Choose Your Deployment Mode\n",
    "\n",
    "**For v0.8.0, we recommend K8s-native mode (simpler, no extra infrastructure):**\n",
    "- **K8s-Native Path**: Skip Steps 2-4 below and go directly to Section 3\n",
    "- **NATS/etcd Path** (100+ nodes only): Continue with Steps 2-4\n",
    "\n",
    "### Overview\n",
    "\n",
    "**K8s-Native Mode (Recommended)**:\n",
    "- No NATS or etcd installation required\n",
    "- Uses Kubernetes EndpointSlices for discovery\n",
    "- TCP transport (default)\n",
    "- Sufficient for 2-50 node clusters\n",
    "\n",
    "**NATS/etcd Mode (Optional Advanced)**:\n",
    "- Requires Steps 2-4 below\n",
    "- For 100+ nodes, multi-region, custom routing\n",
    "- See release notes for configuration details\n",
    "\n",
    "### Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables (use defaults if not already set)\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ğŸŒ² Lab 3: Distributed Dynamo Configuration\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Release Version:  $RELEASE_VERSION\"\n",
    "echo \"  Namespace:        $NAMESPACE\"\n",
    "echo \"  Node IP:          $NODE_IP\"\n",
    "echo \"\"\n",
    "echo \"âœ“ Environment configured for distributed Dynamo setup\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be0de7",
   "metadata": {},
   "source": [
    "### Step 2: Install NATS Message Bus\n",
    "\n",
    "NATS handles distributed coordination metadata between Dynamo components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b00a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for NATS\n",
    "kubectl create namespace nats-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Add NATS Helm repository\n",
    "echo \"Adding NATS Helm repository...\"\n",
    "helm repo add nats https://nats-io.github.io/k8s/helm/charts/\n",
    "helm repo update\n",
    "\n",
    "# Install NATS (with Prometheus exporter)\n",
    "echo \"Installing NATS with metrics exporter...\"\n",
    "helm upgrade --install nats nats/nats \\\n",
    "  --namespace nats-system \\\n",
    "  --set config.jetstream.enabled=true \\\n",
    "  --set config.jetstream.fileStore.pvc.size=1Gi \\\n",
    "  --set promExporter.enabled=true \\\n",
    "  --set promExporter.port=7777 \\\n",
    "  --wait \\\n",
    "  --timeout 5m\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ NATS installed successfully\"\n",
    "echo \"  Connection: nats://nats.nats-system:4222\"\n",
    "echo \"  Metrics: Port 7777\"\n",
    "echo \"\"\n",
    "echo \"Note: NATS handles metadata (cache events, routing tables).\"\n",
    "echo \"      Actual KV cache data transfers via NIXL (RDMA/TCP).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc74a6",
   "metadata": {},
   "source": [
    "### Step 3: Install etcd Coordination Layer\n",
    "\n",
    "etcd provides distributed coordination for Grove components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd478fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for etcd\n",
    "kubectl create namespace etcd-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Add Bitnami Helm repository\n",
    "echo \"Adding Bitnami Helm repository...\"\n",
    "helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "helm repo update\n",
    "\n",
    "# Install etcd (using legacy Bitnami mirror)\n",
    "echo \"Installing etcd...\"\n",
    "helm upgrade --install etcd bitnami/etcd \\\n",
    "  --namespace etcd-system \\\n",
    "  --set replicaCount=1 \\\n",
    "  --set auth.rbac.create=false \\\n",
    "  --set image.registry=docker.io \\\n",
    "  --set image.repository=bitnamilegacy/etcd \\\n",
    "  --set persistence.size=1Gi \\\n",
    "  --set preUpgradeHook.enabled=false \\\n",
    "  --wait \\\n",
    "  --timeout 5m\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ etcd installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514b741",
   "metadata": {},
   "source": [
    "### Step 4: Verify Grove Infrastructure\n",
    "\n",
    "Check that NATS and etcd are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS pods\n",
    "echo \"Checking NATS deployment...\"\n",
    "kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking NATS service...\"\n",
    "kubectl get svc -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "\n",
    "# Check etcd pods\n",
    "echo \"Checking etcd deployment...\"\n",
    "kubectl get pods -n etcd-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking etcd service...\"\n",
    "kubectl get svc -n etcd-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Grove infrastructure verified\"\n",
    "echo \"  NATS:  nats://nats.nats-system:4222 (metadata/coordination)\"\n",
    "echo \"  etcd:  http://etcd.etcd-system:2379 (service discovery)\"\n",
    "echo \"  NIXL will handle KV cache data transfer between workers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b781d",
   "metadata": {},
   "source": [
    "### Step 4: Enable Prometheus Monitoring\n",
    "\n",
    "Create PodMonitors so Prometheus can scrape NATS and etcd metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009417fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create PodMonitor for NATS (scrapes the prometheus-nats-exporter sidecar)\n",
    "echo \"Enabling NATS metrics collection...\"\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PodMonitor\n",
    "metadata:\n",
    "  name: nats\n",
    "  namespace: nats-system\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/name: nats\n",
    "  podMetricsEndpoints:\n",
    "  - port: prom-metrics\n",
    "    path: /metrics\n",
    "EOF\n",
    "\n",
    "# Create PodMonitor for etcd\n",
    "echo \"Enabling etcd metrics collection...\"\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PodMonitor\n",
    "metadata:\n",
    "  name: etcd\n",
    "  namespace: etcd-system\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/name: etcd\n",
    "  podMetricsEndpoints:\n",
    "  - port: client\n",
    "    path: /metrics\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Prometheus monitoring enabled for distributed infrastructure\"\n",
    "echo \"  Metrics will be available in Grafana within 2-3 minutes\"\n",
    "echo \"\"\n",
    "echo \"  NATS metrics: Scraped from prometheus-nats-exporter (port: prom-metrics)\"\n",
    "echo \"  etcd metrics: Scraped directly from etcd's /metrics endpoint (port: client)\"\n",
    "echo \"\"\n",
    "echo \"Note: etcd metrics typically appear faster than NATS metrics\"\n",
    "echo \"      NATS metrics show coordination traffic, not KV cache data volume\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ba079",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Deploy Distributed Dynamo Model\n",
    "\n",
    "### Understanding Dynamo's Distributed Architecture\n",
    "\n",
    "Dynamo (orchestrated by Grove) automatically uses NATS and etcd for distributed coordination when they are available in the cluster. The deployment will:\n",
    "\n",
    "**1. Workers register via NATS**: Each worker announces itself and its cache state\n",
    "**2. Frontend discovers workers**: The frontend finds workers through NATS service discovery\n",
    "**3. KV-aware Router**: Routes requests to workers with relevant cached data\n",
    "**4. NIXL handles KV cache data**: Workers transfer actual KV cache tensors via NIXL (RDMA/TCP), not through NATS\n",
    "\n",
    "**Note**: In Dynamo 0.8+, Kubernetes-native discovery (EndpointSlices) is available as an alternative to NATS/etcd for simpler deployments without KV-aware routing.\n",
    "\n",
    "### Step 1: Create Distributed Dynamo Deployment\n",
    "\n",
    "We'll create a deployment with 2 workers to demonstrate distributed architecture and KV-aware routing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac440aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create distributed Dynamo deployment\n",
    "echo \"Creating distributed Dynamo deployment with 2 workers...\"\n",
    "\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: nvidia.com/v1alpha1\n",
    "kind: DynamoGraphDeployment\n",
    "metadata:\n",
    "  name: vllm-distributed-demo\n",
    "  namespace: dynamo\n",
    "spec:\n",
    "  services:\n",
    "    Frontend:\n",
    "      dynamoNamespace: vllm-distributed-demo\n",
    "      componentType: frontend\n",
    "      replicas: 1\n",
    "      extraPodSpec:\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.7.1\n",
    "    VllmWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-distributed-demo\n",
    "      componentType: worker\n",
    "      replicas: 2\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: info\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: /data/huggingface-cache\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.7.1\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --tensor-parallel-size 1\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Distributed Dynamo deployment created\"\n",
    "echo \"  Deployment: vllm-distributed-demo\"\n",
    "echo \"  Workers: 2 (will use NATS for coordination and NIXL for cache transfer)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7808e",
   "metadata": {},
   "source": [
    "### Step 2: Create NodePort Service\n",
    "\n",
    "Expose the frontend for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a444ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create NodePort service\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: vllm-distributed-demo-frontend-np\n",
    "  namespace: dynamo\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    nvidia.com/dynamo-component: Frontend\n",
    "    nvidia.com/dynamo-graph-deployment-name: vllm-distributed-demo\n",
    "  ports:\n",
    "  - port: 8000\n",
    "    targetPort: 8000\n",
    "    nodePort: 30200\n",
    "    protocol: TCP\n",
    "    name: http\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ NodePort service created on port 30200\"\n",
    "echo \"  Access at: http://$NODE_IP:30200\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b62695",
   "metadata": {},
   "source": [
    "### Step 3: Wait for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30908968",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Wait for pods to be ready\n",
    "echo \"Waiting for distributed Dynamo deployment...\"\n",
    "echo \"This may take 2-3 minutes for model download and initialization...\"\n",
    "echo \"\"\n",
    "\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Wait for pods to be ready\n",
    "kubectl wait --for=condition=ready --timeout=300s \\\n",
    "  pods -l nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo \\\n",
    "  -n $NAMESPACE 2>/dev/null || echo \"Pods are initializing...\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Deployment status:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Distributed Dynamo deployment ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b01d3",
   "metadata": {},
   "source": [
    "### Step 4: Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Test the deployment\n",
    "echo \"Testing inference...\"\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain distributed inference in one sentence\"}],\n",
    "    \"max_tokens\": 50\n",
    "  }' | python3 -m json.tool\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Distributed Dynamo deployment is serving requests\"\n",
    "echo \"  Router uses NATS for worker coordination\"\n",
    "echo \"  NIXL handles KV cache data transfer between workers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d1a9d5",
   "metadata": {},
   "source": [
    "### Step 5: Verify NATS and NIXL Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check worker logs for NATS connectivity and NIXL initialization\n",
    "echo \"Verifying NATS and NIXL integration...\"\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "WORKER_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker,nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)\n",
    "\n",
    "if [ -n \"$WORKER_POD\" ]; then\n",
    "    echo \"Checking worker: $WORKER_POD\"\n",
    "    kubectl logs -n $NAMESPACE $WORKER_POD 2>&1 | grep -i \"nats\\|nixl\" | head -5\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"âœ“ Workers are using:\"\n",
    "    echo \"  â€¢ NATS for coordination and cache awareness\"\n",
    "    echo \"  â€¢ NIXL for KV cache data transfer (RDMA/TCP/SSD)\"\n",
    "    echo \"\"\n",
    "    echo \"Note: NATS carries metadata (cache events, routing tables).\"\n",
    "    echo \"      NIXL transfers the actual tensor data between workers.\"\n",
    "else\n",
    "    echo \"âš ï¸ No worker pods found. Make sure the deployment is running:\"\n",
    "    kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d87ac3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Monitoring Distributed Components\n",
    "\n",
    "### Step 1: Access Grafana Dashboards\n",
    "\n",
    "The distributed infrastructure dashboards were created during the oneshot.sh bootstrap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get Grafana URL\n",
    "BREV_ID=$(hostname | cut -d'-' -f2)\n",
    "GRAFANA_URL=\"https://grafana0-${BREV_ID}.brevlab.com/\"\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ğŸ“Š Distributed Dynamo Monitoring Dashboards\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Grafana URL: $GRAFANA_URL\"\n",
    "echo \"\"\n",
    "echo \"  Available Dashboards:\"\n",
    "echo \"    â€¢ NATS Overview - Message bus metrics (metadata/coordination)\"\n",
    "echo \"    â€¢ etcd Overview - Service discovery metrics\"\n",
    "echo \"    â€¢ Dynamo Inference Metrics - Model serving metrics\"\n",
    "echo \"\"\n",
    "echo \"ğŸ”— Open Grafana and search for these dashboards\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ea21a",
   "metadata": {},
   "source": [
    "### Step 2: Understanding NATS Metrics\n",
    "\n",
    "The NATS Overview dashboard shows real-time metrics about the message bus that Dynamo uses for distributed coordination metadata (not KV cache data).\n",
    "\n",
    "#### Connection Metrics (Top Left Panel)\n",
    "\n",
    "**`nats_varz_connections`** - Current active connections to NATS\n",
    "- **What it shows**: Number of Dynamo components connected to NATS\n",
    "- **Expected value**: \n",
    "  - With distributed deployment running: 2-4 connections (workers + frontend)\n",
    "  - Without active deployment: 0\n",
    "- **Why it matters**: Each Dynamo component (frontend, workers) maintains a connection to NATS for coordination\n",
    "\n",
    "#### Message Rate Metrics (Top Center Panels)\n",
    "\n",
    "**`rate(nats_varz_in_msgs[1m])`** - Incoming messages per second\n",
    "- **What it shows**: Coordination messages NATS is receiving (cache events, routing metadata)\n",
    "- **Expected value**: \n",
    "  - Idle: Low (< 1 msg/s for heartbeats)\n",
    "  - During load: 10-100+ msg/s depending on request rate\n",
    "- **Why it matters**: Shows the coordination throughput (NOT KV cache data volume)\n",
    "- **Important**: NATS messages are small metadata packets, not gigabytes of tensor data\n",
    "\n",
    "**`rate(nats_varz_out_msgs[1m])`** - Outgoing messages per second\n",
    "- **What it shows**: How many coordination messages NATS is distributing\n",
    "- **Expected value**: Similar to or slightly higher than incoming rate\n",
    "- **Why it matters**: NATS may send multiple copies of messages to subscribers (pub/sub pattern)\n",
    "\n",
    "#### Resource Metrics (Top Right Panel)\n",
    "\n",
    "**`nats_varz_cpu`** - NATS CPU usage percentage\n",
    "- **What it shows**: CPU usage of the NATS process\n",
    "- **Expected value**: \n",
    "  - Idle: < 1%\n",
    "  - Under load: 5-20%\n",
    "  - High load: > 50% (consider scaling)\n",
    "- **Why it matters**: High CPU might indicate NATS is becoming a bottleneck\n",
    "\n",
    "#### Message Rate Graph (Middle Panel)\n",
    "\n",
    "**Time series visualization** of message rates\n",
    "- **Green line**: Incoming messages (rate(nats_varz_in_msgs[1m]))\n",
    "- **Blue line**: Outgoing messages (rate(nats_varz_out_msgs[1m]))\n",
    "- **What to look for**:\n",
    "  - Spikes during traffic bursts\n",
    "  - Correlation between in/out rates\n",
    "  - Steady state during constant load\n",
    "  - Drops to zero when idle\n",
    "\n",
    "#### Memory Usage (Bottom Left Panel)\n",
    "\n",
    "**`nats_varz_mem`** - NATS memory consumption in MB\n",
    "- **What it shows**: RAM used by the NATS process\n",
    "- **Expected value**: \n",
    "  - Base: 20-50 MB\n",
    "  - With JetStream: 50-200 MB\n",
    "  - Under load: May increase with buffered messages\n",
    "- **Why it matters**: Monitors memory leaks or excessive buffering\n",
    "\n",
    "#### NATS Statistics (Bottom Right Panel)\n",
    "\n",
    "**`nats_varz_subscriptions`** - Active subscriptions\n",
    "- **What it shows**: Number of topics/subjects that components are subscribed to\n",
    "- **Expected value**: 5-20 subscriptions (depending on number of workers and endpoints)\n",
    "- **Why it matters**: Each Dynamo service registers subscriptions for the requests it can handle\n",
    "\n",
    "**`nats_server_total_messages`** - Total messages processed\n",
    "- **What it shows**: Cumulative count of all messages since NATS started\n",
    "- **Expected value**: Increases steadily under load\n",
    "- **Why it matters**: Overall message volume indicator\n",
    "\n",
    "**`nats_server_total_streams`** - JetStream streams\n",
    "- **What it shows**: Number of persistent message streams\n",
    "- **Expected value**: Usually 0-2 for Grove (depends on configuration)\n",
    "- **Why it matters**: JetStream provides message persistence and replay capabilities\n",
    "\n",
    "#### Interpreting the Dashboard\n",
    "\n",
    "**Healthy State**:\n",
    "- âœ… Connections: 2-4 (workers + frontend connected)\n",
    "- âœ… Message rates: Correlated with request traffic (metadata only)\n",
    "- âœ… CPU: < 20%\n",
    "- âœ… Memory: Stable, not growing continuously\n",
    "- âœ… Subscriptions: Non-zero (services registered)\n",
    "\n",
    "**Problem Indicators**:\n",
    "- âš ï¸ Connections: 0 when deployment exists â†’ connectivity issue\n",
    "- âš ï¸ Message rate: Out > In by large margin â†’ message amplification/looping\n",
    "- âš ï¸ CPU: Sustained > 80% â†’ NATS bottleneck\n",
    "- âš ï¸ Memory: Continuously growing â†’ memory leak or message backlog\n",
    "- âš ï¸ Subscriptions: 0 â†’ services not registering with NATS\n",
    "\n",
    "**Important Note**: NATS message volume does NOT reflect KV cache data transfer volume. NIXL handles the heavy tensor data transfer (gigabytes) separately via RDMA/TCP.\n",
    "\n",
    "### Step 3: Understanding etcd Metrics\n",
    "\n",
    "Key etcd metrics to monitor:\n",
    "\n",
    "**Health Metrics**:\n",
    "- `etcd_server_has_leader` - Whether cluster has a leader (should be 1)\n",
    "- `etcd_server_is_leader` - Whether this instance is the leader\n",
    "\n",
    "**Performance Metrics**:\n",
    "- `etcd_mvcc_db_total_size_in_bytes` - Database size\n",
    "- `rate(etcd_server_proposals_committed_total[5m])` - Proposal commit rate\n",
    "\n",
    "**Operation Metrics**:\n",
    "- `etcd_debugging_mvcc_put_total` - Total PUT operations\n",
    "- `etcd_debugging_mvcc_range_total` - Total GET operations\n",
    "\n",
    "### Step 4: Test Distributed Dynamo with Traffic\n",
    "\n",
    "Generate meaningful traffic to see distributed coordination in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Generate test traffic with concurrent requests\n",
    "echo \"Generating traffic to distributed Dynamo deployment...\"\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Function to send a request\n",
    "send_request() {\n",
    "    local id=$1\n",
    "    curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d \"{\n",
    "        \\\"model\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\",\n",
    "        \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Explain distributed systems in 2 sentences. Request $id\\\"}],\n",
    "        \\\"stream\\\": false,\n",
    "        \\\"max_tokens\\\": 100\n",
    "      }\" > /dev/null 2>&1\n",
    "}\n",
    "\n",
    "# Send 30 requests with 3 concurrent workers\n",
    "echo \"Sending 30 requests with 3 concurrent connections...\"\n",
    "echo \"This will generate metrics for:\"\n",
    "echo \"  - NATS coordination message throughput\"\n",
    "echo \"  - Worker utilization across 2 workers\"\n",
    "echo \"  - KV-aware request routing\"\n",
    "echo \"\"\n",
    "\n",
    "for i in {1..10}; do\n",
    "    send_request $((i*3-2)) &\n",
    "    send_request $((i*3-1)) &\n",
    "    send_request $((i*3)) &\n",
    "    wait\n",
    "    echo \"Batch $i/10 complete (requests $((i*3-2))-$((i*3)))\"\n",
    "    sleep 0.5\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Sent 30 requests with concurrent load\"\n",
    "echo \"\"\n",
    "echo \"Check metrics in Grafana:\"\n",
    "echo \"  - Dynamo Inference: Request throughput, TTFT, ITL across workers\"\n",
    "echo \"  - NATS Overview: Coordination message rates (metadata only)\"\n",
    "echo \"  - etcd Overview: Service discovery operations\"\n",
    "echo \"\"\n",
    "echo \"View Grafana: https://grafana0-$(hostname | sed 's/^brev-//').brevlab.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d032995c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Understanding Distributed Dynamo Trade-offs\n",
    "\n",
    "### K8s-Native vs NATS/etcd Comparison (v0.8.0+)\n",
    "\n",
    "| Aspect | K8s-Native | NATS/etcd |\n",
    "|--------|------------|-----------|\n",
    "| **Setup Complexity** | âœ… Simple (no extra infra) | âš ï¸ Complex (2 systems to manage) |\n",
    "| **Latency** | âœ… Lower (direct TCP) | âš ï¸ Slightly higher (pub/sub) |\n",
    "| **Scale Sweet Spot** | 2-50 nodes | 50-1000+ nodes |\n",
    "| **Discovery** | EndpointSlices (built-in) | etcd (external) |\n",
    "| **Transport** | TCP | NATS + TCP |\n",
    "| **Ops Burden** | âœ… Low | âš ï¸ Medium-High |\n",
    "| **Multi-Region** | âš ï¸ Limited | âœ… Excellent |\n",
    "| **Custom Routing** | âš ï¸ Basic | âœ… Advanced |\n",
    "| **Cache Coordination** | âœ… Yes (via planner) | âœ… Yes (via NATS) |\n",
    "| **NIXL Support** | âœ… Yes | âœ… Yes |\n",
    "\n",
    "**Recommendation:** Start with K8s-native. Only add NATS/etcd if you hit scale limits (100+ nodes) or need multi-region.\n",
    "\n",
    "### Single-Node vs Multi-Node\n",
    "\n",
    "**Single Node with Multiple GPUs (Typical Dev Setup)**:\n",
    "```\n",
    "âœ“ KV-aware routing still beneficial (routes to worker with cached data)\n",
    "âœ“ Learning opportunity to understand architecture\n",
    "âœ“ Workers can share cache blocks via NIXL locally\n",
    "âœ“ K8s-native = simpler (no NATS/etcd overhead)\n",
    "âœ— Less dramatic network benefits (same machine)\n",
    "```\n",
    "\n",
    "**Multi-Node (Production)**:\n",
    "```\n",
    "âœ“ KV-aware Router directs requests to nodes with relevant cache\n",
    "âœ“ NIXL transfers cache data efficiently (RDMA/TCP between nodes)\n",
    "âœ“ Improved cache hit rates = lower latency\n",
    "âœ“ Better resource utilization across cluster\n",
    "âœ“ K8s-native sufficient for 2-50 nodes\n",
    "âœ“ NATS/etcd for 100+ nodes or multi-region\n",
    "```\n",
    "âœ“ Enables advanced features (cache migration, load balancing)\n",
    "âœ— Network latency between nodes\n",
    "âœ— Increased complexity in debugging\n",
    "```\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "```bash\n",
    "# Display performance comparison\n",
    "cat <<'EOF'\n",
    "\n",
    "Performance Impact of Distributed Dynamo:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Metric              â”‚ Single Node      â”‚ Multi-Node   â”‚\n",
    "â”‚                     â”‚ (Multi-GPU)      â”‚              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Cache Hit Rate      â”‚ +10-20%          â”‚ +20-40%      â”‚\n",
    "â”‚ Latency (P50)       â”‚ +2-5ms           â”‚ +2-5ms       â”‚\n",
    "â”‚ Latency (P99)       â”‚ +5-10ms          â”‚ +5-10ms      â”‚\n",
    "â”‚ Throughput          â”‚ Same to +10%     â”‚ +30-60%      â”‚\n",
    "â”‚ Memory Overhead     â”‚ +100-200MB       â”‚ +100-200MB   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "When Distributed Dynamo Helps Most:\n",
    "  â€¢ Multiple GPUs or nodes with high traffic\n",
    "  â€¢ Repeated queries (high cache hit potential)\n",
    "  â€¢ Long context lengths (expensive to recompute)\n",
    "  â€¢ Batch processing workloads\n",
    "\n",
    "When It May Not Help:\n",
    "  â€¢ Single GPU deployments\n",
    "  â€¢ Unique queries every time (low cache hit rate)\n",
    "  â€¢ Very short context lengths\n",
    "  â€¢ Real-time streaming with completely unique prompts\n",
    "\n",
    "Architecture Notes:\n",
    "  â€¢ Grove = Kubernetes Operator (orchestration)\n",
    "  â€¢ Dynamo = Serving Framework (actual inference)\n",
    "  â€¢ NATS = Metadata/coordination (small messages)\n",
    "  â€¢ NIXL = KV cache data transfer (large tensors via RDMA/TCP)\n",
    "EOF\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Section 6: Advanced Distributed Features\n",
    "\n",
    "### Cache Monitoring\n",
    "\n",
    "Check distributed coordination through worker logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10840250",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get cache stats from worker logs\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "WORKER_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker,nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)\n",
    "\n",
    "if [ -n \"$WORKER_POD\" ]; then\n",
    "    echo \"Checking NIXL/NATS activity in worker logs...\"\n",
    "    echo \"\"\n",
    "    kubectl logs -n $NAMESPACE $WORKER_POD --tail=100 | grep -i \"nixl\\|nats\" | tail -10\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"Worker pod: $WORKER_POD\"\n",
    "    echo \"\"\n",
    "    echo \"What to look for:\"\n",
    "    echo \"  - NIXL initialization messages (KV cache transfer setup)\"\n",
    "    echo \"  - NATS connection status (coordination layer)\"\n",
    "    echo \"  - KV cache registration events\"\n",
    "    echo \"  - UCX backend messages (if using RDMA for cache transfer)\"\n",
    "else\n",
    "    echo \"âš ï¸ No worker pods found\"\n",
    "    echo \"Make sure the vllm-distributed-demo deployment is running\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354a0e8",
   "metadata": {},
   "source": [
    "**Note**: Cache hit/miss metrics depend on workload patterns. Even on a single node with multiple GPUs, KV-aware routing can improve cache hits by directing requests to the worker that already has relevant cache blocks.\n",
    "\n",
    "### NATS Health Check\n",
    "\n",
    "Verify NATS is functioning correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS service health\n",
    "echo \"Checking NATS health...\"\n",
    "kubectl exec -n nats-system nats-0 -- nats-server --version 2>/dev/null || kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"NATS endpoints:\"\n",
    "kubectl get svc -n nats-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e66c13d",
   "metadata": {},
   "source": [
    "### etcd Health Check\n",
    "\n",
    "Verify etcd cluster health:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e21158",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check etcd health\n",
    "echo \"Checking etcd health...\"\n",
    "ETCD_POD=$(kubectl get pods -n etcd-system -l app.kubernetes.io/name=etcd -o jsonpath='{.items[0].metadata.name}')\n",
    "\n",
    "if [ -n \"$ETCD_POD\" ]; then\n",
    "    kubectl exec -n etcd-system $ETCD_POD -- etcdctl endpoint health 2>/dev/null || echo \"etcd health check requires auth setup\"\n",
    "    echo \"\"\n",
    "    kubectl exec -n etcd-system $ETCD_POD -- etcdctl member list 2>/dev/null || echo \"etcd member list requires auth setup\"\n",
    "else\n",
    "    echo \"âš ï¸ No etcd pods found\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"etcd endpoints:\"\n",
    "kubectl get svc -n etcd-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cfeb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Cleanup\n",
    "\n",
    "### Step 1: Remove Distributed Demo Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937754b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete the distributed deployment\n",
    "echo \"Removing distributed Dynamo deployment...\"\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "kubectl delete dynamographdeployment vllm-distributed-demo -n $NAMESPACE\n",
    "kubectl delete svc vllm-distributed-demo-frontend-np -n $NAMESPACE\n",
    "\n",
    "echo \"âœ“ Distributed deployment removed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ea0e3",
   "metadata": {},
   "source": [
    "### Step 2: Verify Lab 1 Deployment is Still Running\n",
    "\n",
    "Your original Lab 1 deployment should still be running on port 30100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada3b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check Lab 1 deployment status\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Checking Lab 1 deployment...\"\n",
    "kubectl get dynamographdeployment vllm-disagg-router -n $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"Lab 1 pods:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-disagg-router\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Lab 1 deployment is available at: http://$NODE_IP:30100\"\n",
    "echo \"\"\n",
    "echo \"Test it:\"\n",
    "echo \"  curl http://$NODE_IP:30100/v1/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46077080",
   "metadata": {},
   "source": [
    "### Step 3: Remove Distributed Infrastructure (Optional)\n",
    "\n",
    "Only remove NATS and etcd if you're done experimenting with distributed Dynamo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c61156",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove NATS\n",
    "echo \"Removing NATS...\"\n",
    "helm uninstall nats -n nats-system\n",
    "kubectl delete namespace nats-system\n",
    "\n",
    "# Remove etcd  \n",
    "echo \"Removing etcd...\"\n",
    "helm uninstall etcd -n etcd-system\n",
    "kubectl delete namespace etcd-system\n",
    "\n",
    "# Remove PodMonitors\n",
    "kubectl delete podmonitor nats -n nats-system 2>/dev/null || true\n",
    "kubectl delete podmonitor etcd -n etcd-system 2>/dev/null || true\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Distributed infrastructure removed\"\n",
    "echo \"\"\n",
    "echo \"Note: You can reinstall NATS/etcd anytime by re-running Section 2 of this lab\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31414d85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Known Issues (v0.8.0 Distributed Serving)\n",
    "\n",
    "**âš ï¸ Distributed Deployment Issues:**\n",
    "\n",
    "1. **K8s-native Discovery Propagation**: EndpointSlices may take 5-15 seconds to propagate in large clusters (20+ nodes). First requests may fail with \"no available workers\" until discovery completes.\n",
    "\n",
    "2. **NIXL KV Cache Transfer on ARM**: RDMA support on ARM-based nodes (Graviton, Ampere) is experimental in v0.8.0. Fall back to TCP transport if encountering issues.\n",
    "\n",
    "3. **Multi-Frontend KV-Aware Routing**: In some edge cases with >10 frontend replicas, routing metadata may be stale for 1-2 seconds, resulting in suboptimal cache hits (not failures, just less efficient).\n",
    "\n",
    "4. **NATS/etcd Compatibility**: If mixing K8s-native and NATS/etcd modes in the same cluster, ensure workers use consistent discovery method. Mixed modes are not supported.\n",
    "\n",
    "5. **Worker Gang Scheduling**: In v0.8.0, workers must all be ready before accepting traffic. If one worker pod fails, the entire deployment may be blocked. Use `kubectl describe dynamographdeployment` to debug.\n",
    "\n",
    "**Workarounds:**\n",
    "- Discovery delays: Add `--wait-for-workers-timeout=30s` flag to frontend\n",
    "- NIXL issues: Set `NIXL_TRANSPORT=tcp` env var to force TCP mode\n",
    "- Routing staleness: Reduce frontend replicas to 3-5 for optimal cache awareness\n",
    "- Gang scheduling: Use pod disruption budgets and ensure adequate node resources\n",
    "\n",
    "**Migration from v0.7.x:**\n",
    "- K8s-native discovery is backward compatible\n",
    "- Existing NATS/etcd deployments continue to work\n",
    "- Can upgrade in-place, but test K8s-native in staging first\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- âœ… Distributed Dynamo architecture and components (NATS, etcd, NIXL)\n",
    "- âœ… Difference between Grove (operator) and Dynamo (serving framework)\n",
    "- âœ… Deploying distributed coordination infrastructure\n",
    "- âœ… Creating a distributed Dynamo deployment with KV-aware routing\n",
    "- âœ… Monitoring NATS and etcd with Grafana\n",
    "- âœ… Understanding NATS (metadata) vs NIXL (KV cache data transfer)\n",
    "- âœ… Trade-offs between single-node and multi-node setups\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Architecture Clarity**:\n",
    "- **Grove**: Kubernetes Operator (orchestrates Dynamo deployments)\n",
    "- **Dynamo**: Inference serving framework (does the actual work)\n",
    "- **NATS**: Handles coordination metadata and cache events (small messages)\n",
    "- **NIXL**: Transfers actual KV cache data (gigabytes via RDMA/TCP/SSD)\n",
    "\n",
    "**Distributed Dynamo is Powerful**:\n",
    "- Enables KV-aware routing (even on single node with multiple GPUs)\n",
    "- NIXL transfers cache data efficiently between workers\n",
    "- Improves cache hit rates and throughput\n",
    "- Essential for production scale-out scenarios\n",
    "\n",
    "**Benefits Even on Single Node with Multiple GPUs**:\n",
    "- KV-aware Router directs requests to workers with relevant cache\n",
    "- Improved cache hit rates compared to random routing\n",
    "- Coordination overhead is minimal with NATS\n",
    "\n",
    "**Production Considerations**:\n",
    "- Use distributed Dynamo when scaling beyond single GPU\n",
    "- Monitor NATS message rates for coordination health (not data volume)\n",
    "- Plan for network latency between nodes in multi-node setups\n",
    "- Consider cache hit patterns for your workload\n",
    "- Dynamo 0.8+ supports K8s-native discovery (optional NATS/etcd)\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**When Companies Use Distributed Dynamo**:\n",
    "- Multi-region LLM deployments\n",
    "- High-traffic serving (1000+ RPS)\n",
    "- Multi-GPU and multi-node clusters\n",
    "- Cost optimization (share expensive cache computation)\n",
    "- Enterprise multi-tenant platforms\n",
    "\n",
    "**Deployment Options**:\n",
    "- **Single GPU**: No distributed coordination needed\n",
    "- **Multiple GPUs, single node**: Distributed Dynamo with KV-aware routing beneficial\n",
    "- **Small clusters (2-5 nodes)**: Distributed Dynamo provides clear benefits\n",
    "- **Large clusters (10+ nodes)**: Distributed Dynamo essential for coordination\n",
    "- **Dynamo 0.8+**: Can use K8s-native discovery for simpler deployments\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Experiment**: Try different worker replica counts to see KV-aware routing\n",
    "- **Monitor**: Watch NATS/etcd dashboards during traffic (coordination metadata)\n",
    "- **Compare**: Deploy same model without NATS/etcd and compare metrics\n",
    "- **Scale**: If you have access to multi-node clusters, test distributed benefits\n",
    "- **Learn**: Understand NIXL for KV cache data transfer in Dynamo docs\n",
    "- **Explore**: Check out Dynamo 0.8+ features (K8s-native discovery)\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### NATS Not Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582aa036",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS pods\n",
    "kubectl get pods -n nats-system\n",
    "kubectl logs -n nats-system nats-0\n",
    "\n",
    "# Common issues:\n",
    "# - Insufficient resources (need ~256MB RAM)\n",
    "# - Port conflicts (4222 already in use)\n",
    "# - PersistentVolume issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a460c",
   "metadata": {},
   "source": [
    "### etcd Not Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check etcd pods\n",
    "kubectl get pods -n etcd-system\n",
    "kubectl logs -n etcd-system etcd-0\n",
    "\n",
    "# Common issues:\n",
    "# - Insufficient resources (need ~512MB RAM)\n",
    "# - Volume mounting issues\n",
    "# - Network policies blocking ports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d5d2a",
   "metadata": {},
   "source": [
    "### Workers Not Connecting to Distributed Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8cbe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check worker logs for NATS/NIXL connection messages\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "kubectl logs -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker | grep -i \"nats\\|nixl\"\n",
    "\n",
    "# Verify NATS/etcd service endpoints are correct\n",
    "kubectl get svc -n nats-system\n",
    "kubectl get svc -n etcd-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e27eb",
   "metadata": {},
   "source": [
    "### No Cache Sharing Observed\n",
    "\n",
    "**This is normal behavior!** Understanding what's actually happening:\n",
    "\n",
    "**What NATS Does** (visible in metrics):\n",
    "- Shares metadata about cache state between workers\n",
    "- Enables KV-aware routing (Router knows which worker has which cache blocks)\n",
    "- Low message volume (small coordination packets)\n",
    "\n",
    "**What NIXL Does** (not visible in NATS metrics):\n",
    "- Transfers actual KV cache data (gigabytes of tensors)\n",
    "- Uses RDMA, TCP, or CPU/SSD offload\n",
    "- Direct worker-to-worker communication\n",
    "\n",
    "**On Single Node**:\n",
    "- Workers can still benefit from KV-aware routing\n",
    "- Cache transfers via NIXL are faster (no network)\n",
    "- NATS provides coordination, not data transfer\n",
    "\n",
    "**Benefits Require**:\n",
    "- Multiple workers (even on same node)\n",
    "- Repeated queries with shared prefixes\n",
    "- Workload that generates cache hits\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **Dynamo Deployment Guide**: https://docs.nvidia.com/dynamo/latest/guides/dynamo_deploy/\n",
    "- **Grove Operator Guide**: https://docs.nvidia.com/dynamo/latest/guides/dynamo_deploy/grove.html\n",
    "- **Grove GitHub Repository**: https://github.com/NVIDIA/grove\n",
    "- **NIXL Documentation**: NVIDIA Inference Transfer Library (check Dynamo docs)\n",
    "- **NATS Documentation**: https://docs.nats.io/\n",
    "- **etcd Documentation**: https://etcd.io/docs/\n",
    "- **NVIDIA Dynamo Documentation**: https://docs.nvidia.com/dynamo/latest/\n",
    "- **Distributed Systems Patterns**: Understanding consensus and coordination\n",
    "- **KV Cache Architecture**: Understanding distributed cache strategies\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've completed Lab 3: Distributed Dynamo with Grove Orchestration** ğŸŒ²\n",
    "\n",
    "You now understand the fundamentals of distributed LLM serving, the difference between Grove (operator) and Dynamo (serving framework), and how NATS (metadata) and NIXL (data transfer) work together for distributed coordination!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-kernelspec,-widgets,-language_info"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
