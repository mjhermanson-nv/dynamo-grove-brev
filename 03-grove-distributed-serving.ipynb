{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18be8753",
   "metadata": {},
   "source": [
    "# Lab 3: Distributed Serving with Grove\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you will:\n",
    "- Understand Grove's distributed serving architecture\n",
    "- Deploy NATS and etcd for distributed coordination\n",
    "- Enable distributed KV cache sharing across workers\n",
    "- Monitor distributed components with Grafana\n",
    "- Understand when and why to use Grove in production\n",
    "\n",
    "**Prerequisites**: Complete Lab 1 (Dynamo Deployment) and Lab 2 (Monitoring)\n",
    "\n",
    "**Note**: Grove is designed for multi-node Kubernetes clusters. While we'll deploy it on a single node for learning purposes, its benefits are realized when scaling across multiple nodes.\n",
    "\n",
    "## Duration: ~45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Understanding Grove Architecture\n",
    "\n",
    "### What is Grove?\n",
    "\n",
    "Grove is Dynamo's distributed serving framework that enables:\n",
    "- **Multi-node deployments** across Kubernetes clusters\n",
    "- **Distributed KV cache sharing** between worker nodes via NATS\n",
    "- **Coordination and discovery** using etcd\n",
    "- **Advanced features** like cache migration and load balancing\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Kubernetes Cluster                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
    "â”‚  â”‚  Frontend 1  â”‚        â”‚  Frontend 2  â”‚               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n",
    "â”‚         â”‚                       â”‚                        â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚\n",
    "â”‚                     â”‚                                    â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
    "â”‚         â”‚  NATS Message Bus     â”‚                        â”‚\n",
    "â”‚         â”‚  (Cache Sharing)      â”‚                        â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚\n",
    "â”‚                     â”‚                                    â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
    "â”‚         â”‚  etcd (Coordination)  â”‚                        â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚\n",
    "â”‚                     â”‚                                    â”‚\n",
    "â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\n",
    "â”‚      â”‚              â”‚              â”‚                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”                â”‚\n",
    "â”‚  â”‚Worker 1â”‚    â”‚Worker 2â”‚    â”‚Worker 3â”‚                â”‚\n",
    "â”‚  â”‚(Node 1)â”‚    â”‚(Node 2)â”‚    â”‚(Node 3)â”‚                â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚  Shared KV Cache across workers via NATS                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**NATS**: A high-performance message bus that enables:\n",
    "- Real-time cache synchronization\n",
    "- Low-latency pub/sub messaging\n",
    "- Resilient delivery guarantees\n",
    "\n",
    "**etcd**: A distributed key-value store that provides:\n",
    "- Service discovery and registration\n",
    "- Configuration management\n",
    "- Leader election and coordination\n",
    "\n",
    "**Distributed KV Cache**: Allows workers to share key-value cache entries:\n",
    "- Reduces redundant computation\n",
    "- Improves cache hit rates\n",
    "- Enables efficient multi-node scaling\n",
    "\n",
    "### When to Use Grove\n",
    "\n",
    "| Scenario | Use Grove? | Why |\n",
    "|----------|-----------|-----|\n",
    "| Single node deployment | âŒ No | Adds overhead without benefit |\n",
    "| 2-3 nodes | âš ï¸ Maybe | Benefit depends on cache hit patterns |\n",
    "| 4+ nodes | âœ… Yes | Significant performance improvements |\n",
    "| High traffic, repeated queries | âœ… Yes | Cache sharing reduces latency |\n",
    "| Low traffic, unique queries | âŒ No | Cache misses negate benefits |\n",
    "\n",
    "---\n",
    "\n",
    "## Section 2: Deploy Grove Infrastructure\n",
    "\n",
    "### Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables (use defaults if not already set)\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.7.1}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ğŸŒ² Lab 3: Grove Distributed Serving Configuration\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Release Version:  $RELEASE_VERSION\"\n",
    "echo \"  Namespace:        $NAMESPACE\"\n",
    "echo \"  Node IP:          $NODE_IP\"\n",
    "echo \"\"\n",
    "echo \"âœ“ Environment configured for Grove setup\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e469ae",
   "metadata": {},
   "source": [
    "### Step 2: Install NATS Message Bus\n",
    "\n",
    "NATS will handle distributed cache communication between workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dce52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for NATS\n",
    "kubectl create namespace nats-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Add NATS Helm repository\n",
    "echo \"Adding NATS Helm repository...\"\n",
    "helm repo add nats https://nats-io.github.io/k8s/helm/charts/\n",
    "helm repo update\n",
    "\n",
    "# Install NATS (with Prometheus exporter)\n",
    "echo \"Installing NATS with metrics exporter...\"\n",
    "helm upgrade --install nats nats/nats \\\n",
    "  --namespace nats-system \\\n",
    "  --set config.jetstream.enabled=true \\\n",
    "  --set config.jetstream.fileStore.pvc.size=1Gi \\\n",
    "  --set promExporter.enabled=true \\\n",
    "  --set promExporter.port=7777 \\\n",
    "  --wait \\\n",
    "  --timeout 5m\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ NATS installed successfully\"\n",
    "echo \"  Connection: nats://nats.nats-system:4222\"\n",
    "echo \"  Metrics: Port 7777\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08da698c",
   "metadata": {},
   "source": [
    "### Step 3: Install etcd Coordination Layer\n",
    "\n",
    "etcd provides distributed coordination for Grove components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde57e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for etcd\n",
    "kubectl create namespace etcd-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Add Bitnami Helm repository\n",
    "echo \"Adding Bitnami Helm repository...\"\n",
    "helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "helm repo update\n",
    "\n",
    "# Install etcd (using legacy Bitnami mirror)\n",
    "echo \"Installing etcd...\"\n",
    "helm upgrade --install etcd bitnami/etcd \\\n",
    "  --namespace etcd-system \\\n",
    "  --set replicaCount=1 \\\n",
    "  --set auth.rbac.create=false \\\n",
    "  --set image.registry=docker.io \\\n",
    "  --set image.repository=bitnamilegacy/etcd \\\n",
    "  --set persistence.size=1Gi \\\n",
    "  --set preUpgradeHook.enabled=false \\\n",
    "  --wait \\\n",
    "  --timeout 5m\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ etcd installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d528b39",
   "metadata": {},
   "source": [
    "### Step 4: Verify Grove Infrastructure\n",
    "\n",
    "Check that NATS and etcd are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS pods\n",
    "echo \"Checking NATS deployment...\"\n",
    "kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking NATS service...\"\n",
    "kubectl get svc -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "\n",
    "# Check etcd pods\n",
    "echo \"Checking etcd deployment...\"\n",
    "kubectl get pods -n etcd-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking etcd service...\"\n",
    "kubectl get svc -n etcd-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Grove infrastructure verified\"\n",
    "echo \"  NATS:  nats://nats.nats-system:4222\"\n",
    "echo \"  etcd:  http://etcd.etcd-system:2379\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2d834",
   "metadata": {},
   "source": [
    "### Step 4: Enable Prometheus Monitoring\n",
    "\n",
    "Create PodMonitors so Prometheus can scrape NATS and etcd metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ffb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create PodMonitor for NATS (scrapes the prometheus-nats-exporter sidecar)\n",
    "echo \"Enabling NATS metrics collection...\"\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PodMonitor\n",
    "metadata:\n",
    "  name: nats\n",
    "  namespace: nats-system\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/name: nats\n",
    "  podMetricsEndpoints:\n",
    "  - port: prom-metrics\n",
    "    path: /metrics\n",
    "EOF\n",
    "\n",
    "# Create PodMonitor for etcd\n",
    "echo \"Enabling etcd metrics collection...\"\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PodMonitor\n",
    "metadata:\n",
    "  name: etcd\n",
    "  namespace: etcd-system\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/name: etcd\n",
    "  podMetricsEndpoints:\n",
    "  - port: client\n",
    "    path: /metrics\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Prometheus monitoring enabled for Grove infrastructure\"\n",
    "echo \"  Metrics will be available in Grafana within 2-3 minutes\"\n",
    "echo \"\"\n",
    "echo \"  NATS metrics: Scraped from prometheus-nats-exporter (port: prom-metrics)\"\n",
    "echo \"  etcd metrics: Scraped directly from etcd's /metrics endpoint (port: client)\"\n",
    "echo \"\"\n",
    "echo \"Note: etcd metrics typically appear faster than NATS metrics\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43789a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Deploy Grove-Enabled Model\n",
    "\n",
    "### Understanding Dynamo's NATS Integration\n",
    "\n",
    "Dynamo automatically uses NATS for distributed communication when NATS and etcd are available in the cluster. The deployment will:\n",
    "\n",
    "**1. Workers register via NATS**: Each worker announces itself to the message bus\n",
    "**2. Frontend discovers workers**: The frontend finds workers through NATS service discovery\n",
    "**3. NIXL handles KV cache**: NVIDIA's distributed KV cache system coordinates cache sharing\n",
    "\n",
    "### Step 1: Create Grove-Enabled Deployment\n",
    "\n",
    "We'll create a deployment with 2 workers to demonstrate Grove's distributed architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17856cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create Grove-enabled deployment\n",
    "echo \"Creating Grove-enabled deployment with 2 workers...\"\n",
    "\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: nvidia.com/v1alpha1\n",
    "kind: DynamoGraphDeployment\n",
    "metadata:\n",
    "  name: vllm-grove-demo\n",
    "  namespace: dynamo\n",
    "spec:\n",
    "  services:\n",
    "    Frontend:\n",
    "      dynamoNamespace: vllm-grove-demo\n",
    "      componentType: frontend\n",
    "      replicas: 1\n",
    "      extraPodSpec:\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.7.1\n",
    "    VllmWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-grove-demo\n",
    "      componentType: worker\n",
    "      replicas: 2\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: info\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: /data/huggingface-cache\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.7.1\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --tensor-parallel-size 1\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Grove-enabled deployment created\"\n",
    "echo \"  Deployment: vllm-grove-demo\"\n",
    "echo \"  Workers: 2 (will use NATS for discovery)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5554e7",
   "metadata": {},
   "source": [
    "### Step 2: Create NodePort Service\n",
    "\n",
    "Expose the frontend for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd99cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create NodePort service\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: vllm-grove-demo-frontend-np\n",
    "  namespace: dynamo\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    nvidia.com/dynamo-component: Frontend\n",
    "    nvidia.com/dynamo-graph-deployment-name: vllm-grove-demo\n",
    "  ports:\n",
    "  - port: 8000\n",
    "    targetPort: 8000\n",
    "    nodePort: 30200\n",
    "    protocol: TCP\n",
    "    name: http\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ NodePort service created on port 30200\"\n",
    "echo \"  Access at: http://$NODE_IP:30200\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9922f128",
   "metadata": {},
   "source": [
    "### Step 3: Wait for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Wait for pods to be ready\n",
    "echo \"Waiting for Grove-enabled deployment...\"\n",
    "echo \"This may take 2-3 minutes for model download and initialization...\"\n",
    "echo \"\"\n",
    "\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Wait for pods to be ready\n",
    "kubectl wait --for=condition=ready --timeout=300s \\\n",
    "  pods -l nvidia.com/dynamo-graph-deployment-name=vllm-grove-demo \\\n",
    "  -n $NAMESPACE 2>/dev/null || echo \"Pods are initializing...\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Deployment status:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-grove-demo\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Grove-enabled deployment ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73ea98",
   "metadata": {},
   "source": [
    "### Step 4: Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f295dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Test the deployment\n",
    "echo \"Testing inference...\"\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain Grove in one sentence\"}],\n",
    "    \"max_tokens\": 50\n",
    "  }' | python3 -m json.tool\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Grove deployment is serving requests via NATS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689aca2",
   "metadata": {},
   "source": [
    "### Step 5: Verify NATS Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c76e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check worker logs for NATS connectivity\n",
    "echo \"Verifying NATS integration...\"\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "WORKER_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker,nvidia.com/dynamo-graph-deployment-name=vllm-grove-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)\n",
    "\n",
    "if [ -n \"$WORKER_POD\" ]; then\n",
    "    echo \"Checking worker: $WORKER_POD\"\n",
    "    kubectl logs -n $NAMESPACE $WORKER_POD 2>&1 | grep -i \"nats\\|nixl\" | head -5\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"âœ“ Workers are using NATS for distributed coordination\"\n",
    "    echo \"  NIXL (NVIDIA's distributed KV cache system) is active\"\n",
    "else\n",
    "    echo \"âš ï¸ No worker pods found. Make sure the deployment is running:\"\n",
    "    kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-grove-demo\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19d28b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Monitoring Grove Components\n",
    "\n",
    "### Step 1: Access Grafana Dashboards\n",
    "\n",
    "The Grove infrastructure dashboards were created during the oneshot.sh bootstrap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ce07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get Grafana URL\n",
    "BREV_ID=$(hostname | cut -d'-' -f2)\n",
    "GRAFANA_URL=\"https://grafana0-${BREV_ID}.brevlab.com/\"\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ğŸ“Š Grove Monitoring Dashboards\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Grafana URL: $GRAFANA_URL\"\n",
    "echo \"\"\n",
    "echo \"  Available Dashboards:\"\n",
    "echo \"    â€¢ NATS Overview - Message bus metrics\"\n",
    "echo \"    â€¢ etcd Overview - Coordination layer metrics\"\n",
    "echo \"    â€¢ Dynamo Inference Metrics - Model serving metrics\"\n",
    "echo \"\"\n",
    "echo \"ğŸ”— Open Grafana and search for these dashboards\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa05e6",
   "metadata": {},
   "source": [
    "### Step 2: Understanding NATS Metrics\n",
    "\n",
    "The NATS Overview dashboard shows real-time metrics about the message bus that Grove uses for distributed coordination.\n",
    "\n",
    "#### Connection Metrics (Top Left Panel)\n",
    "\n",
    "**`nats_varz_connections`** - Current active connections to NATS\n",
    "- **What it shows**: Number of clients currently connected to the NATS server\n",
    "- **Expected value**: \n",
    "  - With Grove deployment running: 2-4 connections (workers + frontend)\n",
    "  - Without active deployment: 0\n",
    "- **Why it matters**: Each Dynamo component (frontend, workers) maintains a connection to NATS for request routing\n",
    "\n",
    "#### Message Rate Metrics (Top Center Panels)\n",
    "\n",
    "**`rate(nats_varz_in_msgs[1m])`** - Incoming messages per second\n",
    "- **What it shows**: How many messages NATS is receiving per second\n",
    "- **Expected value**: \n",
    "  - Idle: 0 msg/s\n",
    "  - During load: 10-100+ msg/s depending on request rate\n",
    "- **Why it matters**: Shows the message throughput into NATS from Dynamo components\n",
    "\n",
    "**`rate(nats_varz_out_msgs[1m])`** - Outgoing messages per second\n",
    "- **What it shows**: How many messages NATS is sending per second\n",
    "- **Expected value**: Similar to or slightly higher than incoming rate\n",
    "- **Why it matters**: NATS may send multiple copies of messages to subscribers (pub/sub pattern)\n",
    "\n",
    "#### Resource Metrics (Top Right Panel)\n",
    "\n",
    "**`nats_varz_cpu`** - NATS CPU usage percentage\n",
    "- **What it shows**: CPU usage of the NATS process\n",
    "- **Expected value**: \n",
    "  - Idle: < 1%\n",
    "  - Under load: 5-20%\n",
    "  - High load: > 50% (consider scaling)\n",
    "- **Why it matters**: High CPU might indicate NATS is becoming a bottleneck\n",
    "\n",
    "#### Message Rate Graph (Middle Panel)\n",
    "\n",
    "**Time series visualization** of message rates\n",
    "- **Green line**: Incoming messages (rate(nats_varz_in_msgs[1m]))\n",
    "- **Blue line**: Outgoing messages (rate(nats_varz_out_msgs[1m]))\n",
    "- **What to look for**:\n",
    "  - Spikes during traffic bursts\n",
    "  - Correlation between in/out rates\n",
    "  - Steady state during constant load\n",
    "  - Drops to zero when idle\n",
    "\n",
    "#### Memory Usage (Bottom Left Panel)\n",
    "\n",
    "**`nats_varz_mem`** - NATS memory consumption in MB\n",
    "- **What it shows**: RAM used by the NATS process\n",
    "- **Expected value**: \n",
    "  - Base: 20-50 MB\n",
    "  - With JetStream: 50-200 MB\n",
    "  - Under load: May increase with buffered messages\n",
    "- **Why it matters**: Monitors memory leaks or excessive buffering\n",
    "\n",
    "#### NATS Statistics (Bottom Right Panel)\n",
    "\n",
    "**`nats_varz_subscriptions`** - Active subscriptions\n",
    "- **What it shows**: Number of topics/subjects that components are subscribed to\n",
    "- **Expected value**: 5-20 subscriptions (depending on number of workers and endpoints)\n",
    "- **Why it matters**: Each Dynamo service registers subscriptions for the requests it can handle\n",
    "\n",
    "**`nats_server_total_messages`** - Total messages processed\n",
    "- **What it shows**: Cumulative count of all messages since NATS started\n",
    "- **Expected value**: Increases steadily under load\n",
    "- **Why it matters**: Overall message volume indicator\n",
    "\n",
    "**`nats_server_total_streams`** - JetStream streams\n",
    "- **What it shows**: Number of persistent message streams\n",
    "- **Expected value**: Usually 0-2 for Grove (depends on configuration)\n",
    "- **Why it matters**: JetStream provides message persistence and replay capabilities\n",
    "\n",
    "#### Interpreting the Dashboard\n",
    "\n",
    "**Healthy State**:\n",
    "- âœ… Connections: 2-4 (workers + frontend connected)\n",
    "- âœ… Message rates: Correlated with request traffic\n",
    "- âœ… CPU: < 20%\n",
    "- âœ… Memory: Stable, not growing continuously\n",
    "- âœ… Subscriptions: Non-zero (services registered)\n",
    "\n",
    "**Problem Indicators**:\n",
    "- âš ï¸ Connections: 0 when deployment exists â†’ connectivity issue\n",
    "- âš ï¸ Message rate: Out > In by large margin â†’ message amplification/looping\n",
    "- âš ï¸ CPU: Sustained > 80% â†’ NATS bottleneck\n",
    "- âš ï¸ Memory: Continuously growing â†’ memory leak or message backlog\n",
    "- âš ï¸ Subscriptions: 0 â†’ services not registering with NATS\n",
    "\n",
    "### Step 3: Understanding etcd Metrics\n",
    "\n",
    "Key etcd metrics to monitor:\n",
    "\n",
    "**Health Metrics**:\n",
    "- `etcd_server_has_leader` - Whether cluster has a leader (should be 1)\n",
    "- `etcd_server_is_leader` - Whether this instance is the leader\n",
    "\n",
    "**Performance Metrics**:\n",
    "- `etcd_mvcc_db_total_size_in_bytes` - Database size\n",
    "- `rate(etcd_server_proposals_committed_total[5m])` - Proposal commit rate\n",
    "\n",
    "**Operation Metrics**:\n",
    "- `etcd_debugging_mvcc_put_total` - Total PUT operations\n",
    "- `etcd_debugging_mvcc_range_total` - Total GET operations\n",
    "\n",
    "### Step 4: Test Grove with Traffic\n",
    "\n",
    "Generate meaningful traffic to see Grove in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Generate test traffic with concurrent requests\n",
    "echo \"Generating traffic to Grove-enabled deployment...\"\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Function to send a request\n",
    "send_request() {\n",
    "    local id=$1\n",
    "    curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d \"{\n",
    "        \\\"model\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\",\n",
    "        \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Explain distributed systems in 2 sentences. Request $id\\\"}],\n",
    "        \\\"stream\\\": false,\n",
    "        \\\"max_tokens\\\": 100\n",
    "      }\" > /dev/null 2>&1\n",
    "}\n",
    "\n",
    "# Send 30 requests with 3 concurrent workers\n",
    "echo \"Sending 30 requests with 3 concurrent connections...\"\n",
    "echo \"This will generate metrics for:\"\n",
    "echo \"  - NATS message throughput\"\n",
    "echo \"  - Worker utilization across 2 workers\"\n",
    "echo \"  - Request distribution via NATS\"\n",
    "echo \"\"\n",
    "\n",
    "for i in {1..10}; do\n",
    "    send_request $((i*3-2)) &\n",
    "    send_request $((i*3-1)) &\n",
    "    send_request $((i*3)) &\n",
    "    wait\n",
    "    echo \"Batch $i/10 complete (requests $((i*3-2))-$((i*3)))\"\n",
    "    sleep 0.5\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Sent 30 requests with concurrent load\"\n",
    "echo \"\"\n",
    "echo \"Check metrics in Grafana:\"\n",
    "echo \"  - Dynamo Inference: Request throughput, TTFT, ITL across workers\"\n",
    "echo \"  - etcd Overview: Key operations (if Grove uses etcd for coordination)\"\n",
    "echo \"\"\n",
    "echo \"View Grafana: https://grafana0-$(hostname | sed 's/^brev-//').brevlab.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf5a64",
   "metadata": {},
   "source": [
    "### Step 5: Query NATS Metrics Directly\n",
    "\n",
    "The generic NATS dashboard may not work, but you can view NATS metrics directly in Grafana Explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<'EOF'\n",
    "\n",
    "ğŸ“Š To view NATS metrics in Grafana:\n",
    "\n",
    "1. Open Grafana: https://grafana0-$(hostname | sed 's/^brev-//')â€‹.brevlab.com/\n",
    "2. Go to Explore (compass icon in left sidebar)\n",
    "3. Try these PromQL queries:\n",
    "\n",
    "**Dynamo's NATS Client Metrics:**\n",
    "- dynamo_component_nats_client_current_connections\n",
    "- rate(dynamo_component_nats_client_in_messages[1m])\n",
    "- rate(dynamo_component_nats_client_out_messages[1m])\n",
    "- rate(dynamo_component_nats_service_requests_total[1m])\n",
    "\n",
    "**NATS Server Metrics:**\n",
    "- nats_varz_connections{pod=\"nats-0\"}\n",
    "- rate(nats_varz_in_msgs[1m])\n",
    "- rate(nats_varz_out_msgs[1m])\n",
    "- nats_varz_mem / (1024*1024)  # Memory in MB\n",
    "- nats_server_total_messages\n",
    "\n",
    "**etcd Health:**\n",
    "- etcd_server_has_leader\n",
    "- etcd_mvcc_db_total_size_in_bytes / (1024*1024)  # Size in MB\n",
    "\n",
    "These metrics show Grove's NATS usage and etcd coordination in real-time!\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40af15f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Understanding Grove Trade-offs\n",
    "\n",
    "### Single-Node vs Multi-Node\n",
    "\n",
    "**Single Node (Current Setup)**:\n",
    "```\n",
    "âœ— No benefit from cache sharing (all workers on same node)\n",
    "âœ— Added latency from NATS message passing\n",
    "âœ— Additional resource overhead (NATS + etcd)\n",
    "âœ“ Learning opportunity to understand architecture\n",
    "```\n",
    "\n",
    "**Multi-Node (Production)**:\n",
    "```\n",
    "âœ“ Workers share cache across nodes\n",
    "âœ“ Improved cache hit rates = lower latency\n",
    "âœ“ Better resource utilization across cluster\n",
    "âœ“ Enables advanced features (cache migration, load balancing)\n",
    "âœ— Network latency between nodes\n",
    "âœ— Increased complexity in debugging\n",
    "```\n",
    "\n",
    "### Performance Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Display performance comparison\n",
    "cat <<'EOF'\n",
    "\n",
    "Performance Impact of Grove:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Metric              â”‚ Single Node  â”‚ Multi-Node   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Cache Hit Rate      â”‚ Same         â”‚ +20-40%      â”‚\n",
    "â”‚ Latency (P50)       â”‚ +5-10ms      â”‚ +2-5ms       â”‚\n",
    "â”‚ Latency (P99)       â”‚ +10-20ms     â”‚ +5-10ms      â”‚\n",
    "â”‚ Throughput          â”‚ -5-10%       â”‚ +30-60%      â”‚\n",
    "â”‚ Memory Overhead     â”‚ +100-200MB   â”‚ +100-200MB   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "When Grove Helps Most:\n",
    "  â€¢ Multiple nodes with high traffic\n",
    "  â€¢ Repeated queries (high cache hit potential)\n",
    "  â€¢ Long context lengths (expensive to recompute)\n",
    "  â€¢ Batch processing workloads\n",
    "\n",
    "When Grove May Not Help:\n",
    "  â€¢ Single node deployments\n",
    "  â€¢ Unique queries (low cache hit rate)\n",
    "  â€¢ Short context lengths\n",
    "  â€¢ Real-time streaming with varying prompts\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c5442",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Advanced Grove Features (Optional)\n",
    "\n",
    "### Cache Monitoring\n",
    "\n",
    "Check cache statistics from the workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f67d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get cache stats from a worker pod\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "WORKER_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker,nvidia.com/dynamo-graph-deployment-name=vllm-grove-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)\n",
    "\n",
    "if [ -n \"$WORKER_POD\" ]; then\n",
    "    echo \"Checking cache stats from worker: $WORKER_POD\"\n",
    "    kubectl exec -n $NAMESPACE $WORKER_POD -- curl -s localhost:8000/metrics 2>/dev/null | grep -E \"(cache_hit|cache_miss)\" || echo \"Cache metrics not available yet\"\n",
    "else\n",
    "    echo \"âš ï¸ No worker pods found\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87c91a",
   "metadata": {},
   "source": [
    "### NATS Health Check\n",
    "\n",
    "Verify NATS is functioning correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1aeb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS service health\n",
    "echo \"Checking NATS health...\"\n",
    "kubectl exec -n nats-system nats-0 -- nats-server --version 2>/dev/null || kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"NATS endpoints:\"\n",
    "kubectl get svc -n nats-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a4d21",
   "metadata": {},
   "source": [
    "### etcd Health Check\n",
    "\n",
    "Verify etcd cluster health:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6550516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check etcd health\n",
    "echo \"Checking etcd health...\"\n",
    "ETCD_POD=$(kubectl get pods -n etcd-system -l app.kubernetes.io/name=etcd -o jsonpath='{.items[0].metadata.name}')\n",
    "\n",
    "if [ -n \"$ETCD_POD\" ]; then\n",
    "    kubectl exec -n etcd-system $ETCD_POD -- etcdctl endpoint health 2>/dev/null || echo \"etcd health check requires auth setup\"\n",
    "    echo \"\"\n",
    "    kubectl exec -n etcd-system $ETCD_POD -- etcdctl member list 2>/dev/null || echo \"etcd member list requires auth setup\"\n",
    "else\n",
    "    echo \"âš ï¸ No etcd pods found\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"etcd endpoints:\"\n",
    "kubectl get svc -n etcd-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cbb89b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Cleanup\n",
    "\n",
    "### Step 1: Remove Grove Demo Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete the Grove deployment\n",
    "echo \"Removing Grove deployment...\"\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "kubectl delete dynamographdeployment vllm-grove-demo -n $NAMESPACE\n",
    "kubectl delete svc vllm-grove-demo-frontend-np -n $NAMESPACE\n",
    "\n",
    "echo \"âœ“ Grove deployment removed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6c27f",
   "metadata": {},
   "source": [
    "### Step 2: (Optional) Restore Lab 1 Deployment\n",
    "\n",
    "If you want to restore the original Lab 1 deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Restore Lab 1 deployment from backup\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "if [ -f /tmp/lab1-deployment-backup.yaml ]; then\n",
    "    echo \"Restoring Lab 1 deployment...\"\n",
    "    kubectl apply -f /tmp/lab1-deployment-backup.yaml\n",
    "    \n",
    "    echo \"Waiting for Lab 1 deployment to be ready...\"\n",
    "    sleep 10\n",
    "    kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-disagg-router\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"âœ“ Lab 1 deployment restored\"\n",
    "    echo \"  Access at: http://$NODE_IP:30100\"\n",
    "else\n",
    "    echo \"No backup found at /tmp/lab1-deployment-backup.yaml\"\n",
    "    echo \"You can redeploy Lab 1 by running the cells in 01-dynamo-deployment-guide.ipynb\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6591e4e4",
   "metadata": {},
   "source": [
    "### Step 3: (Optional) Remove Grove Infrastructure\n",
    "\n",
    "If you want to remove NATS and etcd after learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc6cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This is optional - only do if you're done with Grove learning\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ğŸ§¹ Grove Infrastructure Cleanup (Optional)\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"\"\n",
    "echo \"This will remove NATS and etcd from your cluster.\"\n",
    "echo \"\"\n",
    "read -p \"Continue? (y/N) \" -n 1 -r\n",
    "echo \"\"\n",
    "\n",
    "if [[ $REPLY =~ ^[Yy]$ ]]; then\n",
    "    # Uninstall NATS\n",
    "    echo \"Removing NATS...\"\n",
    "    helm uninstall nats -n nats-system 2>/dev/null || echo \"  NATS not found\"\n",
    "    kubectl delete namespace nats-system 2>/dev/null || echo \"  Namespace not found\"\n",
    "    \n",
    "    # Uninstall etcd\n",
    "    echo \"Removing etcd...\"\n",
    "    helm uninstall etcd -n etcd-system 2>/dev/null || echo \"  etcd not found\"\n",
    "    kubectl delete namespace etcd-system 2>/dev/null || echo \"  Namespace not found\"\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"âœ“ Grove infrastructure removed\"\n",
    "else\n",
    "    echo \"Cleanup cancelled - Grove infrastructure kept for future experiments\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd38a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- âœ… Grove architecture and components (NATS, etcd, NIXL)\n",
    "- âœ… Deploying distributed coordination infrastructure\n",
    "- âœ… Creating a Grove-enabled Dynamo deployment\n",
    "- âœ… Monitoring NATS and etcd with Grafana\n",
    "- âœ… Understanding NATS-based worker discovery\n",
    "- âœ… Trade-offs between single-node and multi-node setups\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Grove is Powerful for Multi-Node**:\n",
    "- Enables distributed KV cache sharing\n",
    "- Improves cache hit rates and throughput\n",
    "- Essential for production scale-out scenarios\n",
    "\n",
    "**Adds Overhead on Single Node**:\n",
    "- NATS/etcd resource consumption\n",
    "- Message passing latency\n",
    "- Coordination complexity\n",
    "\n",
    "**Production Considerations**:\n",
    "- Use Grove when scaling beyond 3-4 nodes\n",
    "- Monitor NATS message rates to ensure efficiency\n",
    "- Plan for network latency between nodes\n",
    "- Consider cache hit patterns for your workload\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**When Companies Use Grove**:\n",
    "- Multi-region LLM deployments\n",
    "- High-traffic serving (1000+ RPS)\n",
    "- Cost optimization (share expensive cache)\n",
    "- Enterprise multi-tenant platforms\n",
    "\n",
    "**Grove Alternatives**:\n",
    "- Single-node: No distributed cache needed\n",
    "- Small clusters (2-3 nodes): Consider Ray's native cache sharing\n",
    "- Very large clusters (50+ nodes): May need custom sharding strategies\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Experiment**: Try different worker replica counts\n",
    "- **Monitor**: Watch NATS/etcd dashboards during traffic\n",
    "- **Compare**: Deploy same model without Grove and compare metrics\n",
    "- **Scale**: If you have access to multi-node clusters, test Grove benefits\n",
    "- **Explore**: Check out Dynamo's advanced Grove features in the docs\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### NATS Not Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc52ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS pods\n",
    "kubectl get pods -n nats-system\n",
    "kubectl logs -n nats-system nats-0\n",
    "\n",
    "# Common issues:\n",
    "# - Insufficient resources (need ~256MB RAM)\n",
    "# - Port conflicts (4222 already in use)\n",
    "# - PersistentVolume issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9a9f3",
   "metadata": {},
   "source": [
    "### etcd Not Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check etcd pods\n",
    "kubectl get pods -n etcd-system\n",
    "kubectl logs -n etcd-system etcd-0\n",
    "\n",
    "# Common issues:\n",
    "# - Insufficient resources (need ~512MB RAM)\n",
    "# - Volume mounting issues\n",
    "# - Network policies blocking ports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f8378",
   "metadata": {},
   "source": [
    "### Workers Not Connecting to Grove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6bb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check worker logs for Grove connection messages\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "kubectl logs -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker | grep -i grove\n",
    "\n",
    "# Verify NATS/etcd service endpoints are correct\n",
    "kubectl get svc -n nats-system\n",
    "kubectl get svc -n etcd-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb601a3",
   "metadata": {},
   "source": [
    "### No Cache Sharing Observed\n",
    "\n",
    "**This is expected on single node!** Grove's cache sharing benefits require:\n",
    "- Multiple Kubernetes nodes\n",
    "- Workers distributed across nodes\n",
    "- Repeated queries to build cache\n",
    "\n",
    "On a single node, all workers share memory naturally, so Grove adds overhead without benefit.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **Grove Documentation**: https://docs.nvidia.com/dynamo/latest/kubernetes/grove/\n",
    "- **NATS Documentation**: https://docs.nats.io/\n",
    "- **etcd Documentation**: https://etcd.io/docs/\n",
    "- **Distributed Systems Patterns**: Understanding consensus and coordination\n",
    "- **Cache Sharing Strategies**: Martin Kleppmann's \"Designing Data-Intensive Applications\"\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've completed Lab 3: Distributed Serving with Grove** ğŸŒ²\n",
    "\n",
    "You now understand the fundamentals of distributed LLM serving and are prepared for multi-node production deployments!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-kernelspec,-widgets,-language_info"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
