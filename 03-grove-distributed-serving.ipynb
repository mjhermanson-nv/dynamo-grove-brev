{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bcbad0a",
   "metadata": {},
   "source": [
    "# Lab 3: Distributed Serving with Grove\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you will:\n",
    "- Understand Grove's distributed serving architecture\n",
    "- Deploy NATS and etcd for distributed coordination\n",
    "- Enable distributed KV cache sharing across workers\n",
    "- Monitor distributed components with Grafana\n",
    "- Understand when and why to use Grove in production\n",
    "\n",
    "**Prerequisites**: Complete Lab 1 (Dynamo Deployment) and Lab 2 (Monitoring)\n",
    "\n",
    "**Note**: Grove is designed for multi-node Kubernetes clusters. While we'll deploy it on a single node for learning purposes, its benefits are realized when scaling across multiple nodes.\n",
    "\n",
    "## Duration: ~45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Understanding Grove Architecture\n",
    "\n",
    "### What is Grove?\n",
    "\n",
    "Grove is Dynamo's distributed serving framework that enables:\n",
    "- **Multi-node deployments** across Kubernetes clusters\n",
    "- **Distributed KV cache sharing** between worker nodes via NATS\n",
    "- **Coordination and discovery** using etcd\n",
    "- **Advanced features** like cache migration and load balancing\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "```\n",
    "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "               â”‚  Cloud Load Balancer       â”‚\n",
    "               â”‚  or Ingress Controller     â”‚\n",
    "               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚                â”‚\n",
    "    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "    â”‚Frontend 1â”‚    â”‚Frontend 2â”‚    â”‚Frontend 3â”‚\n",
    "    â”‚ (Node 1) â”‚    â”‚ (Node 2) â”‚    â”‚ (Node 3) â”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                â”‚                â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  NATS Message Bus     â”‚\n",
    "              â”‚  (Request Routing &   â”‚\n",
    "              â”‚   Cache Sharing)      â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  etcd (Coordination)  â”‚\n",
    "              â”‚  (Service Discovery)  â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                â”‚                â”‚\n",
    "    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Worker 1 â”‚    â”‚ Worker 2 â”‚    â”‚ Worker 3 â”‚\n",
    "    â”‚ (Node 4) â”‚    â”‚ (Node 5) â”‚    â”‚ (Node 6) â”‚\n",
    "    â”‚  +GPU    â”‚    â”‚  +GPU    â”‚    â”‚  +GPU    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \n",
    "    Shared KV Cache across workers via NATS\n",
    "\n",
    "Note: Workers typically run on GPU nodes (4-6), separate from\n",
    "      CPU-only frontend nodes (1-3). In smaller clusters, they\n",
    "      may share nodes with frontends.\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**NATS**: A high-performance message bus that enables:\n",
    "- Real-time cache synchronization\n",
    "- Low-latency pub/sub messaging\n",
    "- Resilient delivery guarantees\n",
    "\n",
    "**etcd**: A distributed key-value store that provides:\n",
    "- Service discovery and registration\n",
    "- Configuration management\n",
    "- Leader election and coordination\n",
    "\n",
    "**Distributed KV Cache**: Allows workers to share key-value cache entries:\n",
    "- Reduces redundant computation\n",
    "- Improves cache hit rates\n",
    "- Enables efficient multi-node scaling\n",
    "\n",
    "### How Multiple Frontends Work\n",
    "\n",
    "The architecture diagram shows 2 frontends, but **frontend replicas â‰  one per node**. Here's how it actually works in production:\n",
    "\n",
    "**Frontend Scaling Strategy**:\n",
    "```\n",
    "Small cluster (3 nodes):    2-3 frontend replicas\n",
    "Medium cluster (10 nodes):  3-5 frontend replicas\n",
    "Large cluster (50+ nodes):  5-10 frontend replicas\n",
    "```\n",
    "\n",
    "**Load Balancing via Kubernetes Service**:\n",
    "\n",
    "When you create a Service (NodePort or LoadBalancer), Kubernetes automatically load balances across all frontend pods:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: frontend-service\n",
    "spec:\n",
    "  type: LoadBalancer  # or NodePort\n",
    "  selector:\n",
    "    component: frontend  # Selects ALL frontend pods\n",
    "  ports:\n",
    "  - port: 8000\n",
    "```\n",
    "\n",
    "**How Traffic Flows**:\n",
    "1. **External Load Balancer** (cloud provider or Ingress) receives request\n",
    "2. **Kubernetes Service** load balances to any frontend pod\n",
    "3. **Frontend** publishes inference request to NATS\n",
    "4. **NATS** routes to an available worker (on any node)\n",
    "5. **Worker** responds via NATS\n",
    "6. **Frontend** returns HTTP response\n",
    "\n",
    "**Key Benefits**:\n",
    "- âœ… **High Availability**: If one frontend crashes, others continue\n",
    "- âœ… **Load Distribution**: Spread HTTP connections across pods\n",
    "- âœ… **Dynamic Discovery**: NATS decouples frontends from workers\n",
    "- âœ… **Flexible Scaling**: Add/remove frontends independently\n",
    "\n",
    "**Single Node (This Lab)**:\n",
    "In your single-node setup, multiple frontends provide less benefit since there's no network distribution. But you can still see how NATS-based service discovery works!\n",
    "\n",
    "### When to Use Grove\n",
    "\n",
    "| Scenario | Use Grove? | Why |\n",
    "|----------|-----------|-----|\n",
    "| Single node deployment | âŒ No | Adds overhead without benefit |\n",
    "| 2-3 nodes | âš ï¸ Maybe | Benefit depends on cache hit patterns |\n",
    "| 4+ nodes | âœ… Yes | Significant performance improvements |\n",
    "| High traffic, repeated queries | âœ… Yes | Cache sharing reduces latency |\n",
    "| Low traffic, unique queries | âŒ No | Cache misses negate benefits |\n",
    "\n",
    "---\n",
    "\n",
    "## Section 2: Deploy Grove Infrastructure\n",
    "\n",
    "### Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables (use defaults if not already set)\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.7.1}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ğŸŒ² Lab 3: Grove Distributed Serving Configuration\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Release Version:  $RELEASE_VERSION\"\n",
    "echo \"  Namespace:        $NAMESPACE\"\n",
    "echo \"  Node IP:          $NODE_IP\"\n",
    "echo \"\"\n",
    "echo \"âœ“ Environment configured for Grove setup\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7010539f",
   "metadata": {},
   "source": [
    "### Step 2: Install NATS Message Bus\n",
    "\n",
    "NATS will handle distributed cache communication between workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4558961",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for NATS\n",
    "kubectl create namespace nats-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Add NATS Helm repository\n",
    "echo \"Adding NATS Helm repository...\"\n",
    "helm repo add nats https://nats-io.github.io/k8s/helm/charts/\n",
    "helm repo update\n",
    "\n",
    "# Install NATS (with Prometheus exporter)\n",
    "echo \"Installing NATS with metrics exporter...\"\n",
    "helm upgrade --install nats nats/nats \\\n",
    "  --namespace nats-system \\\n",
    "  --set config.jetstream.enabled=true \\\n",
    "  --set config.jetstream.fileStore.pvc.size=1Gi \\\n",
    "  --set promExporter.enabled=true \\\n",
    "  --set promExporter.port=7777 \\\n",
    "  --wait \\\n",
    "  --timeout 5m\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ NATS installed successfully\"\n",
    "echo \"  Connection: nats://nats.nats-system:4222\"\n",
    "echo \"  Metrics: Port 7777\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf9067",
   "metadata": {},
   "source": [
    "### Step 3: Install etcd Coordination Layer\n",
    "\n",
    "etcd provides distributed coordination for Grove components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for etcd\n",
    "kubectl create namespace etcd-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Add Bitnami Helm repository\n",
    "echo \"Adding Bitnami Helm repository...\"\n",
    "helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "helm repo update\n",
    "\n",
    "# Install etcd (using legacy Bitnami mirror)\n",
    "echo \"Installing etcd...\"\n",
    "helm upgrade --install etcd bitnami/etcd \\\n",
    "  --namespace etcd-system \\\n",
    "  --set replicaCount=1 \\\n",
    "  --set auth.rbac.create=false \\\n",
    "  --set image.registry=docker.io \\\n",
    "  --set image.repository=bitnamilegacy/etcd \\\n",
    "  --set persistence.size=1Gi \\\n",
    "  --set preUpgradeHook.enabled=false \\\n",
    "  --wait \\\n",
    "  --timeout 5m\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ etcd installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a6658",
   "metadata": {},
   "source": [
    "### Step 4: Verify Grove Infrastructure\n",
    "\n",
    "Check that NATS and etcd are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aefb017",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS pods\n",
    "echo \"Checking NATS deployment...\"\n",
    "kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking NATS service...\"\n",
    "kubectl get svc -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "\n",
    "# Check etcd pods\n",
    "echo \"Checking etcd deployment...\"\n",
    "kubectl get pods -n etcd-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking etcd service...\"\n",
    "kubectl get svc -n etcd-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Grove infrastructure verified\"\n",
    "echo \"  NATS:  nats://nats.nats-system:4222\"\n",
    "echo \"  etcd:  http://etcd.etcd-system:2379\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af38331",
   "metadata": {},
   "source": [
    "### Step 4: Enable Prometheus Monitoring\n",
    "\n",
    "Create PodMonitors so Prometheus can scrape NATS and etcd metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b203538",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create PodMonitor for NATS (scrapes the prometheus-nats-exporter sidecar)\n",
    "echo \"Enabling NATS metrics collection...\"\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PodMonitor\n",
    "metadata:\n",
    "  name: nats\n",
    "  namespace: nats-system\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/name: nats\n",
    "  podMetricsEndpoints:\n",
    "  - port: prom-metrics\n",
    "    path: /metrics\n",
    "EOF\n",
    "\n",
    "# Create PodMonitor for etcd\n",
    "echo \"Enabling etcd metrics collection...\"\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PodMonitor\n",
    "metadata:\n",
    "  name: etcd\n",
    "  namespace: etcd-system\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/name: etcd\n",
    "  podMetricsEndpoints:\n",
    "  - port: client\n",
    "    path: /metrics\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Prometheus monitoring enabled for Grove infrastructure\"\n",
    "echo \"  Metrics will be available in Grafana within 2-3 minutes\"\n",
    "echo \"\"\n",
    "echo \"  NATS metrics: Scraped from prometheus-nats-exporter (port: prom-metrics)\"\n",
    "echo \"  etcd metrics: Scraped directly from etcd's /metrics endpoint (port: client)\"\n",
    "echo \"\"\n",
    "echo \"Note: etcd metrics typically appear faster than NATS metrics\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee6166",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Deploy Grove-Enabled Model\n",
    "\n",
    "### Understanding Dynamo's NATS Integration\n",
    "\n",
    "Dynamo automatically uses NATS for distributed communication when NATS and etcd are available in the cluster. The deployment will:\n",
    "\n",
    "**1. Workers register via NATS**: Each worker announces itself to the message bus\n",
    "**2. Frontend discovers workers**: The frontend finds workers through NATS service discovery\n",
    "**3. NIXL handles KV cache**: NVIDIA's distributed KV cache system coordinates cache sharing\n",
    "\n",
    "### Step 1: Create Grove-Enabled Deployment\n",
    "\n",
    "We'll create a deployment with 2 workers to demonstrate Grove's distributed architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f9f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create Grove-enabled deployment\n",
    "echo \"Creating Grove-enabled deployment with 2 workers...\"\n",
    "\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: nvidia.com/v1alpha1\n",
    "kind: DynamoGraphDeployment\n",
    "metadata:\n",
    "  name: vllm-grove-demo\n",
    "  namespace: dynamo\n",
    "spec:\n",
    "  services:\n",
    "    Frontend:\n",
    "      dynamoNamespace: vllm-grove-demo\n",
    "      componentType: frontend\n",
    "      replicas: 1\n",
    "      extraPodSpec:\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.7.1\n",
    "    VllmWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-grove-demo\n",
    "      componentType: worker\n",
    "      replicas: 2\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: info\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: /data/huggingface-cache\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.7.1\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --tensor-parallel-size 1\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Grove-enabled deployment created\"\n",
    "echo \"  Deployment: vllm-grove-demo\"\n",
    "echo \"  Workers: 2 (will use NATS for discovery)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed475ff3",
   "metadata": {},
   "source": [
    "### Step 2: Create NodePort Service\n",
    "\n",
    "Expose the frontend for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c3a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create NodePort service\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: vllm-grove-demo-frontend-np\n",
    "  namespace: dynamo\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    nvidia.com/dynamo-component: Frontend\n",
    "    nvidia.com/dynamo-graph-deployment-name: vllm-grove-demo\n",
    "  ports:\n",
    "  - port: 8000\n",
    "    targetPort: 8000\n",
    "    nodePort: 30200\n",
    "    protocol: TCP\n",
    "    name: http\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ NodePort service created on port 30200\"\n",
    "echo \"  Access at: http://$NODE_IP:30200\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288f1d84",
   "metadata": {},
   "source": [
    "### Step 3: Wait for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Wait for pods to be ready\n",
    "echo \"Waiting for Grove-enabled deployment...\"\n",
    "echo \"This may take 2-3 minutes for model download and initialization...\"\n",
    "echo \"\"\n",
    "\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Wait for pods to be ready\n",
    "kubectl wait --for=condition=ready --timeout=300s \\\n",
    "  pods -l nvidia.com/dynamo-graph-deployment-name=vllm-grove-demo \\\n",
    "  -n $NAMESPACE 2>/dev/null || echo \"Pods are initializing...\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Deployment status:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-grove-demo\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Grove-enabled deployment ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf556fd",
   "metadata": {},
   "source": [
    "### Step 4: Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e165928",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Test the deployment\n",
    "echo \"Testing inference...\"\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain Grove in one sentence\"}],\n",
    "    \"max_tokens\": 50\n",
    "  }' | python3 -m json.tool\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Grove deployment is serving requests via NATS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ba758",
   "metadata": {},
   "source": [
    "### Step 5: Verify NATS Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check worker logs for NATS connectivity\n",
    "echo \"Verifying NATS integration...\"\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "WORKER_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker,nvidia.com/dynamo-graph-deployment-name=vllm-grove-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)\n",
    "\n",
    "if [ -n \"$WORKER_POD\" ]; then\n",
    "    echo \"Checking worker: $WORKER_POD\"\n",
    "    kubectl logs -n $NAMESPACE $WORKER_POD 2>&1 | grep -i \"nats\\|nixl\" | head -5\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"âœ“ Workers are using NATS for distributed coordination\"\n",
    "    echo \"  NIXL (NVIDIA's distributed KV cache system) is active\"\n",
    "else\n",
    "    echo \"âš ï¸ No worker pods found. Make sure the deployment is running:\"\n",
    "    kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-grove-demo\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874e330",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Monitoring Grove Components\n",
    "\n",
    "### Step 1: Access Grafana Dashboards\n",
    "\n",
    "The Grove infrastructure dashboards were created during the oneshot.sh bootstrap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get Grafana URL\n",
    "BREV_ID=$(hostname | cut -d'-' -f2)\n",
    "GRAFANA_URL=\"https://grafana0-${BREV_ID}.brevlab.com/\"\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ğŸ“Š Grove Monitoring Dashboards\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Grafana URL: $GRAFANA_URL\"\n",
    "echo \"\"\n",
    "echo \"  Available Dashboards:\"\n",
    "echo \"    â€¢ NATS Overview - Message bus metrics\"\n",
    "echo \"    â€¢ etcd Overview - Coordination layer metrics\"\n",
    "echo \"    â€¢ Dynamo Inference Metrics - Model serving metrics\"\n",
    "echo \"\"\n",
    "echo \"ğŸ”— Open Grafana and search for these dashboards\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f142edf",
   "metadata": {},
   "source": [
    "### Step 2: Understanding NATS Metrics\n",
    "\n",
    "The NATS Overview dashboard shows real-time metrics about the message bus that Grove uses for distributed coordination.\n",
    "\n",
    "#### Connection Metrics (Top Left Panel)\n",
    "\n",
    "**`nats_varz_connections`** - Current active connections to NATS\n",
    "- **What it shows**: Number of clients currently connected to the NATS server\n",
    "- **Expected value**: \n",
    "  - With Grove deployment running: 2-4 connections (workers + frontend)\n",
    "  - Without active deployment: 0\n",
    "- **Why it matters**: Each Dynamo component (frontend, workers) maintains a connection to NATS for request routing\n",
    "\n",
    "#### Message Rate Metrics (Top Center Panels)\n",
    "\n",
    "**`rate(nats_varz_in_msgs[1m])`** - Incoming messages per second\n",
    "- **What it shows**: How many messages NATS is receiving per second\n",
    "- **Expected value**: \n",
    "  - Idle: 0 msg/s\n",
    "  - During load: 10-100+ msg/s depending on request rate\n",
    "- **Why it matters**: Shows the message throughput into NATS from Dynamo components\n",
    "\n",
    "**`rate(nats_varz_out_msgs[1m])`** - Outgoing messages per second\n",
    "- **What it shows**: How many messages NATS is sending per second\n",
    "- **Expected value**: Similar to or slightly higher than incoming rate\n",
    "- **Why it matters**: NATS may send multiple copies of messages to subscribers (pub/sub pattern)\n",
    "\n",
    "#### Resource Metrics (Top Right Panel)\n",
    "\n",
    "**`nats_varz_cpu`** - NATS CPU usage percentage\n",
    "- **What it shows**: CPU usage of the NATS process\n",
    "- **Expected value**: \n",
    "  - Idle: < 1%\n",
    "  - Under load: 5-20%\n",
    "  - High load: > 50% (consider scaling)\n",
    "- **Why it matters**: High CPU might indicate NATS is becoming a bottleneck\n",
    "\n",
    "#### Message Rate Graph (Middle Panel)\n",
    "\n",
    "**Time series visualization** of message rates\n",
    "- **Green line**: Incoming messages (rate(nats_varz_in_msgs[1m]))\n",
    "- **Blue line**: Outgoing messages (rate(nats_varz_out_msgs[1m]))\n",
    "- **What to look for**:\n",
    "  - Spikes during traffic bursts\n",
    "  - Correlation between in/out rates\n",
    "  - Steady state during constant load\n",
    "  - Drops to zero when idle\n",
    "\n",
    "#### Memory Usage (Bottom Left Panel)\n",
    "\n",
    "**`nats_varz_mem`** - NATS memory consumption in MB\n",
    "- **What it shows**: RAM used by the NATS process\n",
    "- **Expected value**: \n",
    "  - Base: 20-50 MB\n",
    "  - With JetStream: 50-200 MB\n",
    "  - Under load: May increase with buffered messages\n",
    "- **Why it matters**: Monitors memory leaks or excessive buffering\n",
    "\n",
    "#### NATS Statistics (Bottom Right Panel)\n",
    "\n",
    "**`nats_varz_subscriptions`** - Active subscriptions\n",
    "- **What it shows**: Number of topics/subjects that components are subscribed to\n",
    "- **Expected value**: 5-20 subscriptions (depending on number of workers and endpoints)\n",
    "- **Why it matters**: Each Dynamo service registers subscriptions for the requests it can handle\n",
    "\n",
    "**`nats_server_total_messages`** - Total messages processed\n",
    "- **What it shows**: Cumulative count of all messages since NATS started\n",
    "- **Expected value**: Increases steadily under load\n",
    "- **Why it matters**: Overall message volume indicator\n",
    "\n",
    "**`nats_server_total_streams`** - JetStream streams\n",
    "- **What it shows**: Number of persistent message streams\n",
    "- **Expected value**: Usually 0-2 for Grove (depends on configuration)\n",
    "- **Why it matters**: JetStream provides message persistence and replay capabilities\n",
    "\n",
    "#### Interpreting the Dashboard\n",
    "\n",
    "**Healthy State**:\n",
    "- âœ… Connections: 2-4 (workers + frontend connected)\n",
    "- âœ… Message rates: Correlated with request traffic\n",
    "- âœ… CPU: < 20%\n",
    "- âœ… Memory: Stable, not growing continuously\n",
    "- âœ… Subscriptions: Non-zero (services registered)\n",
    "\n",
    "**Problem Indicators**:\n",
    "- âš ï¸ Connections: 0 when deployment exists â†’ connectivity issue\n",
    "- âš ï¸ Message rate: Out > In by large margin â†’ message amplification/looping\n",
    "- âš ï¸ CPU: Sustained > 80% â†’ NATS bottleneck\n",
    "- âš ï¸ Memory: Continuously growing â†’ memory leak or message backlog\n",
    "- âš ï¸ Subscriptions: 0 â†’ services not registering with NATS\n",
    "\n",
    "### Step 3: Understanding etcd Metrics\n",
    "\n",
    "Key etcd metrics to monitor:\n",
    "\n",
    "**Health Metrics**:\n",
    "- `etcd_server_has_leader` - Whether cluster has a leader (should be 1)\n",
    "- `etcd_server_is_leader` - Whether this instance is the leader\n",
    "\n",
    "**Performance Metrics**:\n",
    "- `etcd_mvcc_db_total_size_in_bytes` - Database size\n",
    "- `rate(etcd_server_proposals_committed_total[5m])` - Proposal commit rate\n",
    "\n",
    "**Operation Metrics**:\n",
    "- `etcd_debugging_mvcc_put_total` - Total PUT operations\n",
    "- `etcd_debugging_mvcc_range_total` - Total GET operations\n",
    "\n",
    "### Step 4: Test Grove with Traffic\n",
    "\n",
    "Generate meaningful traffic to see Grove in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78055670",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Generate test traffic with concurrent requests\n",
    "echo \"Generating traffic to Grove-enabled deployment...\"\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Function to send a request\n",
    "send_request() {\n",
    "    local id=$1\n",
    "    curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d \"{\n",
    "        \\\"model\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\",\n",
    "        \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Explain distributed systems in 2 sentences. Request $id\\\"}],\n",
    "        \\\"stream\\\": false,\n",
    "        \\\"max_tokens\\\": 100\n",
    "      }\" > /dev/null 2>&1\n",
    "}\n",
    "\n",
    "# Send 30 requests with 3 concurrent workers\n",
    "echo \"Sending 30 requests with 3 concurrent connections...\"\n",
    "echo \"This will generate metrics for:\"\n",
    "echo \"  - NATS message throughput\"\n",
    "echo \"  - Worker utilization across 2 workers\"\n",
    "echo \"  - Request distribution via NATS\"\n",
    "echo \"\"\n",
    "\n",
    "for i in {1..10}; do\n",
    "    send_request $((i*3-2)) &\n",
    "    send_request $((i*3-1)) &\n",
    "    send_request $((i*3)) &\n",
    "    wait\n",
    "    echo \"Batch $i/10 complete (requests $((i*3-2))-$((i*3)))\"\n",
    "    sleep 0.5\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Sent 30 requests with concurrent load\"\n",
    "echo \"\"\n",
    "echo \"Check metrics in Grafana:\"\n",
    "echo \"  - Dynamo Inference: Request throughput, TTFT, ITL across workers\"\n",
    "echo \"  - etcd Overview: Key operations (if Grove uses etcd for coordination)\"\n",
    "echo \"\"\n",
    "echo \"View Grafana: https://grafana0-$(hostname | sed 's/^brev-//').brevlab.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2408b4f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Understanding Grove Trade-offs\n",
    "\n",
    "### Single-Node vs Multi-Node\n",
    "\n",
    "**Single Node (Current Setup)**:\n",
    "```\n",
    "âœ— No benefit from cache sharing (all workers on same node)\n",
    "âœ— Added latency from NATS message passing\n",
    "âœ— Additional resource overhead (NATS + etcd)\n",
    "âœ“ Learning opportunity to understand architecture\n",
    "```\n",
    "\n",
    "**Multi-Node (Production)**:\n",
    "```\n",
    "âœ“ Workers share cache across nodes\n",
    "âœ“ Improved cache hit rates = lower latency\n",
    "âœ“ Better resource utilization across cluster\n",
    "âœ“ Enables advanced features (cache migration, load balancing)\n",
    "âœ— Network latency between nodes\n",
    "âœ— Increased complexity in debugging\n",
    "```\n",
    "\n",
    "### Performance Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f35fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Display performance comparison\n",
    "cat <<'EOF'\n",
    "\n",
    "Performance Impact of Grove:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Metric              â”‚ Single Node  â”‚ Multi-Node   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Cache Hit Rate      â”‚ Same         â”‚ +20-40%      â”‚\n",
    "â”‚ Latency (P50)       â”‚ +5-10ms      â”‚ +2-5ms       â”‚\n",
    "â”‚ Latency (P99)       â”‚ +10-20ms     â”‚ +5-10ms      â”‚\n",
    "â”‚ Throughput          â”‚ -5-10%       â”‚ +30-60%      â”‚\n",
    "â”‚ Memory Overhead     â”‚ +100-200MB   â”‚ +100-200MB   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "When Grove Helps Most:\n",
    "  â€¢ Multiple nodes with high traffic\n",
    "  â€¢ Repeated queries (high cache hit potential)\n",
    "  â€¢ Long context lengths (expensive to recompute)\n",
    "  â€¢ Batch processing workloads\n",
    "\n",
    "When Grove May Not Help:\n",
    "  â€¢ Single node deployments\n",
    "  â€¢ Unique queries (low cache hit rate)\n",
    "  â€¢ Short context lengths\n",
    "  â€¢ Real-time streaming with varying prompts\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e4b64a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Advanced Grove Features\n",
    "\n",
    "### Cache Monitoring\n",
    "\n",
    "Check Grove coordination through worker logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250905f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get cache stats from worker logs\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "WORKER_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker,nvidia.com/dynamo-graph-deployment-name=vllm-grove-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)\n",
    "\n",
    "if [ -n \"$WORKER_POD\" ]; then\n",
    "    echo \"Checking NIXL/Grove activity in worker logs...\"\n",
    "    echo \"\"\n",
    "    kubectl logs -n $NAMESPACE $WORKER_POD --tail=100 | grep -i \"nixl\\|grove\\|nats\" | tail -10\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"Worker pod: $WORKER_POD\"\n",
    "    echo \"\"\n",
    "    echo \"What to look for:\"\n",
    "    echo \"  - NIXL initialization messages\"\n",
    "    echo \"  - NATS connection status\"\n",
    "    echo \"  - KV cache registration\"\n",
    "    echo \"  - UCX backend messages (if using RDMA)\"\n",
    "else\n",
    "    echo \"âš ï¸ No worker pods found\"\n",
    "    echo \"Make sure the vllm-grove-demo deployment is running\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90f7b8f",
   "metadata": {},
   "source": [
    "**Note**: Cache hit/miss metrics depend on workload patterns. In a single-node setup, local cache is more efficient than distributed cache, so you may not see significant Grove cache sharing activity.\n",
    "\n",
    "### NATS Health Check\n",
    "\n",
    "Verify NATS is functioning correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS service health\n",
    "echo \"Checking NATS health...\"\n",
    "kubectl exec -n nats-system nats-0 -- nats-server --version 2>/dev/null || kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"NATS endpoints:\"\n",
    "kubectl get svc -n nats-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45647a0d",
   "metadata": {},
   "source": [
    "### etcd Health Check\n",
    "\n",
    "Verify etcd cluster health:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be81c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check etcd health\n",
    "echo \"Checking etcd health...\"\n",
    "ETCD_POD=$(kubectl get pods -n etcd-system -l app.kubernetes.io/name=etcd -o jsonpath='{.items[0].metadata.name}')\n",
    "\n",
    "if [ -n \"$ETCD_POD\" ]; then\n",
    "    kubectl exec -n etcd-system $ETCD_POD -- etcdctl endpoint health 2>/dev/null || echo \"etcd health check requires auth setup\"\n",
    "    echo \"\"\n",
    "    kubectl exec -n etcd-system $ETCD_POD -- etcdctl member list 2>/dev/null || echo \"etcd member list requires auth setup\"\n",
    "else\n",
    "    echo \"âš ï¸ No etcd pods found\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"etcd endpoints:\"\n",
    "kubectl get svc -n etcd-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeba8fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Cleanup\n",
    "\n",
    "### Step 1: Remove Grove Demo Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete the Grove deployment\n",
    "echo \"Removing Grove deployment...\"\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "kubectl delete dynamographdeployment vllm-grove-demo -n $NAMESPACE\n",
    "kubectl delete svc vllm-grove-demo-frontend-np -n $NAMESPACE\n",
    "\n",
    "echo \"âœ“ Grove deployment removed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342a5d3",
   "metadata": {},
   "source": [
    "### Step 2: Verify Lab 1 Deployment is Still Running\n",
    "\n",
    "Your original Lab 1 deployment should still be running on port 30100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check Lab 1 deployment status\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Checking Lab 1 deployment...\"\n",
    "kubectl get dynamographdeployment vllm-disagg-router -n $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"Lab 1 pods:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-disagg-router\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Lab 1 deployment is available at: http://$NODE_IP:30100\"\n",
    "echo \"\"\n",
    "echo \"Test it:\"\n",
    "echo \"  curl http://$NODE_IP:30100/v1/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fda543",
   "metadata": {},
   "source": [
    "### Step 3: Remove Grove Infrastructure (Optional)\n",
    "\n",
    "Only remove NATS and etcd if you're done experimenting with Grove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f41b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove NATS\n",
    "echo \"Removing NATS...\"\n",
    "helm uninstall nats -n nats-system\n",
    "kubectl delete namespace nats-system\n",
    "\n",
    "# Remove etcd  \n",
    "echo \"Removing etcd...\"\n",
    "helm uninstall etcd -n etcd-system\n",
    "kubectl delete namespace etcd-system\n",
    "\n",
    "# Remove PodMonitors\n",
    "kubectl delete podmonitor nats -n nats-system 2>/dev/null || true\n",
    "kubectl delete podmonitor etcd -n etcd-system 2>/dev/null || true\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Grove infrastructure removed\"\n",
    "echo \"\"\n",
    "echo \"Note: You can reinstall NATS/etcd anytime by re-running Section 2 of this lab\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874bc298",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- âœ… Grove architecture and components (NATS, etcd, NIXL)\n",
    "- âœ… Deploying distributed coordination infrastructure\n",
    "- âœ… Creating a Grove-enabled Dynamo deployment\n",
    "- âœ… Monitoring NATS and etcd with Grafana\n",
    "- âœ… Understanding NATS-based worker discovery\n",
    "- âœ… Trade-offs between single-node and multi-node setups\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Grove is Powerful for Multi-Node**:\n",
    "- Enables distributed KV cache sharing\n",
    "- Improves cache hit rates and throughput\n",
    "- Essential for production scale-out scenarios\n",
    "\n",
    "**Adds Overhead on Single Node**:\n",
    "- NATS/etcd resource consumption\n",
    "- Message passing latency\n",
    "- Coordination complexity\n",
    "\n",
    "**Production Considerations**:\n",
    "- Use Grove when scaling beyond 3-4 nodes\n",
    "- Monitor NATS message rates to ensure efficiency\n",
    "- Plan for network latency between nodes\n",
    "- Consider cache hit patterns for your workload\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**When Companies Use Grove**:\n",
    "- Multi-region LLM deployments\n",
    "- High-traffic serving (1000+ RPS)\n",
    "- Cost optimization (share expensive cache)\n",
    "- Enterprise multi-tenant platforms\n",
    "\n",
    "**Grove Alternatives**:\n",
    "- Single-node: No distributed cache needed\n",
    "- Small clusters (2-3 nodes): Consider Ray's native cache sharing\n",
    "- Very large clusters (50+ nodes): May need custom sharding strategies\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Experiment**: Try different worker replica counts\n",
    "- **Monitor**: Watch NATS/etcd dashboards during traffic\n",
    "- **Compare**: Deploy same model without Grove and compare metrics\n",
    "- **Scale**: If you have access to multi-node clusters, test Grove benefits\n",
    "- **Explore**: Check out Dynamo's advanced Grove features in the docs\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### NATS Not Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS pods\n",
    "kubectl get pods -n nats-system\n",
    "kubectl logs -n nats-system nats-0\n",
    "\n",
    "# Common issues:\n",
    "# - Insufficient resources (need ~256MB RAM)\n",
    "# - Port conflicts (4222 already in use)\n",
    "# - PersistentVolume issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca73b6",
   "metadata": {},
   "source": [
    "### etcd Not Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check etcd pods\n",
    "kubectl get pods -n etcd-system\n",
    "kubectl logs -n etcd-system etcd-0\n",
    "\n",
    "# Common issues:\n",
    "# - Insufficient resources (need ~512MB RAM)\n",
    "# - Volume mounting issues\n",
    "# - Network policies blocking ports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03409cb1",
   "metadata": {},
   "source": [
    "### Workers Not Connecting to Grove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check worker logs for Grove connection messages\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "kubectl logs -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker | grep -i grove\n",
    "\n",
    "# Verify NATS/etcd service endpoints are correct\n",
    "kubectl get svc -n nats-system\n",
    "kubectl get svc -n etcd-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999c661b",
   "metadata": {},
   "source": [
    "### No Cache Sharing Observed\n",
    "\n",
    "**This is expected on single node!** Grove's cache sharing benefits require:\n",
    "- Multiple Kubernetes nodes\n",
    "- Workers distributed across nodes\n",
    "- Repeated queries to build cache\n",
    "\n",
    "On a single node, all workers share memory naturally, so Grove adds overhead without benefit.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **Grove Deployment Guide**: https://docs.nvidia.com/dynamo/latest/guides/dynamo_deploy/grove.html\n",
    "- **Grove GitHub Repository**: https://github.com/NVIDIA/grove\n",
    "- **NATS Documentation**: https://docs.nats.io/\n",
    "- **etcd Documentation**: https://etcd.io/docs/\n",
    "- **NVIDIA Dynamo Documentation**: https://docs.nvidia.com/dynamo/latest/\n",
    "- **Distributed Systems Patterns**: Understanding consensus and coordination\n",
    "- **Cache Sharing Strategies**: Martin Kleppmann's \"Designing Data-Intensive Applications\"\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've completed Lab 3: Distributed Serving with Grove** ğŸŒ²\n",
    "\n",
    "You now understand the fundamentals of distributed LLM serving and are prepared for multi-node production deployments!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-kernelspec,-widgets,-language_info"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
