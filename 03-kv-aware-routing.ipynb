{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58951249",
   "metadata": {},
   "source": [
    "# Lab 3: KV-Aware Routing with Data-Parallel Workers\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you'll learn about **KV-aware routing**, an intelligent load balancing feature that routes requests to workers based on their cached data. Unlike simple round-robin routing, KV-aware routing tracks which workers have already processed similar prompts and directs new requests to workers with matching cached blocks. This dramatically reduces the time to first token (TTFT) for repeated or similar queries.\n",
    "\n",
    "**The Architecture:**\n",
    "- **2 Independent Workers** (GPUs 0-1): Each handles full inference (prefill + decode)\n",
    "- **KV-Aware Router**: Tracks which worker has cached which prompt prefixes\n",
    "- **NATS Message Bus**: Coordinates cache state across workers\n",
    "- Each worker stores its own local KV cache and publishes cache events to NATS\n",
    "\n",
    "**How KV-Aware Routing Works:**\n",
    "1. Request with prompt \"Explain quantum computing\" arrives\n",
    "2. Router checks: No worker has this cached â†’ sends to Worker 1\n",
    "3. Worker 1 processes request and caches the prefill computation\n",
    "4. Worker 1 publishes cache event to NATS: \"I have blocks for 'Explain quantum computing'\"\n",
    "5. Router updates its tracking: Worker 1 has those cached blocks\n",
    "6. Next request: \"Explain quantum computing in simple terms\" arrives\n",
    "7. Router sees: Worker 1 has cached blocks for \"Explain quantum computing\" â†’ sends to Worker 1\n",
    "8. Worker 1 reuses cached prefill blocks â†’ much faster TTFT!\n",
    "\n",
    "**Why This Matters:**\n",
    "When users ask variations of similar questions, the router intelligently directs requests to workers that have already cached related computations. This avoids redundant prefill work and reduces time-to-first-token for cache-friendly workloads.\n",
    "\n",
    "**When to use KV-aware routing:**\n",
    "- Chatbots with conversation history (similar context across turns)\n",
    "- Document Q&A systems (multiple questions about the same document)\n",
    "- Batch processing with shared system prompts\n",
    "- Any workload where prompt prefixes are repeated across requests\n",
    "\n",
    "**Prerequisites**: Complete Lab 1 (Dynamo Deployment) and Lab 2 (Monitoring)\n",
    "\n",
    "**Duration**: ~60 minutes\n",
    "\n",
    "**Note**: Requires 2 GPUs. If Lab 1 is still running, you'll need to clean it up first.\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Understanding KV-Aware Routing\n",
    "\n",
    "### What is KV-Aware Routing?\n",
    "\n",
    "Traditional load balancers distribute requests randomly or in round-robin fashion across workers, treating all workers as identical. But large language models cache intermediate computations (the \"KV cache\") to avoid reprocessing tokens they've already seen. **KV-aware routing** leverages this by tracking which workers have which cached blocks and intelligently routing requests to workers that can reuse cached data.\n",
    "\n",
    "**Example Scenario:**\n",
    "1. User asks: \"Explain quantum computing\" â†’ Router sends to Worker 1\n",
    "2. Worker 1 processes the prompt and caches it\n",
    "3. User follows up: \"Explain quantum computing in simple terms\" â†’ Router notices the shared prefix and sends to Worker 1\n",
    "4. Worker 1 reuses the cached computation for \"Explain quantum computing\", only processes the new part\n",
    "5. Result: **Much faster** time-to-first-token (TTFT)\n",
    "\n",
    "### How KV-Aware Routing Works\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Worker 1 â”‚  â”‚ Worker 2 â”‚  (Data-parallel: identical workers)\n",
    "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "     â”‚             â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "            â”‚ Publish cache events\n",
    "            â†“\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚  NATS Server   â”‚  (Message bus for cache coordination)\n",
    "   â”‚  (Message Bus) â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "            â”‚ Subscribe to events\n",
    "            â†“\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚  KV Router     â”‚  (Tracks which worker has which cached blocks)\n",
    "   â”‚ (Global Index) â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "            â†“\n",
    "   Routes requests to workers with matching cached prefixes\n",
    "```\n",
    "\n",
    "**The Flow:**\n",
    "1. **Request arrives**: \"Explain quantum computing\"\n",
    "2. **Router checks cache index**: No worker has this cached â†’ picks Worker 1\n",
    "3. **Worker 1 processes**: Creates KV cache blocks 0-5\n",
    "4. **Worker 1 publishes to NATS**: \"I cached blocks 0-5 for prefix 'Explain quantum computing'\"\n",
    "5. **Router updates index**: Worker 1 has these blocks\n",
    "6. **Next request arrives**: \"Explain quantum computing in simple terms\"\n",
    "7. **Router checks cache index**: Worker 1 has matching prefix â†’ routes to Worker 1\n",
    "8. **Worker 1 reuses cache**: Blocks 0-5 already computed, only processes new tokens\n",
    "\n",
    "**Why NATS?** Kubernetes provides service discovery (which workers exist) but not cache coordination (what's cached where). NATS handles thousands of cache events per second with low latency.\n",
    "\n",
    "### When KV-Aware Routing Helps\n",
    "\n",
    "**Best for:**\n",
    "- âœ… Chatbots and conversational AI (repeated system prompts, conversation history)\n",
    "- âœ… Document Q&A (same document, different questions)\n",
    "- âœ… Batch processing with shared prefixes\n",
    "- âœ… Code assistants (repeatedly analyzing same files)\n",
    "\n",
    "**Not ideal for:**\n",
    "- âš ï¸ Completely unique prompts every time\n",
    "- âš ï¸ Single worker deployments (no routing decisions to make)\n",
    "- âš ï¸ Very short contexts (cache overhead exceeds benefit)\n",
    "\n",
    "### Understanding Multi-GPU/Multi-Node Benefits\n",
    "\n",
    "**In this lab (single node, 2 GPUs):**\n",
    "- Each GPU runs a separate worker\n",
    "- Router can direct requests to the worker with the best KV cache match\n",
    "- Workers store their own local KV cache (no transfer between workers in data-parallel mode)\n",
    "\n",
    "> **ðŸ’¡ Note: Production Multi-Node Deployments**\n",
    "> \n",
    "> In production environments with multiple nodes:\n",
    "> - Scale workers across nodes for higher throughput\n",
    "> - Deploy multiple frontend replicas for high availability\n",
    "> - Kubernetes Services automatically load balance across frontend replicas\n",
    "> - KV-aware routing works across nodes via NATS coordination\n",
    "\n",
    "---\n",
    "\n",
    "## Section 2: Deploy NATS for Cache Coordination\n",
    "\n",
    "Now that you understand how KV-aware routing works, let's deploy NATS to enable cache coordination between workers and the router.\n",
    "\n",
    "### Step 1: Add NATS Helm Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b4acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Add NATS Helm repository\n",
    "echo \"Adding NATS Helm repository...\"\n",
    "helm repo add nats https://nats-io.github.io/k8s/helm/charts/\n",
    "helm repo update\n",
    "\n",
    "echo \"âœ“ NATS repository added\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489bc563",
   "metadata": {},
   "source": [
    "### Step 2: Install NATS with JetStream\n",
    "\n",
    "JetStream provides persistent event storage, allowing routers to recover cache state after restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for NATS\n",
    "kubectl create namespace nats-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Create NATS values file with JetStream configuration\n",
    "cat > /tmp/nats-values.yaml <<EOF\n",
    "nats:\n",
    "  jetstream:\n",
    "    enabled: true\n",
    "    fileStore:\n",
    "      enabled: true\n",
    "      dir: /data\n",
    "      pvc:\n",
    "        enabled: true\n",
    "        size: 10Gi\n",
    "\n",
    "config:\n",
    "  merge:\n",
    "    jetstream:\n",
    "      max_file_store: 10737418240  # 10GB in bytes\n",
    "      store_dir: /data\n",
    "EOF\n",
    "\n",
    "# Install NATS with JetStream enabled\n",
    "echo \"Installing NATS with JetStream...\"\n",
    "helm upgrade --install nats nats/nats \\\n",
    "  --namespace nats-system \\\n",
    "  --values /tmp/nats-values.yaml \\\n",
    "  --wait\n",
    "\n",
    "echo \"âœ“ NATS installed successfully\"\n",
    "echo \"  Connection: nats://nats.nats-system:4222\"\n",
    "echo \"  JetStream: Enabled with 10Gi file storage\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af625f6",
   "metadata": {},
   "source": [
    "### Step 3: Verify NATS Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS pods\n",
    "echo \"Checking NATS deployment...\"\n",
    "kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking NATS service...\"\n",
    "kubectl get svc -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Expected output:\"\n",
    "echo \"  - Pod: nats-0 (1/1 Running)\"\n",
    "echo \"  - Service: nats (ClusterIP, port 4222)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f29c0f0",
   "metadata": {},
   "source": [
    "### Step 4: Test NATS Connectivity (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7748d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Quick connectivity test using nats-box\n",
    "echo \"Testing NATS connectivity...\"\n",
    "\n",
    "# Create a test Job\n",
    "kubectl apply -f - <<EOF\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: nats-test\n",
    "  namespace: nats-system\n",
    "spec:\n",
    "  ttlSecondsAfterFinished: 30\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: nats-box\n",
    "        image: natsio/nats-box:latest\n",
    "        command:\n",
    "        - nats\n",
    "        - pub\n",
    "        - -s\n",
    "        - nats://nats.nats-system:4222\n",
    "        - test\n",
    "        - \"Hello from NATS test\"\n",
    "      restartPolicy: Never\n",
    "  backoffLimit: 2\n",
    "EOF\n",
    "\n",
    "# Wait for job to complete\n",
    "echo \"Waiting for test to complete...\"\n",
    "kubectl wait --for=condition=complete --timeout=30s job/nats-test -n nats-system 2>/dev/null\n",
    "\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"\"\n",
    "    echo \"âœ“ NATS connectivity test successful\"\n",
    "    echo \"  Published test message to NATS server\"\n",
    "else\n",
    "    echo \"\"\n",
    "    echo \"âš ï¸ NATS connectivity test failed or timed out\"\n",
    "    kubectl logs -n nats-system job/nats-test 2>/dev/null\n",
    "fi\n",
    "\n",
    "# Cleanup will happen automatically after 30 seconds (ttlSecondsAfterFinished)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a353f34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Environment Setup\n",
    "\n",
    "### Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables (use defaults if not already set)\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ðŸŒ² Lab 3: KV-Aware Routing Configuration\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Release Version:  $RELEASE_VERSION\"\n",
    "echo \"  Namespace:        $NAMESPACE\"\n",
    "echo \"  Node IP:          $NODE_IP\"\n",
    "echo \"  NATS:             nats://nats.nats-system:4222\"\n",
    "echo \"\"\n",
    "echo \"âœ“ Environment configured for KV-aware routing\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19b753",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pre-Deployment: Check GPU Availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088c79fa",
   "metadata": {},
   "source": [
    "### Step 1: Check Current GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"=== Checking GPU Availability ===\"\n",
    "echo \"\"\n",
    "echo \"Total GPUs on this node:\"\n",
    "kubectl get nodes -o custom-columns=NAME:.metadata.name,GPUs:.status.capacity.nvidia\\\\.com/gpu\n",
    "\n",
    "echo \"\"\n",
    "echo \"Currently allocated GPUs:\"\n",
    "kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[].resources.limits.\"nvidia.com/gpu\" != null) | \"\\(.metadata.namespace)/\\(.metadata.name): \\(.spec.containers[].resources.limits.\"nvidia.com/gpu\") GPU(s)\"'\n",
    "\n",
    "echo \"\"\n",
    "echo \"GPU requests by namespace:\"\n",
    "kubectl get pods -A -o json | jq -r '.items | group_by(.metadata.namespace) | .[] | \"\\(.[0].metadata.namespace): \\([.[] | .spec.containers[].resources.limits.\"nvidia.com/gpu\" // \"0\"] | add) GPU(s)\"' | grep -v \": 0 GPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b670f71",
   "metadata": {},
   "source": [
    "### Step 2: Delete Lab 1 Deployment (If Still Running)\n",
    "\n",
    "**âš ï¸ WARNING**: If the Lab 1 model deployment is still running, you MUST delete it first to free GPUs for Lab 3.\n",
    "\n",
    "Lab 3 requires:\n",
    "- **2 GPUs** for 2 data-parallel workers (1 GPU each)\n",
    "\n",
    "If you have only 2 GPUs total and Lab 1's deployment is using them, delete it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5121dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"Checking for Lab 1 deployment...\"\n",
    "if kubectl get dynamographdeployment vllm-disagg-router -n $NAMESPACE &>/dev/null; then\n",
    "    echo \"\"\n",
    "    echo \"âš ï¸  Lab 1 deployment (vllm-disagg-router) is still running!\"\n",
    "    echo \"   Deleting it to free GPUs for Lab 3...\"\n",
    "    echo \"\"\n",
    "    \n",
    "    # Delete the deployment\n",
    "    kubectl delete dynamographdeployment vllm-disagg-router -n $NAMESPACE\n",
    "    kubectl delete svc vllm-frontend-nodeport -n $NAMESPACE 2>/dev/null || true\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"âœ“ Lab 1 deployment deleted - waiting for pods to terminate...\"\n",
    "    kubectl wait --for=delete pod -l nvidia.com/dynamo-graph-deployment-name=vllm-disagg-router -n $NAMESPACE --timeout=60s 2>/dev/null || true\n",
    "    \n",
    "    echo \"âœ“ GPUs freed for Lab 3\"\n",
    "else\n",
    "    echo \"âœ“ Lab 1 deployment not found - GPUs should be available\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff540b1",
   "metadata": {},
   "source": [
    "### Step 3: Verify GPUs Are Available\n",
    "\n",
    "After deleting Lab 1's deployment (if needed), verify GPUs are free:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"=== Final GPU Check ===\"\n",
    "kubectl get nodes -o custom-columns=NAME:.metadata.name,TOTAL:.status.capacity.nvidia\\\\.com/gpu,ALLOCATABLE:.status.allocatable.nvidia\\\\.com/gpu\n",
    "\n",
    "echo \"\"\n",
    "echo \"If ALLOCATABLE shows 2 GPUs, you're ready for Lab 3!\"\n",
    "echo \"If ALLOCATABLE shows 0, pods are still terminating - wait 30 seconds and re-run.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec4b32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Deploy Data-Parallel Workers with KV-Aware Routing\n",
    "\n",
    "Now let's deploy 2 identical workers with a KV-aware router that uses NATS for cache coordination.\n",
    "\n",
    "### Step 1: Create Data-Parallel Deployment with KV-Aware Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584831be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Create deployment with KV-aware routing\n",
    "echo \"Creating data-parallel deployment with KV-aware routing...\"\n",
    "\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: nvidia.com/v1alpha1\n",
    "kind: DynamoGraphDeployment\n",
    "metadata:\n",
    "  name: vllm-kv-demo\n",
    "  namespace: ${NAMESPACE}\n",
    "spec:\n",
    "  services:\n",
    "    Frontend:\n",
    "      dynamoNamespace: vllm-kv-demo\n",
    "      componentType: frontend\n",
    "      replicas: 1\n",
    "      extraPodSpec:\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:${RELEASE_VERSION}\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - |\n",
    "              python3 -m dynamo.frontend \\\\\n",
    "                --http-port 8000 \\\\\n",
    "                --router-mode kv \\\\\n",
    "                --kv-overlap-score-weight 1.0\n",
    "          env:\n",
    "            - name: NATS_SERVER\n",
    "              value: \"nats://nats.nats-system:4222\"\n",
    "            - name: DYN_LOG\n",
    "              value: info\n",
    "    VllmWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-kv-demo\n",
    "      componentType: worker\n",
    "      replicas: 2\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: info\n",
    "        - name: NATS_SERVER\n",
    "          value: \"nats://nats.nats-system:4222\"\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: /data/huggingface-cache\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:${RELEASE_VERSION}\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --tensor-parallel-size 1 --enable-prefix-caching --enable-local-indexer true\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Data-parallel deployment created with KV-aware routing\"\n",
    "echo \"  Deployment: vllm-kv-demo\"\n",
    "echo \"  Workers: 2 (identical, data parallel)\"\n",
    "echo \"  Router Mode: kv (cache-aware)\"\n",
    "echo \"  NATS: nats://nats.nats-system:4222\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb54f46",
   "metadata": {},
   "source": [
    "**Key Configuration Flags:**\n",
    "\n",
    "**Frontend:**\n",
    "- `--router-mode kv`: Enables KV-aware routing\n",
    "- `--kv-overlap-score-weight 1.0`: Balances cache hits vs load distribution\n",
    "- `NATS_SERVER`: Connection to NATS for subscribing to cache events\n",
    "\n",
    "**Workers:**\n",
    "- `--enable-prefix-caching`: Enables cache block tracking and event publishing\n",
    "- `NATS_SERVER`: Where to publish cache events\n",
    "- `--tensor-parallel-size 1`: Each worker uses 1 GPU (not splitting model)\n",
    "\n",
    "### Step 2: Create NodePort Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846853f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Create NodePort service\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: vllm-kv-frontend-np\n",
    "  namespace: ${NAMESPACE}\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    nvidia.com/dynamo-component: Frontend\n",
    "    nvidia.com/dynamo-graph-deployment-name: vllm-kv-demo\n",
    "  ports:\n",
    "  - port: 8000\n",
    "    targetPort: 8000\n",
    "    nodePort: 30200\n",
    "    protocol: TCP\n",
    "    name: http\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ NodePort service created on port 30200\"\n",
    "echo \"  Access at: http://$NODE_IP:30200\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0546487f",
   "metadata": {},
   "source": [
    "### Step 3: Wait for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c17db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Wait for pods to be ready\n",
    "echo \"Waiting for deployment...\"\n",
    "echo \"This may take 2-3 minutes for model download and initialization...\"\n",
    "echo \"\"\n",
    "\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Wait for pods to be ready\n",
    "kubectl wait --for=condition=ready --timeout=300s \\\n",
    "  pods -l nvidia.com/dynamo-graph-deployment-name=vllm-kv-demo \\\n",
    "  -n $NAMESPACE 2>/dev/null || echo \"Pods are initializing...\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Deployment status:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-kv-demo\n",
    "\n",
    "echo \"\"\n",
    "echo \"Expected pods:\"\n",
    "echo \"  - vllm-kv-demo-frontend-xxxxx (Frontend with KV-aware router)\"\n",
    "echo \"  - vllm-kv-demo-vllmworker-xxxxx (Worker 1)\"\n",
    "echo \"  - vllm-kv-demo-vllmworker-xxxxx (Worker 2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b9274",
   "metadata": {},
   "source": [
    "### Step 4: Test Basic Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d17c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Test the deployment\n",
    "echo \"Testing inference...\"\n",
    "export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is AI?\"}],\n",
    "    \"max_tokens\": 50\n",
    "  }' | jq -r '.choices[0].message.content'\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Deployment is serving requests\"\n",
    "echo \"  Router: KV-aware (tracking cache state)\"\n",
    "echo \"  Workers: Publishing cache events to NATS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e90129",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Demonstrate Cache-Aware Routing\n",
    "\n",
    "Now we'll demonstrate KV-aware routing by sending requests with shared prefixes. The router should direct these to the same worker for cache reuse.\n",
    "\n",
    "### Step 1: Send Requests to Verify Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eafb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Sending test requests to verify KV-aware routing...\"\n",
    "echo \"\"\n",
    "\n",
    "# Request 1\n",
    "echo \"Request 1:\"\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is quantum computing?\"}],\n",
    "    \"max_tokens\": 30\n",
    "  }' | jq -r '.choices[0].message.content'\n",
    "\n",
    "echo \"\"\n",
    "sleep 1\n",
    "\n",
    "# Request 2\n",
    "echo \"Request 2:\"\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n",
    "    \"max_tokens\": 30\n",
    "  }' | jq -r '.choices[0].message.content'\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Routing is working (requests processed successfully)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ca550c",
   "metadata": {},
   "source": [
    "**Note on Measuring Cache Benefits**:\n",
    "With Qwen 1.5B (small, fast model) and short prompts (~3-4 tokens shared prefix), cache benefits are **not visible in wall-clock time**. Here's why:\n",
    "\n",
    "- **TTFT savings**: Caching 3-4 tokens saves ~2-5ms of prefill time\n",
    "- **Total request time**: ~300-350ms (includes TTFT + generation + network + JSON processing)\n",
    "- **Cache benefit**: <2% of total time (masked by generation and latency)\n",
    "\n",
    "**When cache benefits ARE visible:**\n",
    "- **Larger models** (7B+, 70B+): Prefill is much more expensive, cache savings are measurable\n",
    "- **Longer shared prefixes**: System prompts (20-50+ tokens), document contexts (100s of tokens)\n",
    "- **High concurrency**: Routing efficiency and memory savings matter at scale\n",
    "- **Specialized tools**: Benchmarking tools like AI-Perf can isolate TTFT from total time\n",
    "\n",
    "For this workshop with 2 GPUs and a 1.5B model, KV-aware routing is **working correctly** (NATS connected, router in KV mode, cache enabled), but timing improvements are too small to measure with `curl`.\n",
    "\n",
    "### Step 2: Visualize KV-Aware Routing in Action\n",
    "\n",
    "This script sends requests and shows the frontend's routing decisions, including which worker handles each request and how many cached blocks are reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7040c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Get frontend pod name\n",
    "FRONTEND=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=Frontend -o jsonpath='{.items[0].metadata.name}')\n",
    "\n",
    "echo \"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\"\n",
    "echo \"â•‘         KV-Aware Routing Visualization                        â•‘\"\n",
    "echo \"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\n",
    "echo \"\"\n",
    "echo \"Frontend: $FRONTEND\"\n",
    "echo \"\"\n",
    "\n",
    "# Function to show latest routing decision from logs\n",
    "show_routing() {\n",
    "  echo \"   [ROUTING] $(kubectl logs -n $NAMESPACE $FRONTEND --tail=20 | grep 'Selected worker' | tail -1 | sed 's/.*worker_id=/Worker: /; s/ dp_rank.*cached blocks:/ | Cached blocks:/; s/,.*//; s/ tree.*//')\"\n",
    "}\n",
    "\n",
    "echo \"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\n",
    "echo \"Sending Test Requests\"\n",
    "echo \"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\n",
    "echo \"\"\n",
    "\n",
    "# Request 1 - Physics prefix\n",
    "echo \"ðŸ“¤ Request 1: Physics tutor + 'What is gravity?'\"\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": \"You are a physics tutor.\"},\n",
    "      {\"role\": \"user\", \"content\": \"What is gravity?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 20\n",
    "  }' | jq -r '.choices[0].message.content | .[0:50]'\n",
    "\n",
    "sleep 1\n",
    "show_routing\n",
    "echo \"\"\n",
    "\n",
    "# Request 2 - Same physics prefix\n",
    "echo \"ðŸ“¤ Request 2: Physics tutor + 'What is velocity?' (SAME PREFIX)\"\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": \"You are a physics tutor.\"},\n",
    "      {\"role\": \"user\", \"content\": \"What is velocity?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 20\n",
    "  }' | jq -r '.choices[0].message.content | .[0:50]'\n",
    "\n",
    "sleep 1\n",
    "show_routing\n",
    "echo \"\"\n",
    "\n",
    "# Request 3 - Different prefix\n",
    "echo \"ðŸ“¤ Request 3: Math tutor + 'What is algebra?' (DIFFERENT PREFIX)\"\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
    "      {\"role\": \"user\", \"content\": \"What is algebra?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 20\n",
    "  }' | jq -r '.choices[0].message.content | .[0:50]'\n",
    "\n",
    "sleep 1\n",
    "show_routing\n",
    "echo \"\"\n",
    "\n",
    "# Request 4 - Back to physics\n",
    "echo \"ðŸ“¤ Request 4: Physics tutor + 'Explain momentum' (BACK TO PHYSICS)\"\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": \"You are a physics tutor.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Explain momentum\"}\n",
    "    ],\n",
    "    \"max_tokens\": 20\n",
    "  }' | jq -r '.choices[0].message.content | .[0:50]'\n",
    "\n",
    "sleep 1\n",
    "show_routing\n",
    "echo \"\"\n",
    "\n",
    "echo \"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\n",
    "echo \"âœ“ Check the [ROUTING] lines above:\"\n",
    "echo \"  - Requests 1, 2, and 4 (physics) should use the SAME Worker ID\"\n",
    "echo \"  - Request 3 (math) may use a DIFFERENT Worker ID\"\n",
    "echo \"  - 'Cached blocks' increases when router reuses cached prefixes\"\n",
    "echo \"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811bb710",
   "metadata": {},
   "source": [
    "**Understanding the Output:**\n",
    "\n",
    "You'll see `[ROUTING]` lines showing the routing decisions. Here's what you're likely to observe:\n",
    "\n",
    "```\n",
    "[ROUTING] Worker: 2575905244297037343 | Cached blocks: 1     â† Request 1 (physics)\n",
    "[ROUTING] Worker: 2575905244297037343 | Cached blocks: 1     â† Request 2 (physics - SAME WORKER)\n",
    "[ROUTING] Worker: 2575905244297037343 | Cached blocks: 1     â† Request 3 (math - SAME WORKER)\n",
    "[ROUTING] Worker: 2575905244297037343 | Cached blocks: 1     â† Request 4 (physics - SAME WORKER)\n",
    "```\n",
    "\n",
    "**What this shows:**\n",
    "\n",
    "1. **Consistent Worker ID** - All requests route to the same worker\n",
    "   - This is CORRECT behavior! With light load, the router efficiently uses one worker\n",
    "   - The worker has capacity, so the router doesn't need to distribute across both GPUs\n",
    "   - This is more efficient than round-robin routing\n",
    "\n",
    "2. **Cached blocks: 1** - Prefix caching is working\n",
    "   - `Cached blocks: 1` means the router found 1+ matching blocks in the cache tree\n",
    "   - With short system prompts (5-10 tokens), you see small cache block counts\n",
    "   - The fact it's consistently `1` and not `0` proves prefix caching is active\n",
    "\n",
    "3. **Why not using both workers?**\n",
    "   - KV-aware routing is SMART: it prefers to use one worker when possible\n",
    "   - Only distributes across workers when load increases or if specific prefixes are pinned elsewhere\n",
    "   - This reduces communication overhead and maximizes cache efficiency\n",
    "\n",
    "**To see multi-worker distribution:**\n",
    "\n",
    "Send many concurrent requests or very long sequences to saturate one worker. Under load, you'll see:\n",
    "```\n",
    "[ROUTING] Worker: 2575905244297037343 | Cached blocks: 1     â† Worker 1 handling physics\n",
    "[ROUTING] Worker: 14409932740882684000 | Cached blocks: 0    â† Worker 2 taking overflow\n",
    "```\n",
    "\n",
    "This proves KV-aware routing is working! The router intelligently tracks which worker has which prefixes cached and directs requests accordingly, maximizing cache reuse and GPU efficiency.\n",
    "\n",
    "### Optional: Experiment with Load Testing\n",
    "\n",
    "âš ï¸ **IMPORTANT: Run these commands in a TERMINAL (not in the notebook). AI-Perf can be resource-intensive.**\n",
    "\n",
    "After completing Step 2 and confirming routing is working, you can experiment with load testing to see multi-worker distribution under concurrent load.\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "1. Make sure your Lab 3 deployment is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2113be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kubectl get pods -n dynamo -l nvidia.com/dynamo-graph-deployment-name=vllm-kv-demo\n",
    "# All pods should be Running (1/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f776490",
   "metadata": {},
   "source": [
    "2. Verify the NodePort service is accessible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "curl -s http://$NODE_IP:30200/health || echo \"Service not ready yet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b809de9",
   "metadata": {},
   "source": [
    "#### Install AI-Perf\n",
    "\n",
    "If not already installed, run in a terminal:\n",
    "\n",
    "```\n",
    "pip install -q aiperf\n",
    "```\n",
    "\n",
    "#### Run Benchmarks in a Terminal\n",
    "\n",
    "Copy and paste these commands into a terminal (not executable in notebook):\n",
    "\n",
    "```\n",
    "# Set up endpoint\n",
    "export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "export FRONTEND_URL=\"http://$NODE_IP:30200\"\n",
    "\n",
    "# Low concurrency baseline (1 concurrent request, 100 total)\n",
    "aiperf profile \\\n",
    "  --model Qwen/Qwen2.5-1.5B-Instruct \\\n",
    "  --url $FRONTEND_URL \\\n",
    "  --endpoint-type chat \\\n",
    "  --streaming \\\n",
    "  --concurrency 1 \\\n",
    "  --request-count 100\n",
    "\n",
    "# High concurrency (4 concurrent requests, 200 total)\n",
    "# This is where you'll see multi-worker distribution!\n",
    "aiperf profile \\\n",
    "  --model Qwen/Qwen2.5-1.5B-Instruct \\\n",
    "  --url $FRONTEND_URL \\\n",
    "  --endpoint-type chat \\\n",
    "  --streaming \\\n",
    "  --concurrency 4 \\\n",
    "  --request-count 200\n",
    "\n",
    "# Sustained request rate (10 requests/sec, 200 total)\n",
    "aiperf profile \\\n",
    "  --model Qwen/Qwen2.5-1.5B-Instruct \\\n",
    "  --url $FRONTEND_URL \\\n",
    "  --endpoint-type chat \\\n",
    "  --streaming \\\n",
    "  --request-rate 10 \\\n",
    "  --request-count 200\n",
    "```\n",
    "\n",
    "#### What to Observe\n",
    "\n",
    "- TTFT (Time To First Token) - lower is better\n",
    "- Request throughput - higher concurrency should utilize both workers\n",
    "- At high concurrency, check frontend logs to see requests distributed across both workers\n",
    "\n",
    "#### Troubleshooting\n",
    "\n",
    "- **Connection refused error**: Ensure Lab 3 deployment is running and service is created (Section 4 Step 2)\n",
    "- **Timeout errors**: Model may still be loading, wait 30 seconds and retry\n",
    "- **High error rate**: Check pod logs for issues: `kubectl logs -n dynamo -l nvidia.com/dynamo-component=Frontend`\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've deployed KV-aware routing with data-parallel workers, where the router intelligently directs requests to workers based on their cached data.\n",
    "\n",
    "**What you learned:**\n",
    "- NATS coordinates cache state across workers (events published/subscribed)\n",
    "- Router tracks which workers have cached which prefixes\n",
    "- Requests with similar prefixes get routed to the same worker for cache reuse\n",
    "- Scales horizontallyâ€”add more workers for more traffic\n",
    "- Works on single nodes with multiple GPUs or across multi-node clusters\n",
    "\n",
    "**Key architectural choice:**\n",
    "- Use **disaggregated serving** (Lab 1) for predictable latency with separate prefill/decode\n",
    "- Use **KV-aware routing** (Lab 3) when you have high traffic with cache-friendly patterns (system prompts, document contexts)\n",
    "\n",
    "**Next steps:** Experiment with different worker counts, or monitor cache hit rates in Grafana.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Core Documentation\n",
    "\n",
    "- **NVIDIA Dynamo Documentation**: https://docs.nvidia.com/dynamo/latest/\n",
    "- **Dynamo Deployment Guide**: https://docs.nvidia.com/dynamo/latest/kubernetes/deployment/\n",
    "- **Grove Operator Guide**: https://docs.nvidia.com/dynamo/latest/kubernetes/grove.html\n",
    "- **Dynamo v0.8.0 Release Notes**: https://github.com/ai-dynamo/dynamo/releases/tag/v0.8.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73215c52",
   "metadata": {},
   "source": [
    "### Community Resources\n",
    "\n",
    "- **Dynamo GitHub**: https://github.com/ai-dynamo/dynamo\n",
    "- **NVIDIA Developer Forums**: https://forums.developer.nvidia.com/\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've completed Lab 3: KV-Aware Routing** ðŸŽ¯\n",
    "\n",
    "You now understand how KV-aware routing works, how NATS coordinates cache state across workers, and how intelligent request placement can improve cache hit rates for workloads with repeated prompt patterns!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-kernelspec,-widgets,-language_info"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
