{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da45a4c9",
   "metadata": {},
   "source": [
    "# Lab 3: Distributed Dynamo with Multi-GPU/Multi-Node Serving\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you'll learn about **KV-aware routing**, an intelligent load balancing feature that routes requests to workers based on their cached data. Unlike simple round-robin routing, KV-aware routing tracks which workers have already processed similar prompts and directs new requests to workers with matching cached blocks. This dramatically reduces the time to first token (TTFT) for repeated or similar queries.\n",
    "\n",
    "**The Architecture:**\n",
    "- **2 Independent Workers** (GPUs 0-1): Each handles full inference (prefill + decode)\n",
    "- **KV-Aware Router**: Tracks which worker has cached which prompt prefixes\n",
    "- **NATS Message Bus**: Coordinates cache state across workers\n",
    "- Each worker stores its own local KV cache and publishes cache events to NATS\n",
    "\n",
    "**How KV-Aware Routing Works:**\n",
    "1. Request with prompt \"Explain quantum computing\" arrives\n",
    "2. Router checks: No worker has this cached ‚Üí sends to Worker 1\n",
    "3. Worker 1 processes request and caches the prefill computation\n",
    "4. Worker 1 publishes cache event to NATS: \"I have blocks for 'Explain quantum computing'\"\n",
    "5. Router updates its tracking: Worker 1 has those cached blocks\n",
    "6. Next request: \"Explain quantum computing in simple terms\" arrives\n",
    "7. Router sees: Worker 1 has cached blocks for \"Explain quantum computing\" ‚Üí sends to Worker 1\n",
    "8. Worker 1 reuses cached prefill blocks ‚Üí much faster TTFT!\n",
    "\n",
    "**Why This Matters:**\n",
    "When users ask variations of similar questions, the router intelligently directs requests to workers that have already cached related computations. This avoids redundant prefill work and can reduce TTFT by 50% or more for cache-friendly workloads.\n",
    "\n",
    "**How this differs from Lab 1:**\n",
    "- **Lab 1**: Disaggregated serving (specialized prefill ‚Üí decode workers)\n",
    "- **Lab 3**: Data-parallel serving (2 generalist workers, each does everything)\n",
    "- **Lab 1**: Workers cooperate on each request (tightly coupled pipeline)\n",
    "- **Lab 3**: Workers operate independently, router makes intelligent placement decisions\n",
    "- **Lab 1**: NIXL for prefill‚Üídecode KV cache transfer\n",
    "- **Lab 3**: Local KV cache per worker, NATS for cache event coordination\n",
    "- **Lab 1**: No NATS needed (direct worker communication via NIXL)\n",
    "- **Lab 3**: NATS required for router to track cache state across workers\n",
    "\n",
    "**When to use KV-aware routing:**\n",
    "- Chatbots with conversation history (similar context across turns)\n",
    "- Document Q&A systems (multiple questions about the same document)\n",
    "- Batch processing with shared system prompts\n",
    "- Any workload where prompt prefixes are repeated across requests\n",
    "\n",
    "**Prerequisites**: Complete Lab 1 (Dynamo Deployment) and Lab 2 (Monitoring)\n",
    "\n",
    "**Duration**: ~60 minutes\n",
    "\n",
    "**Note**: Requires 2 GPUs. If Lab 1 is still running, you'll need to clean it up first.\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Understanding KV-Aware Routing\n",
    "\n",
    "### What is KV-Aware Routing?\n",
    "\n",
    "Traditional load balancers distribute requests randomly or in round-robin fashion across workers, treating all workers as identical. But large language models cache intermediate computations (the \"KV cache\") to avoid reprocessing tokens they've already seen. **KV-aware routing** leverages this by tracking which workers have which cached blocks and intelligently routing requests to workers that can reuse cached data.\n",
    "\n",
    "**Example Scenario:**\n",
    "1. User asks: \"Explain quantum computing\" ‚Üí Router sends to Worker 1\n",
    "2. Worker 1 processes the prompt and caches it\n",
    "3. User follows up: \"Explain quantum computing in simple terms\" ‚Üí Router notices the shared prefix and sends to Worker 1\n",
    "4. Worker 1 reuses the cached computation for \"Explain quantum computing\", only processes the new part\n",
    "5. Result: **Much faster** time-to-first-token (TTFT)\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "```\n",
    "Client Request\n",
    "      ‚Üì\n",
    "Frontend (OpenAI API)\n",
    "      ‚Üì\n",
    "KV-Aware Router ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ NATS (KV Events) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ Workers publish cache events\n",
    "      ‚Üì                                              ‚Üì\n",
    "   Analyzes:                                   \"I cached blocks 0-5\"\n",
    "   - Input tokens                              \"I removed block 3\"\n",
    "   - Cached blocks per worker                  \"I stored block 10\"\n",
    "   - Worker load\n",
    "      ‚Üì\n",
    "   Selects best worker\n",
    "      ‚Üì\n",
    "Worker 1 or Worker 2 (Data Parallel - identical)\n",
    "      ‚Üì\n",
    "Response to Client\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Frontend**: OpenAI-compatible API server\n",
    "2. **KV-Aware Router**: Tracks global cache state, selects optimal worker\n",
    "3. **NATS**: Message bus for cache event coordination\n",
    "4. **Workers**: Identical inference engines (vLLM/SGLang/TensorRT-LLM)\n",
    "\n",
    "### How Cache Events Work\n",
    "\n",
    "Each worker publishes events to NATS when cache blocks are created or removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When a worker processes a request:\n",
    "worker.process_prompt(\"Explain quantum computing\")\n",
    "  ‚Üí Stores KV blocks 0, 1, 2, 3, 4, 5\n",
    "  ‚Üí Publishes to NATS: \"Worker-1: Stored blocks [0,1,2,3,4,5] with hash XYZ\"\n",
    "\n",
    "# Router receives event and updates its global view:\n",
    "router.cache_index[\"Worker-1\"][\"XYZ\"] = [0,1,2,3,4,5]\n",
    "\n",
    "# Next similar request arrives:\n",
    "new_request = \"Explain quantum computing in simple terms\"\n",
    "  ‚Üí Router checks: Which worker has matching prefix?\n",
    "  ‚Üí Finds: Worker-1 has blocks 0-5 matching this prefix\n",
    "  ‚Üí Routes to Worker-1 (avoids recomputation)\n",
    "\n",
    "# Worker-1 reuses cached blocks:\n",
    "worker1.process_request()\n",
    "  ‚Üí Blocks 0-5: CACHED (instant)\n",
    "  ‚Üí Blocks 6-10: NEW (compute only these)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510cd4f1",
   "metadata": {},
   "source": [
    "### Why NATS is Required\n",
    "\n",
    "**The Problem**: How does the router know which worker has which cache blocks?\n",
    "\n",
    "**The Solution**: Workers publish cache events to NATS, router subscribes to these events.\n",
    "\n",
    "**Why not just Kubernetes?**\n",
    "- K8s APIs are for service discovery (which workers exist), not cache coordination (what's cached where)\n",
    "- Cache events happen thousands of times per second - too fast for K8s APIs\n",
    "- NATS provides low-latency pub/sub specifically designed for this use case\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Worker 1 ‚îÇ  ‚îÇ Worker 2 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "     ‚îÇ             ‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ Publish cache events\n",
    "            ‚Üì\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ  NATS Server   ‚îÇ\n",
    "   ‚îÇ  (Message Bus) ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ Subscribe to events\n",
    "            ‚Üì\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ  KV Router     ‚îÇ\n",
    "   ‚îÇ (Global Index) ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Data Parallelism vs Disaggregated Serving\n",
    "\n",
    "**Lab 1 (Disaggregated Serving):**\n",
    "- Prefill Worker ‚Üí Decode Worker (pipeline)\n",
    "- Workers are specialized (different roles)\n",
    "- Tight coupling (must work together)\n",
    "- 1 request = 1 prefill worker + 1 decode worker\n",
    "\n",
    "**Lab 3 (Data Parallel with KV-Aware Routing):**\n",
    "- Worker 1, Worker 2 (both identical)\n",
    "- Workers are independent (same role)\n",
    "- Loose coupling (work in parallel)\n",
    "- 1 request = 1 worker (router chooses which)\n",
    "\n",
    "### When KV-Aware Routing Helps\n",
    "\n",
    "**High Cache Hit Workloads:**\n",
    "- ‚úÖ Chatbots (conversation history repeats)\n",
    "- ‚úÖ Document Q&A (same document, multiple questions)\n",
    "- ‚úÖ Code assistants (repeatedly analyzing same files)\n",
    "- ‚úÖ System prompt reuse (same instructions for many requests)\n",
    "\n",
    "**Low Cache Hit Workloads:**\n",
    "- ‚ö†Ô∏è Random questions (no shared context)\n",
    "- ‚ö†Ô∏è One-shot requests (no follow-ups)\n",
    "- ‚ö†Ô∏è Completely unique prompts each time\n",
    "\n",
    "### Performance Impact\n",
    "\n",
    "**Without KV-Aware Routing (Round-Robin):**\n",
    "```\n",
    "Request 1: \"Explain AI\" ‚Üí Worker 1 (cache miss, slow TTFT)\n",
    "Request 2: \"Explain AI in detail\" ‚Üí Worker 2 (cache miss, slow TTFT)\n",
    "Request 3: \"Explain AI simply\" ‚Üí Worker 3 (cache miss, slow TTFT)\n",
    "Average TTFT: 500ms\n",
    "```\n",
    "\n",
    "**With KV-Aware Routing:**\n",
    "```\n",
    "Request 1: \"Explain AI\" ‚Üí Worker 1 (cache miss, slow TTFT: 500ms)\n",
    "Request 2: \"Explain AI in detail\" ‚Üí Worker 1 (cache hit, fast TTFT: 50ms)\n",
    "Request 3: \"Explain AI simply\" ‚Üí Worker 1 (cache hit, fast TTFT: 50ms)\n",
    "Average TTFT: 200ms (60% improvement!)\n",
    "```\n",
    "- In K8s-native mode: Routing metadata shared via API or direct communication\n",
    "- In NATS mode: NATS shares metadata about cache state\n",
    "- Enables intelligent request routing to workers with relevant cached data\n",
    "- Dramatically reduces prefill latency when cache hits occur\n",
    "\n",
    "**Optional NATS/etcd (for extreme scale)**: Advanced coordination:\n",
    "- **NATS**: Pub/sub messaging for metadata (cache events, routing tables)\n",
    "- **etcd**: Distributed configuration and service discovery\n",
    "- **When to use**: Very large clusters, multi-region, custom routing policies\n",
    "- **Note**: NATS does NOT transfer KV cache data (NIXL does that)\n",
    "- Router directs requests to workers with relevant cached prefixes\n",
    "- Improves cache hit rates even on single node with multiple GPUs\n",
    "- Workers transfer actual cache data via NIXL when needed\n",
    "\n",
    "### Understanding Multi-GPU/Multi-Node Benefits\n",
    "\n",
    "**In this lab (single node, 2 GPUs):**\n",
    "- Each GPU runs a separate worker\n",
    "- Router can direct requests to the worker with the best KV cache match\n",
    "- NIXL can transfer cache data between workers on the same node\n",
    "\n",
    "**In production (multi-node):**\n",
    "- Scale workers across multiple nodes\n",
    "- Scale frontends for high availability (multiple frontend replicas)\n",
    "- NIXL transfers cache data between nodes over the network (RDMA/TCP)\n",
    "- Kubernetes Services automatically load balance traffic across frontend replicas\n",
    "\n",
    "### When to Use Distributed Dynamo\n",
    "\n",
    "| Scenario | Use Distributed Dynamo? | Why |\n",
    "|----------|-----------|-----|\n",
    "| Single GPU | ‚ùå No | Adds overhead without benefit |\n",
    "| Multiple GPUs, single node | ‚úÖ Yes | KV-aware routing improves cache hits between GPU workers |\n",
    "| 2-3 nodes | ‚úÖ Yes | Cache awareness and coordination provide benefits |\n",
    "| 4+ nodes | ‚úÖ Strongly Yes | Significant performance improvements from distributed cache awareness |\n",
    "| High traffic, repeated queries | ‚úÖ Yes | Cache-aware routing reduces latency |\n",
    "| Low traffic, unique queries | ‚ö†Ô∏è Maybe | Lower cache hit rates, but coordination still useful |\n",
    "| Dynamo 0.8+ | ‚ÑπÔ∏è Info | Can use K8s-native discovery (no NATS/etcd required) for simple deployments |\n",
    "\n",
    "---\n",
    "\n",
    "## Section 2: Deploy NATS for Cache Coordination\n",
    "\n",
    "### Why NATS is Required\n",
    "\n",
    "KV-aware routing requires **NATS** (Neural Autonomic Transport System) to coordinate cache state across workers. This is fundamentally different from Lab 1, which used direct worker communication.\n",
    "\n",
    "**What NATS Does:**\n",
    "- Workers publish cache events (\"I stored block X\", \"I removed block Y\")\n",
    "- Router subscribes to these events to maintain a global cache index\n",
    "- Low-latency pub/sub messaging (< 1ms typically)\n",
    "- Handles thousands of events per second\n",
    "\n",
    "**Without NATS**: Router has no visibility into worker cache state ‚Üí random routing ‚Üí poor cache hit rates\n",
    "\n",
    "### Step 1: Add NATS Helm Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Add NATS Helm repository\n",
    "echo \"Adding NATS Helm repository...\"\n",
    "helm repo add nats https://nats-io.github.io/k8s/helm/charts/\n",
    "helm repo update\n",
    "\n",
    "echo \"‚úì NATS repository added\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0958ab",
   "metadata": {},
   "source": [
    "### Step 2: Install NATS with JetStream\n",
    "\n",
    "JetStream provides persistent event storage, allowing routers to recover cache state after restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd11796",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for NATS\n",
    "kubectl create namespace nats-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Install NATS with JetStream enabled\n",
    "echo \"Installing NATS with JetStream...\"\n",
    "helm upgrade --install nats nats/nats \\\n",
    "  --namespace nats-system \\\n",
    "  --set nats.jetstream.enabled=true \\\n",
    "  --set nats.jetstream.memStorage.enabled=true \\\n",
    "  --set nats.jetstream.memStorage.size=1Gi \\\n",
    "  --set nats.jetstream.fileStorage.enabled=true \\\n",
    "  --set nats.jetstream.fileStorage.size=2Gi \\\n",
    "  --wait\n",
    "\n",
    "echo \"‚úì NATS installed successfully\"\n",
    "echo \"  Connection: nats://nats.nats-system:4222\"\n",
    "echo \"  JetStream: Enabled (1Gi memory + 2Gi disk)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720fd53",
   "metadata": {},
   "source": [
    "**What this enables:**\n",
    "- Cache event streaming from workers\n",
    "- Persistent event storage (survives pod restarts)\n",
    "- Router state recovery after crashes\n",
    "\n",
    "### Step 3: Verify NATS Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS pods\n",
    "echo \"Checking NATS deployment...\"\n",
    "kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking NATS service...\"\n",
    "kubectl get svc -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Expected output:\"\n",
    "echo \"  - Pod: nats-0 (1/1 Running)\"\n",
    "echo \"  - Service: nats (ClusterIP, port 4222)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89dd31f",
   "metadata": {},
   "source": [
    "### Step 4: Test NATS Connectivity (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eee4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Quick connectivity test\n",
    "kubectl run -it --rm nats-test --image=natsio/nats-box:latest --restart=Never -- \\\n",
    "  nats-sub -s nats://nats.nats-system:4222 test\n",
    "\n",
    "# If successful, you'll see \"Subscribing on test\"\n",
    "# Press Ctrl+C to exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32242467",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Environment Setup\n",
    "\n",
    "### Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb86141",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables (use defaults if not already set)\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n",
    "echo \"üå≤ Lab 3: KV-Aware Routing Configuration\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n",
    "echo \"  Release Version:  $RELEASE_VERSION\"\n",
    "echo \"  Namespace:        $NAMESPACE\"\n",
    "echo \"  Node IP:          $NODE_IP\"\n",
    "echo \"  NATS:             nats://nats.nats-system:4222\"\n",
    "echo \"\"\n",
    "echo \"‚úì Environment configured for KV-aware routing\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffbbde",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pre-Deployment: Check GPU Availability\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL**: Lab 3 requires GPUs for data-parallel workers. Before proceeding, verify you have sufficient GPU resources available.\n",
    "\n",
    "### Step 1: Check Current GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"=== Checking GPU Availability ===\"\n",
    "echo \"\"\n",
    "echo \"Total GPUs on this node:\"\n",
    "kubectl get nodes -o custom-columns=NAME:.metadata.name,GPUs:.status.capacity.nvidia\\\\.com/gpu\n",
    "\n",
    "echo \"\"\n",
    "echo \"Currently allocated GPUs:\"\n",
    "kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[].resources.limits.\"nvidia.com/gpu\" != null) | \"\\(.metadata.namespace)/\\(.metadata.name): \\(.spec.containers[].resources.limits.\"nvidia.com/gpu\") GPU(s)\"'\n",
    "\n",
    "echo \"\"\n",
    "echo \"GPU requests by namespace:\"\n",
    "kubectl get pods -A -o json | jq -r '.items | group_by(.metadata.namespace) | .[] | \"\\(.[0].metadata.namespace): \\([.[] | .spec.containers[].resources.limits.\"nvidia.com/gpu\" // \"0\"] | add) GPU(s)\"' | grep -v \": 0 GPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128dc5c",
   "metadata": {},
   "source": [
    "### Step 2: Clean Up Lab 1 Deployment (If Still Running)\n",
    "\n",
    "**‚ö†Ô∏è WARNING**: If Lab 1 deployment is still running, you MUST delete it first to free GPUs for Lab 3.\n",
    "\n",
    "Lab 3 deployment requires:\n",
    "- **2 GPUs** for 2 data-parallel workers (1 GPU each)\n",
    "\n",
    "If you have only 2 GPUs total and Lab 1 is using them, run this cleanup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"Checking for Lab 1 deployment...\"\n",
    "if kubectl get dynamographdeployment vllm-disagg-router -n $NAMESPACE &>/dev/null; then\n",
    "    echo \"\"\n",
    "    echo \"‚ö†Ô∏è  Lab 1 deployment (vllm-disagg-router) is still running!\"\n",
    "    echo \"   This deployment is using GPUs needed for Lab 3.\"\n",
    "    echo \"\"\n",
    "    echo \"Delete Lab 1 deployment? (you can redeploy it later)\"\n",
    "    echo \"\"\n",
    "    echo \"Run: kubectl delete dynamographdeployment vllm-disagg-router -n $NAMESPACE\"\n",
    "    echo \"     kubectl delete svc vllm-frontend-nodeport -n $NAMESPACE\"\n",
    "    echo \"\"\n",
    "    echo \"Or press Ctrl+C to keep Lab 1 running (Lab 3 will fail if insufficient GPUs)\"\n",
    "else\n",
    "    echo \"‚úì Lab 1 deployment not found - GPUs should be available\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc7583",
   "metadata": {},
   "source": [
    "### Step 3: Verify GPUs Are Available\n",
    "\n",
    "After cleaning up Lab 1 (if needed), verify GPUs are free:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefcd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"=== Final GPU Check ===\"\n",
    "kubectl get nodes -o custom-columns=NAME:.metadata.name,TOTAL:.status.capacity.nvidia\\\\.com/gpu,ALLOCATABLE:.status.allocatable.nvidia\\\\.com/gpu\n",
    "\n",
    "echo \"\"\n",
    "echo \"If ALLOCATABLE shows 2 GPUs, you're ready for Lab 3!\"\n",
    "echo \"If ALLOCATABLE shows 0, pods are still terminating - wait 30 seconds and re-run.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c6a4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Deploy Data-Parallel Workers with KV-Aware Routing\n",
    "\n",
    "### Understanding the Deployment\n",
    "\n",
    "We'll deploy 2 identical workers (data parallelism) with a KV-aware router that tracks cache state via NATS.\n",
    "\n",
    "**Configuration:**\n",
    "- **Frontend**: 1 replica with `--router-mode kv` (enables cache-aware routing)\n",
    "- **Workers**: 2 replicas, each with 1 GPU, publishing cache events to NATS\n",
    "- **Architecture**: Data parallel (not disaggregated - no prefill/decode split)\n",
    "- **Cache Coordination**: NATS (workers publish events, router subscribes)\n",
    "\n",
    "**Key Differences from Lab 1:**\n",
    "\n",
    "| Aspect | Lab 1 (Disaggregated) | Lab 3 (Data Parallel + KV-Aware) |\n",
    "|--------|----------------------|-----------------------------------|\n",
    "| Workers | Prefill + Decode (specialized) | Worker 1 + Worker 2 (identical) |\n",
    "| Routing | Disaggregated router (prefill‚Üídecode) | KV-aware router (cache-based) |\n",
    "| Message Bus | Not needed | NATS (required) |\n",
    "| Worker Config | Different roles | Same role, different instances |\n",
    "\n",
    "### Step 1: Create Data-Parallel Deployment with KV-Aware Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e683ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Create deployment with KV-aware routing\n",
    "echo \"Creating data-parallel deployment with KV-aware routing...\"\n",
    "\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: nvidia.com/v1alpha1\n",
    "kind: DynamoGraphDeployment\n",
    "metadata:\n",
    "  name: vllm-kv-demo\n",
    "  namespace: ${NAMESPACE}\n",
    "spec:\n",
    "  services:\n",
    "    Frontend:\n",
    "      dynamoNamespace: vllm-kv-demo\n",
    "      componentType: frontend\n",
    "      replicas: 1\n",
    "      extraPodSpec:\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:${RELEASE_VERSION}\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - |\n",
    "              python3 -m dynamo.frontend \\\\\n",
    "                --http-port 8000 \\\\\n",
    "                --router-mode kv \\\\\n",
    "                --kv-overlap-score-weight 1.0\n",
    "          env:\n",
    "            - name: NATS_SERVER\n",
    "              value: \"nats://nats.nats-system:4222\"\n",
    "            - name: DYN_LOG\n",
    "              value: info\n",
    "    VllmWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-kv-demo\n",
    "      componentType: worker\n",
    "      replicas: 2\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: info\n",
    "        - name: NATS_SERVER\n",
    "          value: \"nats://nats.nats-system:4222\"\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: /data/huggingface-cache\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:${RELEASE_VERSION}\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --tensor-parallel-size 1 --enable-prefix-caching\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Data-parallel deployment created with KV-aware routing\"\n",
    "echo \"  Deployment: vllm-kv-demo\"\n",
    "echo \"  Workers: 2 (identical, data parallel)\"\n",
    "echo \"  Router Mode: kv (cache-aware)\"\n",
    "echo \"  NATS: nats://nats.nats-system:4222\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b56eca",
   "metadata": {},
   "source": [
    "**Key Configuration Flags:**\n",
    "\n",
    "**Frontend:**\n",
    "- `--router-mode kv`: Enables KV-aware routing\n",
    "- `--kv-overlap-score-weight 1.0`: Balances cache hits vs load distribution\n",
    "- `NATS_SERVER`: Connection to NATS for subscribing to cache events\n",
    "\n",
    "**Workers:**\n",
    "- `--enable-prefix-caching`: Enables cache block tracking and event publishing\n",
    "- `NATS_SERVER`: Where to publish cache events\n",
    "- `--tensor-parallel-size 1`: Each worker uses 1 GPU (not splitting model)\n",
    "\n",
    "### Step 2: Create NodePort Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e485b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Create NodePort service\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: vllm-kv-frontend-np\n",
    "  namespace: ${NAMESPACE}\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    nvidia.com/dynamo-component: Frontend\n",
    "    nvidia.com/dynamo-graph-deployment-name: vllm-kv-demo\n",
    "  ports:\n",
    "  - port: 8000\n",
    "    targetPort: 8000\n",
    "    nodePort: 30200\n",
    "    protocol: TCP\n",
    "    name: http\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì NodePort service created on port 30200\"\n",
    "echo \"  Access at: http://\\${NODE_IP}:30200\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b27642b",
   "metadata": {},
   "source": [
    "### Step 3: Wait for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a75adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Wait for pods to be ready\n",
    "echo \"Waiting for deployment...\"\n",
    "echo \"This may take 2-3 minutes for model download and initialization...\"\n",
    "echo \"\"\n",
    "\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Wait for pods to be ready\n",
    "kubectl wait --for=condition=ready --timeout=300s \\\n",
    "  pods -l nvidia.com/dynamo-graph-deployment-name=vllm-kv-demo \\\n",
    "  -n $NAMESPACE 2>/dev/null || echo \"Pods are initializing...\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Deployment status:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-kv-demo\n",
    "\n",
    "echo \"\"\n",
    "echo \"Expected pods:\"\n",
    "echo \"  - vllm-kv-demo-frontend-xxxxx (Frontend with KV-aware router)\"\n",
    "echo \"  - vllm-kv-demo-vllmworker-xxxxx (Worker 1)\"\n",
    "echo \"  - vllm-kv-demo-vllmworker-xxxxx (Worker 2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9a282",
   "metadata": {},
   "source": [
    "### Step 4: Test Basic Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Test the deployment\n",
    "echo \"Testing inference...\"\n",
    "export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is AI?\"}],\n",
    "    \"max_tokens\": 50\n",
    "  }' | jq -r '.choices[0].message.content'\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Deployment is serving requests\"\n",
    "echo \"  Router: KV-aware (tracking cache state)\"\n",
    "echo \"  Workers: Publishing cache events to NATS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205eb387",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Demonstrate Cache-Aware Routing\n",
    "\n",
    "Now we'll demonstrate KV-aware routing by sending requests with shared prefixes. The router should direct these to the same worker for cache reuse.\n",
    "\n",
    "### Step 1: Send Requests with Shared Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b49c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Demonstrating KV-aware routing with shared prefix...\"\n",
    "echo \"All requests start with 'Explain quantum computing'\"\n",
    "echo \"\"\n",
    "\n",
    "# Request 1: Baseline (cache miss expected)\n",
    "echo \"Request 1: Full explanation (cache miss expected)\"\n",
    "time curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n",
    "    \"max_tokens\": 50\n",
    "  }' | jq -r '.choices[0].message.content'\n",
    "\n",
    "echo \"\"\n",
    "sleep 2\n",
    "\n",
    "# Request 2: Similar prefix (cache hit expected)\n",
    "echo \"Request 2: Simple explanation (cache hit expected - shared prefix)\"\n",
    "time curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms\"}],\n",
    "    \"max_tokens\": 50\n",
    "  }' | jq -r '.choices[0].message.content'\n",
    "\n",
    "echo \"\"\n",
    "sleep 2\n",
    "\n",
    "# Request 3: Another variation (cache hit expected)\n",
    "echo \"Request 3: Brief explanation (cache hit expected - shared prefix)\"\n",
    "time curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing briefly\"}],\n",
    "    \"max_tokens\": 50\n",
    "  }' | jq -r '.choices[0].message.content'\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Requests completed\"\n",
    "echo \"  Request 1 should be slower (no cache)\"\n",
    "echo \"  Requests 2-3 should be faster (cache hits with KV-aware routing)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e72ef40",
   "metadata": {},
   "source": [
    "**Expected Behavior:**\n",
    "- Request 1: Slower TTFT (Time To First Token) - no cached blocks\n",
    "- Requests 2 & 3: Faster TTFT - router directs to worker with cached prefix \"Explain quantum computing\"\n",
    "\n",
    "### Step 2: Check Worker Logs for Cache Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8625bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Get worker pod names  \n",
    "WORKER_PODS=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker,nvidia.com/dynamo-graph-deployment-name=vllm-kv-demo -o jsonpath='{.items[*].metadata.name}')\n",
    "\n",
    "echo \"Checking worker logs for cache events...\"\n",
    "for POD in $WORKER_PODS; do\n",
    "    echo \"\"\n",
    "    echo \"=== Worker: $POD ===\"\n",
    "    kubectl logs -n $NAMESPACE $POD --tail=30 | grep -E \"(prefix.*cache|kv.*cache|blocks)\" || echo \"No cache messages in recent logs\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9140870",
   "metadata": {},
   "source": [
    "**What to look for:**\n",
    "- \"Prefix cache hit\" messages\n",
    "- Block allocation/reuse statistics\n",
    "- Most requests should hit the same worker (indicated by same pod having activity)\n",
    "\n",
    "### Step 3: Conversation-Style Traffic (System Prompt Reuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d10a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Testing cache reuse with shared system prompt...\"\n",
    "\n",
    "# Turn 1\n",
    "echo \"Turn 1: Physics question\"\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in physics. Always explain concepts clearly.\"},\n",
    "      {\"role\": \"user\", \"content\": \"What is quantum mechanics?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 80\n",
    "  }' | jq -r '.choices[0].message.content'\n",
    "\n",
    "echo \"\"\n",
    "sleep 2\n",
    "\n",
    "# Turn 2 (shares system prompt - cache hit expected)\n",
    "echo \"Turn 2: Different question, same system prompt\"\n",
    "curl -s http://$NODE_IP:30200/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in physics. Always explain concepts clearly.\"},\n",
    "      {\"role\": \"user\", \"content\": \"What is relativity?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 80\n",
    "  }' | jq -r '.choices[0].message.content'\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Both requests shared the system prompt\"\n",
    "echo \"  KV-aware router should route to same worker for cache reuse\"\n",
    "echo \"  System prompt tokens (cached): ~20 tokens\"\n",
    "echo \"  Only the user questions needed to be processed fresh\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf69e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Understanding KV-Aware Routing Trade-offs\n",
    "\n",
    "### K8s-Native vs NATS/etcd Comparison (v0.8.0+)\n",
    "\n",
    "| Aspect | K8s-Native | NATS/etcd |\n",
    "|--------|------------|-----------|\n",
    "| **Setup Complexity** | ‚úÖ Simple (no extra infra) | ‚ö†Ô∏è Complex (2 systems to manage) |\n",
    "| **Latency** | ‚úÖ Lower (direct TCP) | ‚ö†Ô∏è Slightly higher (pub/sub) |\n",
    "| **Scale Sweet Spot** | Most deployments | Extreme scale |\n",
    "| **Discovery** | EndpointSlices (built-in) | etcd (external) |\n",
    "| **Transport** | TCP | NATS + TCP |\n",
    "| **Ops Burden** | ‚úÖ Low | ‚ö†Ô∏è Medium-High |\n",
    "| **Multi-Region** | ‚ö†Ô∏è Limited | ‚úÖ Excellent |\n",
    "| **Custom Routing** | ‚ö†Ô∏è Basic | ‚úÖ Advanced |\n",
    "| **Cache Coordination** | ‚úÖ Yes (via planner) | ‚úÖ Yes (via NATS) |\n",
    "| **NIXL Support** | ‚úÖ Yes | ‚úÖ Yes |\n",
    "\n",
    "**Recommendation:** Start with K8s-native. Only add NATS/etcd if you need extreme scale or multi-region capabilities.\n",
    "\n",
    "### Single-Node vs Multi-Node\n",
    "\n",
    "**Single Node with Multiple GPUs (Typical Dev Setup)**:\n",
    "```\n",
    "‚úì KV-aware routing still beneficial (routes to worker with cached data)\n",
    "‚úì Learning opportunity to understand architecture\n",
    "‚úì Workers can share cache blocks via NIXL locally\n",
    "‚úì K8s-native = simpler (no NATS/etcd overhead)\n",
    "‚úó Less dramatic network benefits (same machine)\n",
    "```\n",
    "\n",
    "**Multi-Node (Production)**:\n",
    "```\n",
    "‚úì KV-aware Router directs requests to nodes with relevant cache\n",
    "‚úì NIXL transfers cache data efficiently (RDMA/TCP between nodes)\n",
    "‚úì Improved cache hit rates = lower latency\n",
    "‚úì Better resource utilization across cluster\n",
    "‚úì K8s-native recommended for most deployments\n",
    "‚úì NATS/etcd for extreme scale or multi-region\n",
    "```\n",
    "‚úì Enables advanced features (cache migration, load balancing)\n",
    "‚úó Network latency between nodes\n",
    "‚úó Increased complexity in debugging\n",
    "```\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "```bash\n",
    "# Display performance comparison\n",
    "cat <<'EOF'\n",
    "\n",
    "Performance Impact of Distributed Dynamo:\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Metric              ‚îÇ Single Node      ‚îÇ Multi-Node   ‚îÇ\n",
    "‚îÇ                     ‚îÇ (Multi-GPU)      ‚îÇ              ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Cache Hit Rate      ‚îÇ +10-20%          ‚îÇ +20-40%      ‚îÇ\n",
    "‚îÇ Latency (P50)       ‚îÇ +2-5ms           ‚îÇ +2-5ms       ‚îÇ\n",
    "‚îÇ Latency (P99)       ‚îÇ +5-10ms          ‚îÇ +5-10ms      ‚îÇ\n",
    "‚îÇ Throughput          ‚îÇ Same to +10%     ‚îÇ +30-60%      ‚îÇ\n",
    "‚îÇ Memory Overhead     ‚îÇ +100-200MB       ‚îÇ +100-200MB   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "When Distributed Dynamo Helps Most:\n",
    "  ‚Ä¢ Multiple GPUs or nodes with high traffic\n",
    "  ‚Ä¢ Repeated queries (high cache hit potential)\n",
    "  ‚Ä¢ Long context lengths (expensive to recompute)\n",
    "  ‚Ä¢ Batch processing workloads\n",
    "\n",
    "When It May Not Help:\n",
    "  ‚Ä¢ Single GPU deployments\n",
    "  ‚Ä¢ Unique queries every time (low cache hit rate)\n",
    "  ‚Ä¢ Very short context lengths\n",
    "  ‚Ä¢ Real-time streaming with completely unique prompts\n",
    "\n",
    "Architecture Notes:\n",
    "  ‚Ä¢ Grove = Kubernetes Operator (orchestration)\n",
    "  ‚Ä¢ Dynamo = Serving Framework (actual inference)\n",
    "  ‚Ä¢ NATS = Metadata/coordination (small messages)\n",
    "  ‚Ä¢ NIXL = KV cache data transfer (large tensors via RDMA/TCP)\n",
    "EOF\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Section 6: Advanced Distributed Features\n",
    "\n",
    "### Cache Monitoring\n",
    "\n",
    "Check distributed coordination through worker logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get cache stats from worker logs\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "WORKER_POD=$(kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker,nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)\n",
    "\n",
    "if [ -n \"$WORKER_POD\" ]; then\n",
    "    echo \"Checking NIXL activity in worker logs...\"\n",
    "    echo \"\"\n",
    "    kubectl logs -n $NAMESPACE $WORKER_POD --tail=100 | grep -i \"nixl\" | tail -10\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"Worker pod: $WORKER_POD\"\n",
    "    echo \"\"\n",
    "    echo \"What to look for:\"\n",
    "    echo \"  - NIXL initialization messages (KV cache transfer setup)\"\n",
    "    echo \"  - KV cache registration events\"\n",
    "    echo \"  - UCX backend messages (if using RDMA for cache transfer)\"\n",
    "    echo \"  - K8s service discovery messages\"\n",
    "else\n",
    "    echo \"‚ö†Ô∏è No worker pods found\"\n",
    "    echo \"Make sure the vllm-distributed-demo deployment is running\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39641d4",
   "metadata": {},
   "source": [
    "**Note**: Cache hit/miss metrics depend on workload patterns. Even on a single node with multiple GPUs, KV-aware routing can improve cache hits by directing requests to the worker that already has relevant cache blocks.\n",
    "\n",
    "---\n",
    "\n",
    "## Section 7: Cleanup\n",
    "\n",
    "### Step 1: Remove Distributed Demo Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c738a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete the distributed deployment\n",
    "echo \"Removing distributed Dynamo deployment...\"\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "kubectl delete dynamographdeployment vllm-distributed-demo -n $NAMESPACE\n",
    "kubectl delete svc vllm-distributed-demo-frontend-np -n $NAMESPACE\n",
    "\n",
    "echo \"‚úì Distributed deployment removed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d975f9",
   "metadata": {},
   "source": [
    "### Step 2: Verify Lab 1 Deployment is Still Running\n",
    "\n",
    "Your original Lab 1 deployment should still be running on port 30100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check Lab 1 deployment status\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Checking Lab 1 deployment...\"\n",
    "kubectl get dynamographdeployment vllm-disagg-router -n $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"Lab 1 pods:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-disagg-router\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Lab 1 deployment is available at: http://$NODE_IP:30100\"\n",
    "echo \"\"\n",
    "echo \"Test it:\"\n",
    "echo \"  curl http://$NODE_IP:30100/v1/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b87ca5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've deployed a distributed Dynamo architecture where multiple workers collaborate to serve requests. Unlike Lab 1's disaggregated approach (specialized prefill/decode workers), this distributed model uses identical workers that share cached computations over the network via NIXL.\n",
    "\n",
    "**What makes this powerful:**\n",
    "- Workers discover each other automatically through Kubernetes\n",
    "- KV cache sharing speeds up similar or follow-up requests\n",
    "- Scales horizontally‚Äîadd more workers for more traffic\n",
    "- Works on single nodes with multiple GPUs or across multi-node clusters\n",
    "\n",
    "**Key architectural choice:**\n",
    "- Use **disaggregated serving** (Lab 1) for predictable latency on individual requests\n",
    "- Use **distributed serving** (Lab 3) when you have high traffic with cache-friendly patterns\n",
    "\n",
    "**Next steps:** Experiment with different worker counts, monitor cache hit rates in Grafana, or explore the optional NATS/etcd setup in Appendix B for extreme-scale deployments.\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Deployment Not Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check deployment status\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "kubectl describe dynamographdeployment vllm-distributed-demo -n $NAMESPACE\n",
    "\n",
    "# Check pod status\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-graph-deployment-name=vllm-distributed-demo\n",
    "\n",
    "# Check worker logs\n",
    "kubectl logs -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker\n",
    "\n",
    "# Common issues:\n",
    "# - Insufficient GPU resources\n",
    "# - Worker gang scheduling waiting for all pods\n",
    "# - Image pull errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647c790",
   "metadata": {},
   "source": [
    "### Workers Not Discovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e19736",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check K8s services and endpoints\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "kubectl get svc -n $NAMESPACE\n",
    "kubectl get endpoints -n $NAMESPACE\n",
    "\n",
    "# Check EndpointSlices (K8s-native discovery)\n",
    "kubectl get endpointslices -n $NAMESPACE\n",
    "\n",
    "# Check worker pods are running\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/dynamo-component=VllmWorker\n",
    "\n",
    "# Common issues:\n",
    "# - Workers not fully ready (check 1/1 Running)\n",
    "# - Service selectors not matching pods\n",
    "# - Network policies blocking communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176662e3",
   "metadata": {},
   "source": [
    "### No Requests Reaching Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Test frontend endpoint\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "curl -v http://$NODE_IP:30200/v1/models\n",
    "\n",
    "# Check frontend logs\n",
    "kubectl logs -n $NAMESPACE -l nvidia.com/dynamo-component=Frontend\n",
    "\n",
    "# Verify NodePort service exists\n",
    "kubectl get svc vllm-distributed-demo-frontend-np -n $NAMESPACE\n",
    "\n",
    "# Common issues:\n",
    "# - NodePort service not created\n",
    "# - Frontend pod not ready\n",
    "# - Port conflicts on node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b7043",
   "metadata": {},
   "source": [
    "### Understanding Cache Sharing with NIXL\n",
    "\n",
    "**NIXL** (NVIDIA Inference Transfer Library) handles KV cache transfer between workers:\n",
    "\n",
    "- Transfers actual KV cache data (gigabytes of tensors)\n",
    "- Uses RDMA, TCP, or CPU/SSD offload  \n",
    "- Direct worker-to-worker communication\n",
    "- Not visible in application logs (happens at library level)\n",
    "\n",
    "**On Single Node**:\n",
    "- Cache transfers via NIXL are faster (local)\n",
    "- Workers coordinate via K8s-native discovery\n",
    "- Benefits still apply with multiple GPU workers\n",
    "\n",
    "**Benefits Require**:\n",
    "- Multiple workers (even on same node)\n",
    "- Repeated queries with shared prefixes\n",
    "- Workload that generates cache hits\n",
    "\n",
    "**For NATS/etcd troubleshooting**, see Appendix B\n",
    "\n",
    "---\n",
    "\n",
    "## Appendix A: NATS/etcd Architecture (Optional - Extreme Scale)\n",
    "\n",
    "This appendix covers the NATS/etcd deployment architecture for extreme scale deployments or multi-region setups. **Most users should use K8s-native deployment** (covered in the main lab).\n",
    "\n",
    "### When You Need NATS/etcd\n",
    "\n",
    "Consider NATS/etcd if you have:\n",
    "- Very large Kubernetes clusters (extreme scale)\n",
    "- Multi-region deployments\n",
    "- Complex custom routing logic\n",
    "- Advanced cache policies and coordination requirements\n",
    "\n",
    "### NATS/etcd Architecture Diagram\n",
    "\n",
    "```\n",
    "               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "               ‚îÇ  Cloud Load Balancer       ‚îÇ\n",
    "               ‚îÇ  or Ingress Controller     ‚îÇ\n",
    "               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                          ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ                ‚îÇ                ‚îÇ\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇFrontend 1‚îÇ    ‚îÇFrontend 2‚îÇ    ‚îÇFrontend 3‚îÇ\n",
    "    ‚îÇ (Node 1) ‚îÇ    ‚îÇ (Node 2) ‚îÇ    ‚îÇ (Node 3) ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                ‚îÇ                ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                          ‚îÇ\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ  NATS Message Bus     ‚îÇ\n",
    "              ‚îÇ  (Metadata, Routing,  ‚îÇ\n",
    "              ‚îÇ   Cache Awareness)    ‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                          ‚îÇ\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ  etcd (Coordination)  ‚îÇ\n",
    "              ‚îÇ  (Service Discovery)  ‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                          ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ                ‚îÇ                ‚îÇ\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ Worker 1 ‚îÇ    ‚îÇ Worker 2 ‚îÇ    ‚îÇ Worker 3 ‚îÇ\n",
    "    ‚îÇ (Node 4) ‚îÇ    ‚îÇ (Node 5) ‚îÇ    ‚îÇ (Node 6) ‚îÇ\n",
    "    ‚îÇ  +GPU    ‚îÇ    ‚îÇ  +GPU    ‚îÇ    ‚îÇ  +GPU    ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                ‚îÇ                ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                          ‚îÇ\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ  NIXL (KV Cache       ‚îÇ\n",
    "              ‚îÇ   Data Transfer)      ‚îÇ\n",
    "              ‚îÇ  RDMA/TCP/SSD         ‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Components\n",
    "\n",
    "**NATS Message Bus:**\n",
    "- Pub/sub messaging for metadata (cache events, routing tables)\n",
    "- Low-latency coordination between frontends and workers\n",
    "- Does NOT transfer KV cache data (NIXL handles that)\n",
    "\n",
    "**etcd:**\n",
    "- Distributed configuration and service discovery\n",
    "- Leader election and coordination\n",
    "- Cluster state management\n",
    "\n",
    "**NIXL:**\n",
    "- Handles actual KV cache data transfer (same as K8s-native mode)\n",
    "- Uses RDMA/TCP for high-speed transfer\n",
    "- Direct worker-to-worker communication\n",
    "\n",
    "### Deployment Steps (Optional)\n",
    "\n",
    "If you need to deploy NATS/etcd, refer to Section 2a in the main lab (marked as \"Optional - Skip for K8s-Native\"). The steps are preserved but skipped in the standard lab flow.\n",
    "\n",
    "### Trade-offs vs K8s-Native\n",
    "\n",
    "| Aspect | K8s-Native | NATS/etcd |\n",
    "|--------|------------|-----------|\n",
    "| Setup Complexity | Simple | Complex |\n",
    "| Ops Burden | Low | Medium-High |\n",
    "| Max Scale | Standard clusters | Extreme scale |\n",
    "| Multi-Region | Limited | Excellent |\n",
    "| Custom Routing | Basic | Advanced |\n",
    "\n",
    "---\n",
    "\n",
    "## Appendix B: NATS/etcd Deployment Steps (Optional)\n",
    "\n",
    "**‚ö†Ô∏è WARNING:** These steps are ONLY for users deploying NATS/etcd for extreme-scale scenarios. Most users should skip this appendix and use K8s-native deployment (covered in the main lab).\n",
    "\n",
    "### When to Use These Steps\n",
    "\n",
    "Deploy NATS/etcd only if you have:\n",
    "- Very large Kubernetes clusters (extreme scale)\n",
    "- Multi-region deployments\n",
    "- Complex custom routing requirements\n",
    "- Advanced cache coordination policies\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Complete Section 2 Step 1 (Environment Setup)\n",
    "- Have cluster-admin access for cluster-wide resources\n",
    "\n",
    "### Step 1: Install NATS Message Bus\n",
    "\n",
    "NATS handles distributed coordination metadata between Dynamo components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d73690",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for NATS\n",
    "kubectl create namespace nats-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Add NATS Helm repository\n",
    "echo \"Adding NATS Helm repository...\"\n",
    "helm repo add nats https://nats-io.github.io/k8s/helm/charts/\n",
    "helm repo update\n",
    "\n",
    "# Install NATS (with Prometheus exporter)\n",
    "echo \"Installing NATS with metrics exporter...\"\n",
    "helm upgrade --install nats nats/nats \\\n",
    "  --namespace nats-system \\\n",
    "  --set config.jetstream.enabled=true \\\n",
    "  --set config.jetstream.fileStore.pvc.size=1Gi \\\n",
    "  --set promExporter.enabled=true \\\n",
    "  --set promExporter.port=7777 \\\n",
    "  --wait \\\n",
    "  --timeout 5m\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì NATS installed successfully\"\n",
    "echo \"  Connection: nats://nats.nats-system:4222\"\n",
    "echo \"  Metrics: Port 7777\"\n",
    "echo \"\"\n",
    "echo \"Note: NATS handles metadata (cache events, routing tables).\"\n",
    "echo \"      Actual KV cache data transfers via NIXL (RDMA/TCP).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63cd784",
   "metadata": {},
   "source": [
    "### Step 2: Install etcd Coordination Layer\n",
    "\n",
    "etcd provides distributed coordination for Grove components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44945672",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create namespace for etcd\n",
    "kubectl create namespace etcd-system --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Add Bitnami Helm repository\n",
    "echo \"Adding Bitnami Helm repository...\"\n",
    "helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "helm repo update\n",
    "\n",
    "# Install etcd (using legacy Bitnami mirror)\n",
    "echo \"Installing etcd...\"\n",
    "helm upgrade --install etcd bitnami/etcd \\\n",
    "  --namespace etcd-system \\\n",
    "  --set replicaCount=1 \\\n",
    "  --set auth.rbac.create=false \\\n",
    "  --set image.registry=docker.io \\\n",
    "  --set image.repository=bitnamilegacy/etcd \\\n",
    "  --set persistence.size=1Gi \\\n",
    "  --set preUpgradeHook.enabled=false \\\n",
    "  --wait \\\n",
    "  --timeout 5m\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì etcd installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f76741",
   "metadata": {},
   "source": [
    "### Step 3: Verify Infrastructure\n",
    "\n",
    "Check that NATS and etcd are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check NATS pods\n",
    "echo \"Checking NATS deployment...\"\n",
    "kubectl get pods -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking NATS service...\"\n",
    "kubectl get svc -n nats-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n",
    "\n",
    "# Check etcd pods\n",
    "echo \"Checking etcd deployment...\"\n",
    "kubectl get pods -n etcd-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"Checking etcd service...\"\n",
    "kubectl get svc -n etcd-system\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Infrastructure verified\"\n",
    "echo \"  NATS:  nats://nats.nats-system:4222 (metadata/coordination)\"\n",
    "echo \"  etcd:  http://etcd.etcd-system:2379 (service discovery)\"\n",
    "echo \"  NIXL will handle KV cache data transfer between workers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f6447",
   "metadata": {},
   "source": [
    "### Step 4: Enable Prometheus Monitoring (Optional)\n",
    "\n",
    "Create PodMonitors so Prometheus can scrape NATS and etcd metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2233c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create PodMonitor for NATS\n",
    "echo \"Enabling NATS metrics collection...\"\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PodMonitor\n",
    "metadata:\n",
    "  name: nats\n",
    "  namespace: nats-system\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/name: nats\n",
    "  podMetricsEndpoints:\n",
    "  - port: prom-metrics\n",
    "    path: /metrics\n",
    "EOF\n",
    "\n",
    "# Create PodMonitor for etcd\n",
    "echo \"Enabling etcd metrics collection...\"\n",
    "cat <<'EOF' | kubectl apply -f -\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PodMonitor\n",
    "metadata:\n",
    "  name: etcd\n",
    "  namespace: etcd-system\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/name: etcd\n",
    "  podMetricsEndpoints:\n",
    "  - port: client\n",
    "    path: /metrics\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Prometheus monitoring enabled\"\n",
    "echo \"  Metrics will be available in Grafana within 2-3 minutes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bced03c",
   "metadata": {},
   "source": [
    "### Cleanup (NATS/etcd)\n",
    "\n",
    "When you're done with NATS/etcd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52745de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove NATS\n",
    "echo \"Removing NATS...\"\n",
    "helm uninstall nats -n nats-system\n",
    "kubectl delete namespace nats-system\n",
    "\n",
    "# Remove etcd  \n",
    "echo \"Removing etcd...\"\n",
    "helm uninstall etcd -n etcd-system\n",
    "kubectl delete namespace etcd-system\n",
    "\n",
    "# Remove PodMonitors\n",
    "kubectl delete podmonitor nats -n nats-system 2>/dev/null || true\n",
    "kubectl delete podmonitor etcd -n etcd-system 2>/dev/null || true\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Infrastructure removed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4232c2",
   "metadata": {},
   "source": [
    "### Configuring Dynamo to Use NATS/etcd\n",
    "\n",
    "After installing NATS/etcd, you need to configure your `DynamoGraphDeployment` to use them. Add these annotations to your deployment spec:\n",
    "\n",
    "```yaml\n",
    "metadata:\n",
    "  annotations:\n",
    "    dynamo.nvidia.com/discovery-backend: \"nats\"  # Use NATS/etcd instead of K8s-native\n",
    "    dynamo.nvidia.com/nats-url: \"nats://nats.nats-system:4222\"\n",
    "    dynamo.nvidia.com/etcd-url: \"http://etcd.etcd-system:2379\"\n",
    "```\n",
    "\n",
    "Refer to Dynamo documentation for complete configuration options.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Core Documentation\n",
    "\n",
    "- **NVIDIA Dynamo Documentation**: https://docs.nvidia.com/dynamo/latest/\n",
    "- **Dynamo Deployment Guide**: https://docs.nvidia.com/dynamo/latest/kubernetes/deployment/\n",
    "- **Grove Operator Guide**: https://docs.nvidia.com/dynamo/latest/kubernetes/grove.html\n",
    "- **Dynamo v0.8.0 Release Notes**: https://github.com/ai-dynamo/dynamo/releases/tag/v0.8.0\n",
    "\n",
    "### Advanced Topics (NATS/etcd - Optional)\n",
    "\n",
    "- **NATS Documentation**: https://docs.nats.io/\n",
    "- **etcd Documentation**: https://etcd.io/docs/\n",
    "\n",
    "### Community Resources\n",
    "\n",
    "- **Dynamo GitHub**: https://github.com/ai-dynamo/dynamo\n",
    "- **NVIDIA Developer Forums**: https://forums.developer.nvidia.com/\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've completed Lab 3: Distributed Dynamo with Grove Orchestration** üå≤\n",
    "\n",
    "You now understand the fundamentals of distributed LLM serving, the difference between Grove (operator) and Dynamo (serving framework), and how K8s-native discovery enables distributed coordination!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-kernelspec,-widgets,-language_info"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
