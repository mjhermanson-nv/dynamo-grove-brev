{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb731b5",
   "metadata": {},
   "source": [
    "# Lab 1: Introduction and Kubernetes-Based Deployment\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you'll deploy a **disaggregated serving** model on Kubernetes using NVIDIA Dynamo. This architecture separates the inference workload into two specialized workers: one handles the initial prompt processing (prefill), and another generates the response tokens (decode). By splitting these tasks, each worker can be optimized for its specific job, leading to better GPU utilization and more predictable performance.\n",
    "\n",
    "You'll deploy a small language model (Qwen 1.5B) that runs on a single node with 2 GPUs‚Äîone GPU for the prefill worker and one for the decode worker. After deployment, you'll test the model using OpenAI-compatible APIs (the same format as ChatGPT's API) and run performance benchmarks to measure response quality.\n",
    "\n",
    "This is the foundation for understanding Dynamo's capabilities. In Lab 2, you'll add monitoring to observe what's happening inside your deployment. In Lab 3, you'll see a different architecture where multiple workers share their work more dynamically.\n",
    "\n",
    "**Duration**: ~90 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Environment Setup\n",
    "\n",
    "### Objectives\n",
    "- Verify Kubernetes access \n",
    "- Install Dynamo dependencies\n",
    "- Set up prerequisites (kubectl, helm)\n",
    "\n",
    "### Prerequisites\n",
    "Before starting, ensure you have:\n",
    "- ‚úÖ Kubernetes cluster access (kubeconfig provided by instructor)\n",
    "- ‚úÖ `kubectl` installed (version 1.24+) or `microk8s kubectl`\n",
    "- ‚úÖ `helm` 3.x installed\n",
    "- ‚úÖ HuggingFace token from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "### Step 2: Set Configuration Variables\n",
    "\n",
    "Run this to set up your environment. The defaults work for most users:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0269ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables (these defaults work for most setups)\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n",
    "echo \"üéì Lab 1: Environment Configuration\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n",
    "echo \"  Release Version:  $RELEASE_VERSION\"\n",
    "echo \"  Namespace:        $NAMESPACE\"\n",
    "echo \"  Cache Path:       $CACHE_PATH\"\n",
    "echo \"  Node IP:          $NODE_IP\"\n",
    "echo \"\"\n",
    "echo \"üìå Service Ports (after deployment):\"\n",
    "echo \"  Frontend API:     http://$NODE_IP:30100\"\n",
    "echo \"  Grafana:          http://$NODE_IP:30080\"\n",
    "echo \"\"\n",
    "echo \"üí° Note: Frontend will be accessible after deploying in Section 3\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8df4c5e",
   "metadata": {},
   "source": [
    "### Step 3: Verify Kubernetes Access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Verify kubectl is installed and configured\n",
    "echo \"=== kubectl version ===\"\n",
    "kubectl version --client\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== Cluster info ===\"\n",
    "kubectl cluster-info\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== GPU nodes ===\"\n",
    "kubectl get nodes -o custom-columns=NAME:.metadata.name,GPUs:.status.capacity.nvidia\\\\.com/gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c269fc79",
   "metadata": {},
   "source": [
    "### Step 4: Set Up NGC Authentication\n",
    "\n",
    "To access NVIDIA's Dynamo container images, you need to authenticate with NGC.\n",
    "\n",
    "#### Get Your NGC API Key\n",
    "\n",
    "1. Go to [NGC](https://ngc.nvidia.com/)\n",
    "2. Sign in or create an account\n",
    "3. Click on your profile in the top right corner\n",
    "4. Select **\"Setup\"** ‚Üí **\"Generate API Key\"**\n",
    "5. Copy your API key (it will only be shown once!)\n",
    "\n",
    "#### Set NGC API Key\n",
    "\n",
    "**Get your NGC API Key from [ngc.nvidia.com](https://ngc.nvidia.com/)** (Go to Profile > Setup > Generate API Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Get NGC API key from user\n",
    "print(\"Enter your NGC API Key from https://ngc.nvidia.com/\")\n",
    "print(\"(Go to Profile > Setup > Generate API Key)\")\n",
    "print(\"\")\n",
    "NGC_API_KEY = getpass.getpass(\"NGC API Key: \")\n",
    "\n",
    "# Save it for later use (creating pull secrets)\n",
    "os.environ['NGC_API_KEY'] = NGC_API_KEY\n",
    "\n",
    "print(\"\")\n",
    "print(\"‚úì NGC API key saved\")\n",
    "print(\"  You can now use it to login and create pull secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97039f0f",
   "metadata": {},
   "source": [
    "#### Set HuggingFace Token\n",
    "\n",
    "**HuggingFace token is required to download models.** Get yours from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) (Create a 'Read' token if you don't have one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Get HuggingFace token from user\n",
    "print(\"Enter your HuggingFace Token from https://huggingface.co/settings/tokens\")\n",
    "print(\"(Create a 'Read' token if you don't have one)\")\n",
    "print(\"\")\n",
    "HF_TOKEN = getpass.getpass(\"HF Token: \")\n",
    "\n",
    "# Save it for later use\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "print(\"\")\n",
    "print(\"‚úì HuggingFace token saved to environment\")\n",
    "print(\"  Available as $HF_TOKEN in bash cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b591133",
   "metadata": {},
   "source": [
    "#### Login to NGC Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cda2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Login to NGC container registry\n",
    "echo \"$NGC_API_KEY\" | helm registry login nvcr.io --username '$oauthtoken' --password-stdin\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì NGC authentication complete\"\n",
    "echo \"  You can now pull Dynamo container images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ae39a",
   "metadata": {},
   "source": [
    "### Step 5: Create Your Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create the namespace\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "kubectl create namespace $NAMESPACE 2>&1 | grep -v \"AlreadyExists\" || true\n",
    "\n",
    "# Verify namespace was created\n",
    "echo \"\"\n",
    "echo \"Verifying namespace:\"\n",
    "kubectl get namespace $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3847896d",
   "metadata": {},
   "source": [
    "### Step 6: Create NGC Pull Secret\n",
    "\n",
    "Create a Kubernetes secret so that pods can pull images from NGC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e621d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get variables\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Create NGC image pull secret\n",
    "kubectl create secret docker-registry ngc-secret \\\n",
    "    --docker-server=nvcr.io \\\n",
    "    --docker-username='$oauthtoken' \\\n",
    "    --docker-password=\"$NGC_API_KEY\" \\\n",
    "    --namespace $NAMESPACE \\\n",
    "    2>&1 | grep -v \"AlreadyExists\" || true\n",
    "\n",
    "# Verify secret was created\n",
    "echo \"\"\n",
    "echo \"Verifying NGC secret:\"\n",
    "kubectl get secret ngc-secret -n $NAMESPACE\n",
    "echo \"‚úì NGC pull secret created in namespace: $NAMESPACE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1463f8",
   "metadata": {},
   "source": [
    "### Step 7: Create HuggingFace Token Secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get variables\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Create HuggingFace token secret\n",
    "kubectl create secret generic hf-token-secret \\\n",
    "    --from-literal=HF_TOKEN=\"$HF_TOKEN\" \\\n",
    "    --namespace $NAMESPACE \\\n",
    "    2>&1 | grep -v \"AlreadyExists\" || true\n",
    "\n",
    "# Verify secret was created\n",
    "echo \"\"\n",
    "echo \"Verifying HuggingFace secret:\"\n",
    "kubectl get secret hf-token-secret -n $NAMESPACE\n",
    "echo \"‚úì HuggingFace token secret created\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f80d1f",
   "metadata": {},
   "source": [
    "## Section 2: Install Dynamo Platform\n",
    "\n",
    "### Objectives\n",
    "- Install Dynamo CRDs (Custom Resource Definitions) with validation webhooks\n",
    "- Install Dynamo platform (operator with K8s-native discovery)\n",
    "- Verify platform components are running\n",
    "\n",
    "### Architecture (v0.8.0 Simplified)\n",
    "\n",
    "```\n",
    "Client Request\n",
    "      ‚Üì\n",
    "Frontend (OpenAI API + Disaggregated Router)\n",
    "      ‚Üì\n",
    "Prefill Worker (GPU 0) ‚Üí Processes prompt ‚Üí Generates KV cache\n",
    "      ‚Üì\n",
    "Decode Worker (GPU 1) ‚Üí Uses KV cache ‚Üí Generates tokens\n",
    "      ‚Üì\n",
    "Response to Client\n",
    "\n",
    "Infrastructure:\n",
    "- Kubernetes EndpointSlices (service discovery)\n",
    "- TCP Transport (direct worker connections)\n",
    "- Dynamo Operator (manages deployments)\n",
    "- Validation Webhooks (catch errors early)\n",
    "\n",
    "Note: This basic deployment doesn't use NATS or etcd.\n",
    "      Lab 3 covers distributed deployments\n",
    "```\n",
    "\n",
    "### Deployment Mode\n",
    "\n",
    "We're using the **recommended cluster-wide deployment** (default). According to the [official Dynamo documentation](https://github.com/ai-dynamo/dynamo/blob/main/deploy/helm/charts/platform/README.md):\n",
    "\n",
    "- ‚úÖ **Recommended**: One cluster-wide operator per cluster (default)\n",
    "- This is the standard deployment for single-node and production clusters\n",
    "- Install a **namespace-scoped Dynamo operator** that only manages resources in your namespace\n",
    "- The CRDs are cluster-wide and should already be installed (check first)\n",
    "\n",
    "### Step 1: Install Dynamo CRDs\n",
    "\n",
    "**Note:** CRDs are cluster-wide resources and only need to be installed **once per cluster**. This step checks if they exist and installs them if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a43344",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set default RELEASE_VERSION if not already set\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "\n",
    "# Check if CRDs already exist, install if not\n",
    "if kubectl get crd dynamographdeployments.nvidia.com &>/dev/null && \\\n",
    "   kubectl get crd dynamocomponentdeployments.nvidia.com &>/dev/null; then\n",
    "    echo \"‚úì CRDs already installed\"\n",
    "    kubectl get crd | grep nvidia.com\n",
    "else\n",
    "    echo \"Installing Dynamo CRDs v$RELEASE_VERSION...\"\n",
    "    helm fetch https://helm.ngc.nvidia.com/nvidia/ai-dynamo/charts/dynamo-crds-$RELEASE_VERSION.tgz\n",
    "    helm install dynamo-crds dynamo-crds-$RELEASE_VERSION.tgz --namespace default\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"Verifying CRD installation:\"\n",
    "    kubectl get crd | grep nvidia.com\n",
    "    echo \"\"\n",
    "    echo \"‚úì CRDs include validation webhooks for early error detection\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dc6530",
   "metadata": {},
   "source": [
    "### Step 2: Install Dynamo Platform\n",
    "\n",
    "**Simplified in v0.8.0:** NATS and etcd are now **optional**. Dynamo uses Kubernetes-native service discovery (EndpointSlices) and TCP transport by default, making deployment simpler and reducing infrastructure dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9746a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set defaults if not already set\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"Using configuration:\"\n",
    "echo \"  RELEASE_VERSION: $RELEASE_VERSION\"\n",
    "echo \"  NAMESPACE: $NAMESPACE\"\n",
    "echo \"\"\n",
    "\n",
    "# Download platform chart\n",
    "echo \"Downloading Dynamo platform chart v$RELEASE_VERSION...\"\n",
    "helm fetch https://helm.ngc.nvidia.com/nvidia/ai-dynamo/charts/dynamo-platform-$RELEASE_VERSION.tgz\n",
    "\n",
    "# Install Dynamo platform (namespace-scoped, K8s-native discovery)\n",
    "echo \"Installing Dynamo platform in namespace: $NAMESPACE\"\n",
    "echo \"Using K8s-native discovery (no NATS/etcd required)\"\n",
    "helm install dynamo-platform \\\n",
    "    dynamo-platform-$RELEASE_VERSION.tgz \\\n",
    "    --namespace $NAMESPACE \\\n",
    "    --set dynamo-operator.namespaceRestriction.enabled=true\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Platform installation initiated\"\n",
    "echo \"  Discovery: Kubernetes EndpointSlices (native)\"\n",
    "echo \"  Transport: TCP (default in v0.8.0)\"\n",
    "echo \"\"\n",
    "echo \"Waiting for pods to be ready...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49918652",
   "metadata": {},
   "source": [
    "### Step 3: Wait for Platform Pods to Be Ready\n",
    "\n",
    "Re-run the following cell until all pods report as \"Running\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283e94c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"Waiting for platform pods to be ready...\"\n",
    "echo \"\"\n",
    "\n",
    "# Wait for pods to be ready (timeout after 5 minutes)\n",
    "TIMEOUT=300\n",
    "ELAPSED=0\n",
    "INTERVAL=5\n",
    "\n",
    "while [ $ELAPSED -lt $TIMEOUT ]; do\n",
    "    # Get pod status\n",
    "    NOT_READY=$(kubectl get pods -n $NAMESPACE --no-headers 2>/dev/null | grep -v \"Running\\|Completed\" | wc -l)\n",
    "    TOTAL=$(kubectl get pods -n $NAMESPACE --no-headers 2>/dev/null | wc -l)\n",
    "    READY=$((TOTAL - NOT_READY))\n",
    "    \n",
    "    echo \"[$ELAPSED s] Pods ready: $READY/$TOTAL\"\n",
    "    kubectl get pods -n $NAMESPACE\n",
    "    \n",
    "    # Check if all pods are ready\n",
    "    if [ $NOT_READY -eq 0 ] && [ $TOTAL -gt 0 ]; then\n",
    "        echo \"\"\n",
    "        echo \"‚úì All platform pods are ready!\"\n",
    "        break\n",
    "    fi\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"Waiting for pods to be ready... (checking again in ${INTERVAL}s)\"\n",
    "    echo \"\"\n",
    "    sleep $INTERVAL\n",
    "    ELAPSED=$((ELAPSED + INTERVAL))\n",
    "done\n",
    "\n",
    "if [ $ELAPSED -ge $TIMEOUT ]; then\n",
    "    echo \"‚ö†Ô∏è  Timeout waiting for pods to be ready\"\n",
    "    echo \"Please check pod status manually: kubectl get pods -n $NAMESPACE\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e407404e",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3b5c8",
   "metadata": {},
   "source": [
    "## Section 3: Deploy Your First Model with Disaggregated Serving\n",
    "\n",
    "### Objectives\n",
    "- Understand disaggregated serving architecture\n",
    "- Configure and deploy a model using vLLM backend with separate prefill and decode workers\n",
    "- Use Kubernetes manifests to deploy Dynamo resources\n",
    "\n",
    "### Available Backends\n",
    "In this lab, we'll use **vLLM** with disaggregated serving:\n",
    "- **vLLM**: High-throughput serving with PagedAttention\n",
    "- Model: `Qwen/Qwen2.5-1.5B-Instruct` (small, fast to download)\n",
    "- Architecture: Disaggregated serving with separate prefill and decode workers\n",
    "\n",
    "**Other backends** (for exploration):\n",
    "- **SGLang**: Optimized for complex prompting and structured generation\n",
    "- **TensorRT-LLM**: Maximum performance on NVIDIA GPUs\n",
    "\n",
    "### What is Disaggregated Serving?\n",
    "\n",
    "Disaggregated serving separates the inference pipeline into specialized workers:\n",
    "\n",
    "**Prefill Worker** (GPU 0):\n",
    "- Processes input prompts (compute-intensive)\n",
    "- Converts tokens into KV cache\n",
    "- Passes KV cache to decode workers\n",
    "\n",
    "**Decode Worker** (GPU 1):\n",
    "- Generates output tokens (memory-intensive)\n",
    "- Uses KV cache from prefill worker\n",
    "- Produces the final response\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ **Independent scaling**: Scale prefill and decode separately based on workload\n",
    "- ‚úÖ **Resource optimization**: Each worker optimized for its specific task\n",
    "- ‚úÖ **Better throughput**: Specialized workers can handle more requests\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Client Request\n",
    "    ‚Üì\n",
    "Frontend (Router)\n",
    "    ‚Üì\n",
    "Prefill Worker (GPU 0) ‚Üí processes prompt ‚Üí generates KV cache\n",
    "    ‚Üì\n",
    "Decode Worker (GPU 1) ‚Üí receives KV cache ‚Üí generates tokens\n",
    "    ‚Üì\n",
    "Response to Client\n",
    "```\n",
    "\n",
    "### Deployment Configuration\n",
    "\n",
    "We'll use a `DynamoGraphDeployment` resource that defines:\n",
    "- **Frontend**: OpenAI-compatible API endpoint with disaggregated routing\n",
    "- **VllmPrefillWorker**: 1 replica on GPU 0 for prompt processing\n",
    "- **VllmDecodeWorker**: 1 replica on GPU 1 for token generation\n",
    "\n",
    "### Step 1: Create Deployment Manifest\n",
    "\n",
    "We will create the `disagg_router.yaml` file dynamically with your specific configuration variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb253ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set defaults if not already set\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Create the deployment YAML with environment variables\n",
    "cat <<EOF > disagg_router.yaml\n",
    "apiVersion: nvidia.com/v1alpha1\n",
    "kind: DynamoGraphDeployment\n",
    "metadata:\n",
    "  name: vllm-disagg-router\n",
    "spec:\n",
    "  services:\n",
    "    Frontend:\n",
    "      dynamoNamespace: vllm-disagg-router\n",
    "      componentType: frontend\n",
    "      replicas: 1\n",
    "      extraPodSpec:\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:$RELEASE_VERSION\n",
    "      envs:\n",
    "        - name: DYN_ROUTER_MODE\n",
    "          value: disaggregated\n",
    "    VllmPrefillWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-disagg-router\n",
    "      componentType: worker\n",
    "      replicas: 1\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: \"info\"\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: $CACHE_PATH\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:$RELEASE_VERSION\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --tensor-parallel-size 1\n",
    "    VllmDecodeWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-disagg-router\n",
    "      componentType: worker\n",
    "      replicas: 1\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: \"info\"\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: $CACHE_PATH\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:$RELEASE_VERSION\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --tensor-parallel-size 1\n",
    "EOF\n",
    "\n",
    "echo \"‚úì Deployment manifest created: disagg_router.yaml\"\n",
    "echo \"  Using Image Version: $RELEASE_VERSION\"\n",
    "echo \"  Using Cache Path:    $CACHE_PATH\"\n",
    "echo \"\"\n",
    "echo \"Verify the configuration:\"\n",
    "grep \"image:\" disagg_router.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ce2c1a",
   "metadata": {},
   "source": [
    "### Step 2: Deploy the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set defaults if not already set\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Apply the deployment\n",
    "kubectl apply -f disagg_router.yaml --namespace $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Deployment created. This will take 4-6 minutes for first run.\"\n",
    "echo \"  - Pulling container images\"\n",
    "echo \"  - Downloading model from HuggingFace\"\n",
    "echo \"  - Loading model into GPU memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a246a",
   "metadata": {},
   "source": [
    "### Step 2b: Expose Frontend Service via NodePort\n",
    "\n",
    "**CRITICAL**: By default, the deployment is internal-only. We must expose it via a NodePort Service to access it on port `30100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9777ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Create NodePort service to expose the frontend\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: vllm-frontend-nodeport\n",
    "  namespace: $NAMESPACE\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    nvidia.com/dynamo-component: Frontend\n",
    "    nvidia.com/dynamo-graph-deployment-name: vllm-disagg-router\n",
    "  ports:\n",
    "  - port: 8000\n",
    "    targetPort: 8000\n",
    "    nodePort: 30100\n",
    "    protocol: TCP\n",
    "    name: http\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Service exposed on NodePort 30100\"\n",
    "echo \"  Access URL: http://$NODE_IP:30100\"\n",
    "echo \"\"\n",
    "echo \"Note: The service will be accessible once the frontend pod is running.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab1818b",
   "metadata": {},
   "source": [
    "### Step 3: Monitor Deployment Progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e172776",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"Expected pods:\"\n",
    "echo \"  - vllm-disagg-router-frontend-xxxxx     (Frontend)\"\n",
    "echo \"  - vllm-disagg-router-vllmprefillworker-xxxxx (Prefill Worker on GPU 0)\"\n",
    "echo \"  - vllm-disagg-router-vllmdecodeworker-xxxxx  (Decode Worker on GPU 1)\"\n",
    "echo \"\"\n",
    "echo \"Waiting for deployment pods to be ready (this may take 4-6 minutes for first run)...\"\n",
    "echo \"\"\n",
    "\n",
    "# Wait for pods to be ready (timeout after 10 minutes for model download)\n",
    "TIMEOUT=600\n",
    "ELAPSED=0\n",
    "INTERVAL=10\n",
    "\n",
    "while [ $ELAPSED -lt $TIMEOUT ]; do\n",
    "    # Get vllm pod status\n",
    "    VLLM_PODS=$(kubectl get pods -n $NAMESPACE 2>/dev/null | grep vllm || true)\n",
    "    NOT_READY=$(echo \"$VLLM_PODS\" | grep -v \"1/1.*Running\" | grep -v \"^$\" | wc -l)\n",
    "    TOTAL=$(echo \"$VLLM_PODS\" | grep -v \"^$\" | wc -l)\n",
    "    READY=$((TOTAL - NOT_READY))\n",
    "    \n",
    "    echo \"[$ELAPSED s] VLLM Pods ready: $READY/$TOTAL\"\n",
    "    kubectl get pods -n $NAMESPACE | grep -E '(NAME|vllm)'\n",
    "    \n",
    "    # Check if all vllm pods are ready\n",
    "    if [ $NOT_READY -eq 0 ] && [ $TOTAL -ge 3 ]; then\n",
    "        echo \"\"\n",
    "        echo \"‚úì All deployment pods are ready!\"\n",
    "        echo \"\"\n",
    "        echo \"DynamoGraphDeployment status:\"\n",
    "        kubectl get dynamographdeployment -n $NAMESPACE\n",
    "        break\n",
    "    fi\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"Waiting for pods to be ready... (checking again in ${INTERVAL}s)\"\n",
    "    echo \"üí° Tip: Model download happens on first run and may take 3-5 minutes\"\n",
    "    echo \"\"\n",
    "    sleep $INTERVAL\n",
    "    ELAPSED=$((ELAPSED + INTERVAL))\n",
    "done\n",
    "\n",
    "if [ $ELAPSED -ge $TIMEOUT ]; then\n",
    "    echo \"‚ö†Ô∏è  Timeout waiting for pods to be ready\"\n",
    "    echo \"Check logs: kubectl logs -l component=VllmPrefillWorker -n $NAMESPACE --tail=50\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ed77b",
   "metadata": {},
   "source": [
    "### Step 4: View Worker Logs (Optional)\n",
    "\n",
    "While waiting for the deployment, you can watch the model loading progress in both workers.\n",
    "\n",
    "**Note**: In disaggregated serving, both the prefill and decode workers load the model separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Get logs from worker pods\n",
    "PREFILL_POD=$(kubectl get pods -n $NAMESPACE | grep vllmprefillworker | awk '{print $1}' | head -1)\n",
    "DECODE_POD=$(kubectl get pods -n $NAMESPACE | grep vllmdecodeworker | awk '{print $1}' | head -1)\n",
    "\n",
    "if [ -n \"$PREFILL_POD\" ]; then\n",
    "    echo \"=== Prefill Worker Logs (GPU 0): $PREFILL_POD ===\"\n",
    "    echo \"Look for:\"\n",
    "    echo \"  - 'Loading model weights...' (downloading)\"\n",
    "    echo \"  - 'Model loading took X.XX GiB' (loaded)\"\n",
    "    echo \"\"\n",
    "    kubectl logs $PREFILL_POD -n $NAMESPACE --tail=30\n",
    "    echo \"\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$DECODE_POD\" ]; then\n",
    "    echo \"=== Decode Worker Logs (GPU 1): $DECODE_POD ===\"\n",
    "    echo \"Look for:\"\n",
    "    echo \"  - 'Loading model weights...' (downloading)\"\n",
    "    echo \"  - 'Model loading took X.XX GiB' (loaded)\"\n",
    "    echo \"\"\n",
    "    kubectl logs $DECODE_POD -n $NAMESPACE --tail=30\n",
    "fi\n",
    "\n",
    "if [ -z \"$PREFILL_POD\" ] && [ -z \"$DECODE_POD\" ]; then\n",
    "    echo \"Worker pods not found yet, please wait and try again\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b514233",
   "metadata": {},
   "source": [
    "## Section 4: Testing and Validation\n",
    "\n",
    "### Objectives\n",
    "- Expose the service locally using port forwarding\n",
    "- Send test requests to the deployment\n",
    "- Verify OpenAI API compatibility\n",
    "- Test streaming and non-streaming responses\n",
    "\n",
    "### Testing Strategy\n",
    "Once your deployment is running (`1/1 Ready`), you'll:\n",
    "1. Connect to the frontend via NodePort (already exposed on port 30100)\n",
    "2. Test with curl commands\n",
    "3. Verify response format and functionality\n",
    "\n",
    "### Step 1: Get Frontend URL\n",
    "\n",
    "The frontend is already exposed via NodePort on port 30100 (set up in Step 2b). Get the connection URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f701f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get the node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Node IP: $NODE_IP\"\n",
    "echo \"\"\n",
    "echo \"Frontend URL: http://$NODE_IP:30100\"\n",
    "echo \"\"\n",
    "echo \"‚úì Access the frontend at: http://$NODE_IP:30100\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a2557c",
   "metadata": {},
   "source": [
    "### Step 2: Test the `/v1/models` Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Test the /v1/models endpoint\n",
    "curl http://$NODE_IP:30100/v1/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7554a",
   "metadata": {},
   "source": [
    "### Step 3: Simple Non-Streaming Chat Completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aba864",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Test non-streaming chat completion\n",
    "curl http://$NODE_IP:30100/v1/chat/completions \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello! How are you?\"}],\n",
    "    \"stream\": false,\n",
    "    \"max_tokens\": 50\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413779db",
   "metadata": {},
   "source": [
    "### Step 4: Test Streaming Response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Test streaming chat completion\n",
    "curl http://$NODE_IP:30100/v1/chat/completions \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem about AI\"}],\n",
    "    \"stream\": true,\n",
    "    \"max_tokens\": 100\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb4b26",
   "metadata": {},
   "source": [
    "### Step 5: Test with Different Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Test with different parameters\n",
    "curl http://$NODE_IP:30100/v1/chat/completions \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in one sentence\"}],\n",
    "    \"stream\": false,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100,\n",
    "    \"top_p\": 0.9\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56007c2",
   "metadata": {},
   "source": [
    "## Section 5: Benchmarking with AI-Perf\n",
    "\n",
    "### Objectives\n",
    "- Install and configure AI-Perf benchmarking tool\n",
    "- Run performance benchmarks against your Kubernetes deployment\n",
    "- Analyze throughput, latency, and token metrics\n",
    "- Compare performance across different configurations\n",
    "\n",
    "### Metrics to Measure\n",
    "- Throughput (requests/second, tokens/second)\n",
    "- Latency (TTFT - Time To First Token, TPOT - Time Per Output Token, end-to-end)\n",
    "- GPU utilization\n",
    "- KV cache efficiency\n",
    "\n",
    "### Benchmarking Setup\n",
    "You'll run AI-Perf from your local machine against the port-forwarded service, simulating:\n",
    "- Different concurrency levels (fixed concurrent requests)\n",
    "- Request rate patterns (requests per second)\n",
    "- Various workload characteristics\n",
    "\n",
    "### Step 1: Install AI-Perf (if not already installed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Ensure pip is available in the venv\n",
    "print(\"Setting up pip in venv...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--default-pip\"], \n",
    "               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# Install AI-Perf in the venv\n",
    "print(\"Installing AI-Perf...\")\n",
    "result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"aiperf\", \"-q\"])\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úì AI-Perf installed successfully\")\n",
    "    # Verify aiperf can be imported\n",
    "    verify = subprocess.run([sys.executable, \"-c\", \"import aiperf\"], capture_output=True)\n",
    "    if verify.returncode == 0:\n",
    "        print(\"  aiperf is ready to use\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Installation had issues, but may still work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9cde0",
   "metadata": {},
   "source": [
    "### Step 2: Run Baseline Benchmark (Low Concurrency)\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Run benchmarks in a TERMINAL, not in notebook cells (aiperf can crash the kernel).**\n",
    "\n",
    "**To run this benchmark:**\n",
    "\n",
    "1. Open a new terminal (File ‚Üí New ‚Üí Terminal in JupyterLab)\n",
    "2. Copy and paste this command:\n",
    "\n",
    "```\n",
    "cd ~/dynamo-grove-brev/resources && ./run-benchmark.sh baseline\n",
    "```\n",
    "\n",
    "This will run a low concurrency benchmark (1 concurrent request, 100 total requests) and display metrics including:\n",
    "- Time to First Token (TTFT)\n",
    "- Token throughput\n",
    "- Request latency\n",
    "- Percentile distributions (p50, p90, p99)\n",
    "\n",
    "### Step 3: Run Benchmark with Higher Concurrency\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Run benchmarks in a TERMINAL, not in notebook cells.**\n",
    "\n",
    "**To run this benchmark:**\n",
    "\n",
    "1. Open a new terminal (File ‚Üí New ‚Üí Terminal in JupyterLab)\n",
    "2. Copy and paste this command:\n",
    "\n",
    "```\n",
    "cd ~/dynamo-grove-brev/resources && ./run-benchmark.sh high\n",
    "```\n",
    "\n",
    "This will run a high concurrency benchmark (4 concurrent requests, 200 total requests) to stress test the system and see how it handles multiple simultaneous users.\n",
    "\n",
    "### Step 4: Run Benchmark with Request Rate\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Run benchmarks in a TERMINAL, not in notebook cells.**\n",
    "\n",
    "**To run this benchmark:**\n",
    "\n",
    "1. Open a new terminal (File ‚Üí New ‚Üí Terminal in JupyterLab)\n",
    "2. Copy and paste this command:\n",
    "\n",
    "```\n",
    "cd ~/dynamo-grove-brev/resources && ./run-benchmark.sh rate\n",
    "```\n",
    "\n",
    "This will run a request rate benchmark (10 requests per second, 200 total requests) to simulate a steady stream of users hitting the API at a controlled rate.\n",
    "\n",
    "### Step 5: Analyze Results\n",
    "\n",
    "Review the benchmark outputs above. Key metrics to look for:\n",
    "- **Throughput**: requests/second and tokens/second\n",
    "- **TTFT (Time To First Token)**: How quickly does the first token appear?\n",
    "- **TPOT (Time Per Output Token)**: Generation speed\n",
    "- **End-to-end latency**: Total request time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dfb023",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- Continue to **Lab 2: Monitoring and Observability** to add dashboards and metrics\n",
    "- Or skip to **Lab 3: Distributed Serving** for multi-GPU deployments\n",
    "- See **Appendix** for cleanup commands (only if completely done with all labs)\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Check Pod Status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d9fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Check all pods in your namespace\n",
    "kubectl get pods -n $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"# To describe a specific pod to see errors:\"\n",
    "echo \"# kubectl describe pod <pod-name> -n $NAMESPACE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de1b0fb",
   "metadata": {},
   "source": [
    "### View Pod Logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06889b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# View logs from a specific component\n",
    "echo \"Frontend logs:\"\n",
    "kubectl logs -l component=Frontend -n $NAMESPACE --tail=50\n",
    "\n",
    "echo \"\"\n",
    "echo \"Worker logs:\"\n",
    "kubectl logs -l component=VllmDecodeWorker -n $NAMESPACE --tail=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c669d61",
   "metadata": {},
   "source": [
    "### Check Deployment Status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Check DynamoGraphDeployment status\n",
    "echo \"DynamoGraphDeployment status:\"\n",
    "kubectl describe dynamographdeployment vllm-disagg-router -n $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"Operator logs:\"\n",
    "kubectl logs -l app.kubernetes.io/name=dynamo-operator -n $NAMESPACE --tail=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8eea6a",
   "metadata": {},
   "source": [
    "### Check Recent Events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d354b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# View recent events in your namespace\n",
    "kubectl get events -n $NAMESPACE --sort-by=.lastTimestamp | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b6070",
   "metadata": {},
   "source": [
    "### Common Issues\n",
    "\n",
    "1. **ImagePullBackOff**: Check if you have access to NGC containers. Verify image version is correct.\n",
    "2. **Pods stuck in Pending**: Check if GPU resources are available: `kubectl describe pod <pod-name> -n $NAMESPACE`\n",
    "3. **Model download slow**: First run takes longer due to model download. Check worker logs for progress.\n",
    "4. **Port forward not working**: Make sure pods are `1/1 Ready` before forwarding. Kill existing port-forward processes: `pkill -f port-forward`\n",
    "\n",
    "---\n",
    "\n",
    "## Known Issues (v0.8.0)\n",
    "\n",
    "**‚ö†Ô∏è Important Notes for Dynamo v0.8.0:**\n",
    "\n",
    "1. **Validation Webhook Timing**: In rare cases, validation webhooks may reject valid configurations during high cluster load. If deployment fails with validation errors, wait 30 seconds and retry.\n",
    "\n",
    "2. **K8s-native Discovery Cold Start**: First request after deployment may take 5-10s longer as EndpointSlices propagate. Subsequent requests are fast.\n",
    "\n",
    "3. **TCP Transport Port Conflicts**: If using custom ports, ensure they don't conflict with existing services. Default ports (8000, 8001) are usually safe.\n",
    "\n",
    "4. **Model Loading on Multiple GPUs**: For multi-GPU setups, ensure sufficient shared memory (`/dev/shm`) is available. Add `--shm-size=2g` to worker pod spec if needed.\n",
    "\n",
    "**Workarounds:**\n",
    "- For webhook issues: Add `--wait --timeout=5m` to helm installs\n",
    "- For discovery delays: Add readiness probe with longer `initialDelaySeconds`\n",
    "- Check [GitHub Issues](https://github.com/ai-dynamo/dynamo/issues) for latest updates\n",
    "\n",
    "**Fixed in v0.8.1+:** Many of these issues are addressed in patch releases. Check release notes for updates.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "- ‚úÖ How to set up a namespace-scoped Dynamo deployment on Kubernetes\n",
    "- ‚úÖ Kubernetes-based disaggregated deployment architecture\n",
    "- ‚úÖ Creating and managing DynamoGraphDeployment resources\n",
    "- ‚úÖ Backend engine deployment (vLLM)\n",
    "- ‚úÖ Testing with OpenAI-compatible API\n",
    "- ‚úÖ Performance benchmarking with AI-Perf\n",
    "\n",
    "### Key Takeaways\n",
    "- Namespace-scoped operators enable safe multi-tenant deployments\n",
    "- Disaggregated serving separates prefill and decode for optimized resource utilization\n",
    "- KV-cache routing provides intelligent load balancing across replicas\n",
    "- DynamoGraphDeployment CRD simplifies complex inference deployments\n",
    "- AI-Perf provides comprehensive performance insights\n",
    "\n",
    "### Next Steps\n",
    "- **(Optional)** Complete the **Monitoring Extension** (`lab1-monitoring.md`) to set up Prometheus and Grafana for observability\n",
    "- In **Lab 2**, you'll explore advanced optimizations and use AIConfigurator to optimize configurations for larger models\n",
    "\n",
    "---\n",
    "\n",
    "## Appendix: Step-by-Step Commands\n",
    "\n",
    "This appendix provides complete commands for each section. Use these as a reference during the lab.\n",
    "\n",
    "**Note for MicroK8s users:** Replace `kubectl` with `microk8s kubectl` in all commands below, or set up an alias:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a29d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "alias kubectl='microk8s kubectl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707bcd9",
   "metadata": {},
   "source": [
    "### A0. Troubleshooting Pod Issues\n",
    "\n",
    "If your pods are in `Error` or `CrashLoopBackOff` state, use this comprehensive diagnostic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"=== Pod Status ===\"\n",
    "kubectl get pods -n $NAMESPACE | grep vllm\n",
    "echo \"\"\n",
    "\n",
    "echo \"=== GPU Availability ===\"\n",
    "kubectl get nodes -o custom-columns=NAME:.metadata.name,GPUs:.status.capacity.nvidia\\\\.com/gpu,GPU-Allocatable:.status.allocatable.nvidia\\\\.com/gpu\n",
    "echo \"\"\n",
    "\n",
    "echo \"=== Checking Secrets ===\"\n",
    "kubectl get secret hf-token-secret -n $NAMESPACE &>/dev/null && echo \"‚úì HF token secret exists\" || echo \"‚úó HF token secret missing!\"\n",
    "kubectl get secret ngc-secret -n $NAMESPACE &>/dev/null && echo \"‚úì NGC secret exists\" || echo \"‚úó NGC secret missing!\"\n",
    "echo \"\"\n",
    "\n",
    "echo \"=== Prefill Worker Logs (last 30 lines) ===\"\n",
    "PREFILL_POD=$(kubectl get pods -n $NAMESPACE | grep vllmprefillworker | awk '{print $1}' | head -1)\n",
    "if [ -n \"$PREFILL_POD\" ]; then\n",
    "    kubectl logs $PREFILL_POD -n $NAMESPACE --tail=30\n",
    "else\n",
    "    echo \"No prefill pod found\"\n",
    "fi\n",
    "echo \"\"\n",
    "\n",
    "echo \"=== Decode Worker Logs (last 30 lines) ===\"\n",
    "DECODE_POD=$(kubectl get pods -n $NAMESPACE | grep vllmdecodeworker | awk '{print $1}' | head -1)\n",
    "if [ -n \"$DECODE_POD\" ]; then\n",
    "    kubectl logs $DECODE_POD -n $NAMESPACE --tail=30\n",
    "else\n",
    "    echo \"No decode pod found\"\n",
    "fi\n",
    "echo \"\"\n",
    "\n",
    "echo \"=== Common Issues ===\"\n",
    "echo \"1. If 'insufficient gpu' error: You need 2 GPUs for disaggregated serving\"\n",
    "echo \"2. If 'HF_TOKEN' error: Make sure you created the hf-token-secret\"\n",
    "echo \"3. If 'ImagePullBackOff': Check NGC secret and credentials\"\n",
    "echo \"4. If model download errors: Check network connectivity to huggingface.co\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a26495",
   "metadata": {},
   "source": [
    "### A1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Verify kubectl is installed and configured\n",
    "kubectl version --client\n",
    "kubectl cluster-info\n",
    "\n",
    "# Set your configuration\n",
    "export NAMESPACE=\"dynamo\"\n",
    "export RELEASE_VERSION=\"0.8.0\"     # Dynamo version\n",
    "export HF_TOKEN=\"your_hf_token\"    # Your HuggingFace token\n",
    "export CACHE_PATH=\"/data/huggingface-cache\"  # Shared cache path\n",
    "\n",
    "# Create your personal namespace\n",
    "kubectl create namespace ${NAMESPACE}\n",
    "\n",
    "# Verify namespace was created\n",
    "kubectl get namespace ${NAMESPACE}\n",
    "\n",
    "# Check GPU nodes are available (optional)\n",
    "kubectl get nodes -o custom-columns=NAME:.metadata.name,GPUs:.status.capacity.nvidia\\\\.com/gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460850a",
   "metadata": {},
   "source": [
    "### A2. Install Dynamo Platform (Namespace-Scoped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e64093",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Step 1: Check if CRDs are already installed (cluster-wide)\n",
    "if kubectl get crd dynamographdeployments.nvidia.com &>/dev/null && \\\n",
    "   kubectl get crd dynamocomponentdeployments.nvidia.com &>/dev/null; then\n",
    "    echo \"‚úì CRDs already installed\"\n",
    "else\n",
    "    echo \"‚ö†Ô∏è  CRDs not found. Ask instructor to install them, or run:\"\n",
    "    echo \"helm fetch https://helm.ngc.nvidia.com/nvidia/ai-dynamo/charts/dynamo-crds-${RELEASE_VERSION}.tgz\"\n",
    "    echo \"helm install dynamo-crds dynamo-crds-${RELEASE_VERSION}.tgz --namespace default\"\n",
    "fi\n",
    "\n",
    "# Step 2: Download Dynamo platform helm chart\n",
    "helm fetch https://helm.ngc.nvidia.com/nvidia/ai-dynamo/charts/dynamo-platform-${RELEASE_VERSION}.tgz\n",
    "\n",
    "# Step 3: Install namespace-scoped Dynamo platform\n",
    "# IMPORTANT: --set dynamo-operator.namespaceRestriction.enabled=true restricts operator to this namespace\n",
    "helm install dynamo-platform dynamo-platform-${RELEASE_VERSION}.tgz \\\n",
    "  --namespace ${NAMESPACE} \\\n",
    "  --set dynamo-operator.namespaceRestriction.enabled=true\n",
    "\n",
    "# Step 4: Wait for platform pods to be ready (~2-3 minutes)\n",
    "echo \"Waiting for platform pods to be ready...\"\n",
    "kubectl wait --for=condition=ready pod \\\n",
    "  --all \\\n",
    "  --namespace ${NAMESPACE} \\\n",
    "  --timeout=300s\n",
    "\n",
    "# Step 5: Verify platform is running\n",
    "kubectl get pods -n ${NAMESPACE}\n",
    "# You should see: dynamo-operator, etcd, and nats pods in Running state\n",
    "\n",
    "# Step 6: Create HuggingFace token secret\n",
    "kubectl create secret generic hf-token-secret \\\n",
    "  --from-literal=HF_TOKEN=\"${HF_TOKEN}\" \\\n",
    "  --namespace ${NAMESPACE}\n",
    "\n",
    "# Verify secret was created\n",
    "kubectl get secret hf-token-secret -n ${NAMESPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd088b2b",
   "metadata": {},
   "source": [
    "### A3. Deploy Your First Model\n",
    "\n",
    "Create a deployment YAML file `disagg_router.yaml`:\n",
    "\n",
    "```yaml\n",
    "# disagg_router.yaml\n",
    "apiVersion: nvidia.com/v1alpha1\n",
    "kind: DynamoGraphDeployment\n",
    "metadata:\n",
    "  name: vllm-disagg-router\n",
    "spec:\n",
    "  services:\n",
    "    Frontend:\n",
    "      dynamoNamespace: vllm-disagg-router\n",
    "      componentType: frontend\n",
    "      replicas: 1\n",
    "      extraPodSpec:\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:${RELEASE_VERSION}\n",
    "      envs:\n",
    "        - name: DYN_ROUTER_MODE\n",
    "          value: disaggregated\n",
    "    VllmPrefillWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-disagg-router\n",
    "      componentType: worker\n",
    "      replicas: 1\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: \"info\"\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: ${CACHE_PATH}  # Defaults to /data/huggingface-cache\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:${RELEASE_VERSION}\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --tensor-parallel-size 1\n",
    "    VllmDecodeWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-disagg-router\n",
    "      componentType: worker\n",
    "      replicas: 1\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: \"info\"\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: ${CACHE_PATH}  # Defaults to /data/huggingface-cache\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:${RELEASE_VERSION}\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --tensor-parallel-size 1\n",
    "```\n",
    "\n",
    "Deploy the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29892385",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Apply the deployment\n",
    "kubectl apply -f disagg_router.yaml --namespace ${NAMESPACE}\n",
    "\n",
    "# Monitor deployment progress\n",
    "kubectl get dynamographdeployment -n ${NAMESPACE}\n",
    "\n",
    "# Watch pods starting up (this takes 4-6 minutes for first run)\n",
    "kubectl get pods -n ${NAMESPACE} -w\n",
    "# Press Ctrl+C to stop watching\n",
    "\n",
    "# Check specific pod status\n",
    "kubectl get pods -n ${NAMESPACE} | grep vllm\n",
    "\n",
    "# View worker logs to see model loading progress\n",
    "WORKER_POD=$(kubectl get pods -n ${NAMESPACE} | grep vllmdecodeworker | head -1 | awk '{print $1}')\n",
    "kubectl logs ${WORKER_POD} -n ${NAMESPACE} --tail=50 --follow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02922572",
   "metadata": {},
   "source": [
    "### A4. Test the Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb77520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The frontend is exposed via NodePort on port 30100\n",
    "# Get the node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Frontend URL: http://$NODE_IP:30100\"\n",
    "echo \"\"\n",
    "echo \"Quick test commands (run in terminal):\"\n",
    "echo \"\"\n",
    "echo \"# Test 1: Check available models\"\n",
    "echo \"curl http://$NODE_IP:30100/v1/models\"\n",
    "echo \"\"\n",
    "echo \"# Test 2: Simple chat completion\"\n",
    "echo \"curl http://$NODE_IP:30100/v1/chat/completions -H 'Content-Type: application/json' -d '{\\\"model\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\", \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello!\\\"}], \\\"stream\\\": false, \\\"max_tokens\\\": 50}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30a67e9",
   "metadata": {},
   "source": [
    "### A5. Benchmark with AI-Perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "FRONTEND_URL=\"http://$NODE_IP:30100\"\n",
    "\n",
    "echo \"Benchmarking frontend at: $FRONTEND_URL\"\n",
    "echo \"\"\n",
    "\n",
    "# Install AI-Perf (if not already installed)\n",
    "pip install aiperf -q\n",
    "\n",
    "echo \"=== Running benchmarks ===\"\n",
    "echo \"\"\n",
    "\n",
    "# Run a simple benchmark (adjust parameters as needed)\n",
    "echo \"1. Low concurrency benchmark...\"\n",
    "aiperf profile     --log-level warning     --model Qwen/Qwen2.5-1.5B-Instruct     --url $FRONTEND_URL     --endpoint-type chat     --streaming     --concurrency 1     --request-count 100\n",
    "\n",
    "# Run with higher concurrency\n",
    "echo \"\"\n",
    "echo \"2. High concurrency benchmark...\"\n",
    "aiperf profile     --log-level warning     --model Qwen/Qwen2.5-1.5B-Instruct     --url $FRONTEND_URL     --endpoint-type chat     --streaming     --concurrency 4     --request-count 200\n",
    "\n",
    "# Run with request rate\n",
    "echo \"\"\n",
    "echo \"3. Request rate benchmark...\"\n",
    "aiperf profile     --log-level warning     --model Qwen/Qwen2.5-1.5B-Instruct     --url $FRONTEND_URL     --endpoint-type chat     --streaming     --request-rate 10     --request-count 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bce9fc7",
   "metadata": {},
   "source": [
    "### A6. Scale Your Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Edit your disagg_router.yaml and change replicas from 1 to 2\n",
    "# Then reapply:\n",
    "kubectl apply -f disagg_router.yaml --namespace ${NAMESPACE}\n",
    "\n",
    "# Watch the new worker come online\n",
    "kubectl get pods -n ${NAMESPACE} -w\n",
    "\n",
    "# Test that load is distributed (KV-cache routing should work)\n",
    "# Run multiple requests and check logs from both workers\n",
    "kubectl logs -l component=VllmDecodeWorker -n ${NAMESPACE} --tail=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44886db",
   "metadata": {},
   "source": [
    "### A7. Cleanup (‚ö†Ô∏è ONLY AFTER COMPLETING ALL LABS)\n",
    "\n",
    "**‚ö†Ô∏è WARNING: DO NOT RUN THIS DURING THE WORKSHOP**\n",
    "\n",
    "This cleanup is **ONLY** for when you're completely done with:\n",
    "- ‚úÖ Lab 1: Deployment\n",
    "- ‚úÖ Lab 2: Monitoring (requires Lab 1 deployment running)\n",
    "- ‚úÖ Lab 3: Distributed Serving (requires platform installed)\n",
    "\n",
    "**If you're continuing to Lab 2 or Lab 3, DO NOT run these commands.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495bbba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Step 1: Delete the Lab 1 deployment (only after Lab 2 is done)\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "kubectl delete dynamographdeployment vllm-disagg-router -n ${NAMESPACE}\n",
    "kubectl delete svc vllm-frontend-nodeport -n ${NAMESPACE}\n",
    "\n",
    "# Step 2: Verify pods are terminating\n",
    "kubectl get pods -n ${NAMESPACE}\n",
    "\n",
    "# Step 3: (Optional) Complete cleanup after ALL labs\n",
    "# This removes everything including the platform:\n",
    "# kubectl delete namespace ${NAMESPACE}\n",
    "# helm uninstall dynamo-platform -n ${NAMESPACE}\n",
    "# helm uninstall dynamo-crds -n default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c444aa",
   "metadata": {},
   "source": [
    "### A8. Troubleshooting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4d283",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check pod status\n",
    "kubectl get pods -n ${NAMESPACE}\n",
    "\n",
    "# Describe a pod to see errors\n",
    "kubectl describe pod <pod-name> -n ${NAMESPACE}\n",
    "\n",
    "# View logs from a specific pod\n",
    "kubectl logs <pod-name> -n ${NAMESPACE}\n",
    "\n",
    "# Check DynamoGraphDeployment status\n",
    "kubectl describe dynamographdeployment vllm-disagg-router -n ${NAMESPACE}\n",
    "\n",
    "# Check operator logs\n",
    "kubectl logs -l app.kubernetes.io/name=dynamo-operator -n ${NAMESPACE}\n",
    "\n",
    "# Check if image pull is working\n",
    "kubectl get events -n ${NAMESPACE} --sort-by='.lastTimestamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740d94f",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-kernelspec,-widgets,-language_info"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
