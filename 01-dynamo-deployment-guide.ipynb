{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06757880",
   "metadata": {},
   "source": [
    "# Lab 1: Introduction and Kubernetes-Based Deployment\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you will:\n",
    "- Set up Kubernetes cluster with Dynamo platform\n",
    "- Deploy Dynamo, NVIDIA's inference serving framework\n",
    "- Configure a model using disaggregated serving (separate prefill and decode workers)\n",
    "- Test the deployment with OpenAI-compatible API\n",
    "- Benchmark the deployment using AI-Perf\n",
    "\n",
    "**What You'll Learn:**\n",
    "- How to deploy Dynamo on Kubernetes\n",
    "- Understanding disaggregated serving architecture\n",
    "- Using Dynamo's OpenAI-compatible API\n",
    "- Monitoring inference performance\n",
    "\n",
    "**Duration**: ~90 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Environment Setup\n",
    "\n",
    "### Objectives\n",
    "- Verify Kubernetes access \n",
    "- Install Dynamo dependencies\n",
    "- Set up prerequisites (kubectl, helm)\n",
    "\n",
    "### Prerequisites\n",
    "Before starting, ensure you have:\n",
    "- ‚úÖ Kubernetes cluster access (kubeconfig provided by instructor)\n",
    "- ‚úÖ `kubectl` installed (version 1.24+) or `microk8s kubectl`\n",
    "- ‚úÖ `helm` 3.x installed\n",
    "- ‚úÖ HuggingFace token from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "### Step 2: Set Configuration Variables\n",
    "\n",
    "Run this to set up your environment. The defaults work for most users:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69367f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables (these defaults work for most setups)\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n",
    "echo \"üéì Lab 1: Environment Configuration\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n",
    "echo \"  Release Version:  $RELEASE_VERSION\"\n",
    "echo \"  Namespace:        $NAMESPACE\"\n",
    "echo \"  Cache Path:       $CACHE_PATH\"\n",
    "echo \"  Node IP:          $NODE_IP\"\n",
    "echo \"\"\n",
    "echo \"üìå Service Ports (after deployment):\"\n",
    "echo \"  Frontend API:     http://$NODE_IP:30100\"\n",
    "echo \"  Grafana:          http://$NODE_IP:30080\"\n",
    "echo \"\"\n",
    "echo \"üí° Note: Frontend will be accessible after deploying in Section 3\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65e551",
   "metadata": {},
   "source": [
    "### Step 3: Verify Kubernetes Access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5dd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Verify kubectl is installed and configured\n",
    "echo \"=== kubectl version ===\"\n",
    "kubectl version --client\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== Cluster info ===\"\n",
    "kubectl cluster-info\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== GPU nodes ===\"\n",
    "kubectl get nodes -o custom-columns=NAME:.metadata.name,GPUs:.status.capacity.nvidia\\\\.com/gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006c5a4",
   "metadata": {},
   "source": [
    "### Step 4: Set Up NGC Authentication\n",
    "\n",
    "To access NVIDIA's Dynamo container images, you need to authenticate with NGC.\n",
    "\n",
    "#### Get Your NGC API Key\n",
    "\n",
    "1. Go to [NGC](https://ngc.nvidia.com/)\n",
    "2. Sign in or create an account\n",
    "3. Click on your profile in the top right corner\n",
    "4. Select **\"Setup\"** ‚Üí **\"Generate API Key\"**\n",
    "5. Copy your API key (it will only be shown once!)\n",
    "\n",
    "#### Set NGC API Key\n",
    "\n",
    "**Get your NGC API Key from [ngc.nvidia.com](https://ngc.nvidia.com/)** (Go to Profile > Setup > Generate API Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Get NGC API key from user\n",
    "print(\"Enter your NGC API Key from https://ngc.nvidia.com/\")\n",
    "print(\"(Go to Profile > Setup > Generate API Key)\")\n",
    "print(\"\")\n",
    "NGC_API_KEY = getpass.getpass(\"NGC API Key: \")\n",
    "\n",
    "# Save it for later use (creating pull secrets)\n",
    "os.environ['NGC_API_KEY'] = NGC_API_KEY\n",
    "\n",
    "print(\"\")\n",
    "print(\"‚úì NGC API key saved\")\n",
    "print(\"  You can now use it to login and create pull secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c464e",
   "metadata": {},
   "source": [
    "#### Set HuggingFace Token\n",
    "\n",
    "**HuggingFace token is required to download models.** Get yours from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) (Create a 'Read' token if you don't have one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d274d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Get HuggingFace token from user\n",
    "print(\"Enter your HuggingFace Token from https://huggingface.co/settings/tokens\")\n",
    "print(\"(Create a 'Read' token if you don't have one)\")\n",
    "print(\"\")\n",
    "HF_TOKEN = getpass.getpass(\"HF Token: \")\n",
    "\n",
    "# Save it for later use\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "print(\"\")\n",
    "print(\"‚úì HuggingFace token saved to environment\")\n",
    "print(\"  Available as $HF_TOKEN in bash cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4899e60",
   "metadata": {},
   "source": [
    "#### Login to NGC Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31184cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Login to NGC container registry\n",
    "echo \"$NGC_API_KEY\" | helm registry login nvcr.io --username '$oauthtoken' --password-stdin\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì NGC authentication complete\"\n",
    "echo \"  You can now pull Dynamo container images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23615f4",
   "metadata": {},
   "source": [
    "### Step 5: Create Your Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create the namespace\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "kubectl create namespace $NAMESPACE 2>&1 | grep -v \"AlreadyExists\" || true\n",
    "\n",
    "# Verify namespace was created\n",
    "echo \"\"\n",
    "echo \"Verifying namespace:\"\n",
    "kubectl get namespace $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb828d",
   "metadata": {},
   "source": [
    "### Step 6: Create NGC Pull Secret\n",
    "\n",
    "Create a Kubernetes secret so that pods can pull images from NGC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64264a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get variables\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Create NGC image pull secret\n",
    "kubectl create secret docker-registry ngc-secret \\\n",
    "    --docker-server=nvcr.io \\\n",
    "    --docker-username='$oauthtoken' \\\n",
    "    --docker-password=\"$NGC_API_KEY\" \\\n",
    "    --namespace $NAMESPACE \\\n",
    "    2>&1 | grep -v \"AlreadyExists\" || true\n",
    "\n",
    "# Verify secret was created\n",
    "echo \"\"\n",
    "echo \"Verifying NGC secret:\"\n",
    "kubectl get secret ngc-secret -n $NAMESPACE\n",
    "echo \"‚úì NGC pull secret created in namespace: $NAMESPACE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06769d93",
   "metadata": {},
   "source": [
    "### Step 7: Create HuggingFace Token Secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb82ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get variables\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Create HuggingFace token secret\n",
    "kubectl create secret generic hf-token-secret \\\n",
    "    --from-literal=HF_TOKEN=\"$HF_TOKEN\" \\\n",
    "    --namespace $NAMESPACE \\\n",
    "    2>&1 | grep -v \"AlreadyExists\" || true\n",
    "\n",
    "# Verify secret was created\n",
    "echo \"\"\n",
    "echo \"Verifying HuggingFace secret:\"\n",
    "kubectl get secret hf-token-secret -n $NAMESPACE\n",
    "echo \"‚úì HuggingFace token secret created\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86f026",
   "metadata": {},
   "source": [
    "## Section 2: Install Dynamo Platform\n",
    "\n",
    "### Objectives\n",
    "- Install Dynamo CRDs (Custom Resource Definitions) with validation webhooks\n",
    "- Install Dynamo platform (operator with K8s-native discovery)\n",
    "- Verify platform components are running\n",
    "\n",
    "### Architecture (v0.8.0 Simplified)\n",
    "\n",
    "```\n",
    "Client Request\n",
    "      ‚Üì\n",
    "Frontend (OpenAI API + Disaggregated Router)\n",
    "      ‚Üì\n",
    "Prefill Worker (GPU 0) ‚Üí Processes prompt ‚Üí Generates KV cache\n",
    "      ‚Üì\n",
    "Decode Worker (GPU 1) ‚Üí Uses KV cache ‚Üí Generates tokens\n",
    "      ‚Üì\n",
    "Response to Client\n",
    "\n",
    "Infrastructure:\n",
    "- Kubernetes EndpointSlices (service discovery)\n",
    "- TCP Transport (direct worker connections)\n",
    "- Dynamo Operator (manages deployments)\n",
    "- Validation Webhooks (catch errors early)\n",
    "\n",
    "Note: This basic deployment doesn't use NATS or etcd.\n",
    "      Lab 3 covers distributed deployments\n",
    "```\n",
    "\n",
    "### Deployment Mode\n",
    "\n",
    "We're using the **recommended cluster-wide deployment** (default). According to the [official Dynamo documentation](https://github.com/ai-dynamo/dynamo/blob/main/deploy/helm/charts/platform/README.md):\n",
    "\n",
    "- ‚úÖ **Recommended**: One cluster-wide operator per cluster (default)\n",
    "- This is the standard deployment for single-node and production clusters\n",
    "- Install a **namespace-scoped Dynamo operator** that only manages resources in your namespace\n",
    "- The CRDs are cluster-wide and should already be installed (check first)\n",
    "\n",
    "### Step 1: Install Dynamo CRDs\n",
    "\n",
    "**Note:** CRDs are cluster-wide resources and only need to be installed **once per cluster**. This step checks if they exist and installs them if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check if CRDs already exist, install if not\n",
    "if kubectl get crd dynamographdeployments.nvidia.com &>/dev/null && \\\n",
    "   kubectl get crd dynamocomponentdeployments.nvidia.com &>/dev/null; then\n",
    "    echo \"‚úì CRDs already installed\"\n",
    "    kubectl get crd | grep nvidia.com\n",
    "else\n",
    "    echo \"Installing Dynamo CRDs v$RELEASE_VERSION...\"\n",
    "    helm fetch https://helm.ngc.nvidia.com/nvidia/ai-dynamo/charts/dynamo-crds-$RELEASE_VERSION.tgz\n",
    "    helm install dynamo-crds dynamo-crds-$RELEASE_VERSION.tgz --namespace default\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"Verifying CRD installation:\"\n",
    "    kubectl get crd | grep nvidia.com\n",
    "    echo \"\"\n",
    "    echo \"‚úì CRDs include validation webhooks for early error detection\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97140f35",
   "metadata": {},
   "source": [
    "### Step 2: Install Dynamo Platform\n",
    "\n",
    "**Simplified in v0.8.0:** NATS and etcd are now **optional**. Dynamo uses Kubernetes-native service discovery (EndpointSlices) and TCP transport by default, making deployment simpler and reducing infrastructure dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Download platform chart\n",
    "echo \"Downloading Dynamo platform chart v$RELEASE_VERSION...\"\n",
    "helm fetch https://helm.ngc.nvidia.com/nvidia/ai-dynamo/charts/dynamo-platform-$RELEASE_VERSION.tgz\n",
    "\n",
    "# Install Dynamo platform (namespace-scoped, K8s-native discovery)\n",
    "echo \"Installing Dynamo platform in namespace: $NAMESPACE\"\n",
    "echo \"Using K8s-native discovery (no NATS/etcd required)\"\n",
    "helm install dynamo-platform \\\n",
    "    dynamo-platform-$RELEASE_VERSION.tgz \\\n",
    "    --namespace $NAMESPACE \\\n",
    "    --set dynamo-operator.namespaceRestriction.enabled=true\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Platform installation initiated\"\n",
    "echo \"  Discovery: Kubernetes EndpointSlices (native)\"\n",
    "echo \"  Transport: TCP (default in v0.8.0)\"\n",
    "echo \"\"\n",
    "echo \"Waiting for pods to be ready...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1651ae",
   "metadata": {},
   "source": [
    "### Step 3: Wait for Platform Pods to Be Ready\n",
    "\n",
    "Re-run the following cell until all pods report as \"Running\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae834e3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"Waiting for platform pods to be ready...\"\n",
    "echo \"\"\n",
    "\n",
    "# Wait for pods to be ready (timeout after 5 minutes)\n",
    "TIMEOUT=300\n",
    "ELAPSED=0\n",
    "INTERVAL=5\n",
    "\n",
    "while [ $ELAPSED -lt $TIMEOUT ]; do\n",
    "    # Get pod status\n",
    "    NOT_READY=$(kubectl get pods -n $NAMESPACE --no-headers 2>/dev/null | grep -v \"Running\\|Completed\" | wc -l)\n",
    "    TOTAL=$(kubectl get pods -n $NAMESPACE --no-headers 2>/dev/null | wc -l)\n",
    "    READY=$((TOTAL - NOT_READY))\n",
    "    \n",
    "    echo \"[$ELAPSED s] Pods ready: $READY/$TOTAL\"\n",
    "    kubectl get pods -n $NAMESPACE\n",
    "    \n",
    "    # Check if all pods are ready\n",
    "    if [ $NOT_READY -eq 0 ] && [ $TOTAL -gt 0 ]; then\n",
    "        echo \"\"\n",
    "        echo \"‚úì All platform pods are ready!\"\n",
    "        break\n",
    "    fi\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"Waiting for pods to be ready... (checking again in ${INTERVAL}s)\"\n",
    "    echo \"\"\n",
    "    sleep $INTERVAL\n",
    "    ELAPSED=$((ELAPSED + INTERVAL))\n",
    "done\n",
    "\n",
    "if [ $ELAPSED -ge $TIMEOUT ]; then\n",
    "    echo \"‚ö†Ô∏è  Timeout waiting for pods to be ready\"\n",
    "    echo \"Please check pod status manually: kubectl get pods -n $NAMESPACE\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea69662",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cb99af",
   "metadata": {},
   "source": [
    "## Section 3: Deploy Your First Model with Disaggregated Serving\n",
    "\n",
    "### Objectives\n",
    "- Understand disaggregated serving architecture\n",
    "- Configure and deploy a model using vLLM backend with separate prefill and decode workers\n",
    "- Use Kubernetes manifests to deploy Dynamo resources\n",
    "\n",
    "### Available Backends\n",
    "In this lab, we'll use **vLLM** with disaggregated serving:\n",
    "- **vLLM**: High-throughput serving with PagedAttention\n",
    "- Model: `Qwen/Qwen2.5-1.5B-Instruct` (small, fast to download)\n",
    "- Architecture: Disaggregated serving with separate prefill and decode workers\n",
    "\n",
    "**Other backends** (for exploration):\n",
    "- **SGLang**: Optimized for complex prompting and structured generation\n",
    "- **TensorRT-LLM**: Maximum performance on NVIDIA GPUs\n",
    "\n",
    "### What is Disaggregated Serving?\n",
    "\n",
    "Disaggregated serving separates the inference pipeline into specialized workers:\n",
    "\n",
    "**Prefill Worker** (GPU 0):\n",
    "- Processes input prompts (compute-intensive)\n",
    "- Converts tokens into KV cache\n",
    "- Passes KV cache to decode workers\n",
    "\n",
    "**Decode Worker** (GPU 1):\n",
    "- Generates output tokens (memory-intensive)\n",
    "- Uses KV cache from prefill worker\n",
    "- Produces the final response\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ **Independent scaling**: Scale prefill and decode separately based on workload\n",
    "- ‚úÖ **Resource optimization**: Each worker optimized for its specific task\n",
    "- ‚úÖ **Better throughput**: Specialized workers can handle more requests\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Client Request\n",
    "    ‚Üì\n",
    "Frontend (Router)\n",
    "    ‚Üì\n",
    "Prefill Worker (GPU 0) ‚Üí processes prompt ‚Üí generates KV cache\n",
    "    ‚Üì\n",
    "Decode Worker (GPU 1) ‚Üí receives KV cache ‚Üí generates tokens\n",
    "    ‚Üì\n",
    "Response to Client\n",
    "```\n",
    "\n",
    "### Deployment Configuration\n",
    "\n",
    "We'll use a `DynamoGraphDeployment` resource that defines:\n",
    "- **Frontend**: OpenAI-compatible API endpoint with disaggregated routing\n",
    "- **VllmPrefillWorker**: 1 replica on GPU 0 for prompt processing\n",
    "- **VllmDecodeWorker**: 1 replica on GPU 1 for token generation\n",
    "\n",
    "### Step 1: Create Deployment Manifest\n",
    "\n",
    "We will create the `disagg_router.yaml` file dynamically with your specific configuration variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf66623",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create the deployment YAML with environment variables\n",
    "cat <<EOF > disagg_router.yaml\n",
    "apiVersion: nvidia.com/v1alpha1\n",
    "kind: DynamoGraphDeployment\n",
    "metadata:\n",
    "  name: vllm-disagg-router\n",
    "spec:\n",
    "  services:\n",
    "    Frontend:\n",
    "      dynamoNamespace: vllm-disagg-router\n",
    "      componentType: frontend\n",
    "      replicas: 1\n",
    "      extraPodSpec:\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:$RELEASE_VERSION\n",
    "      envs:\n",
    "        - name: DYN_ROUTER_MODE\n",
    "          value: disaggregated\n",
    "    VllmPrefillWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-disagg-router\n",
    "      componentType: worker\n",
    "      replicas: 1\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: \"info\"\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: $CACHE_PATH\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:$RELEASE_VERSION\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --worker-type prefill\n",
    "    VllmDecodeWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-disagg-router\n",
    "      componentType: worker\n",
    "      replicas: 1\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: \"info\"\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: $CACHE_PATH\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:$RELEASE_VERSION\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --worker-type decode\n",
    "EOF\n",
    "\n",
    "echo \"‚úì Deployment manifest created: disagg_router.yaml\"\n",
    "echo \"  Using Image Version: $RELEASE_VERSION\"\n",
    "echo \"  Using Cache Path:    $CACHE_PATH\"\n",
    "echo \"\"\n",
    "echo \"Verify the configuration:\"\n",
    "grep \"image:\" disagg_router.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5f89c",
   "metadata": {},
   "source": [
    "### Step 2: Deploy the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473375a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Apply the deployment\n",
    "kubectl apply -f disagg_router.yaml --namespace $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Deployment created. This will take 4-6 minutes for first run.\"\n",
    "echo \"  - Pulling container images\"\n",
    "echo \"  - Downloading model from HuggingFace\"\n",
    "echo \"  - Loading model into GPU memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beebf1e",
   "metadata": {},
   "source": [
    "### Step 2b: Expose Frontend Service via NodePort\n",
    "\n",
    "**CRITICAL**: By default, the deployment is internal-only. We must expose it via a NodePort Service to access it on port `30100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Create NodePort service to expose the frontend\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: vllm-frontend-nodeport\n",
    "  namespace: $NAMESPACE\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    nvidia.com/dynamo-component: Frontend\n",
    "    nvidia.com/dynamo-graph-deployment-name: vllm-disagg-router\n",
    "  ports:\n",
    "  - port: 8000\n",
    "    targetPort: 8000\n",
    "    nodePort: 30100\n",
    "    protocol: TCP\n",
    "    name: http\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Service exposed on NodePort 30100\"\n",
    "echo \"  Access URL: http://$NODE_IP:30100\"\n",
    "echo \"\"\n",
    "echo \"Note: The service will be accessible once the frontend pod is running.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a391f2f",
   "metadata": {},
   "source": [
    "### Step 3: Monitor Deployment Progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749a62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"Expected pods:\"\n",
    "echo \"  - vllm-disagg-router-frontend-xxxxx     (Frontend)\"\n",
    "echo \"  - vllm-disagg-router-vllmprefillworker-xxxxx (Prefill Worker on GPU 0)\"\n",
    "echo \"  - vllm-disagg-router-vllmdecodeworker-xxxxx  (Decode Worker on GPU 1)\"\n",
    "echo \"\"\n",
    "echo \"Waiting for deployment pods to be ready (this may take 4-6 minutes for first run)...\"\n",
    "echo \"\"\n",
    "\n",
    "# Wait for pods to be ready (timeout after 10 minutes for model download)\n",
    "TIMEOUT=600\n",
    "ELAPSED=0\n",
    "INTERVAL=10\n",
    "\n",
    "while [ $ELAPSED -lt $TIMEOUT ]; do\n",
    "    # Get vllm pod status\n",
    "    VLLM_PODS=$(kubectl get pods -n $NAMESPACE 2>/dev/null | grep vllm || true)\n",
    "    NOT_READY=$(echo \"$VLLM_PODS\" | grep -v \"1/1.*Running\" | grep -v \"^$\" | wc -l)\n",
    "    TOTAL=$(echo \"$VLLM_PODS\" | grep -v \"^$\" | wc -l)\n",
    "    READY=$((TOTAL - NOT_READY))\n",
    "    \n",
    "    echo \"[$ELAPSED s] VLLM Pods ready: $READY/$TOTAL\"\n",
    "    kubectl get pods -n $NAMESPACE | grep -E '(NAME|vllm)'\n",
    "    \n",
    "    # Check if all vllm pods are ready\n",
    "    if [ $NOT_READY -eq 0 ] && [ $TOTAL -ge 3 ]; then\n",
    "        echo \"\"\n",
    "        echo \"‚úì All deployment pods are ready!\"\n",
    "        echo \"\"\n",
    "        echo \"DynamoGraphDeployment status:\"\n",
    "        kubectl get dynamographdeployment -n $NAMESPACE\n",
    "        break\n",
    "    fi\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"Waiting for pods to be ready... (checking again in ${INTERVAL}s)\"\n",
    "    echo \"üí° Tip: Model download happens on first run and may take 3-5 minutes\"\n",
    "    echo \"\"\n",
    "    sleep $INTERVAL\n",
    "    ELAPSED=$((ELAPSED + INTERVAL))\n",
    "done\n",
    "\n",
    "if [ $ELAPSED -ge $TIMEOUT ]; then\n",
    "    echo \"‚ö†Ô∏è  Timeout waiting for pods to be ready\"\n",
    "    echo \"Check logs: kubectl logs -l component=VllmPrefillWorker -n $NAMESPACE --tail=50\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b29a8",
   "metadata": {},
   "source": [
    "### Step 4: View Worker Logs (Optional)\n",
    "\n",
    "While waiting for the deployment, you can watch the model loading progress in both workers.\n",
    "\n",
    "**Note**: In disaggregated serving, both the prefill and decode workers load the model separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04509b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Get logs from worker pods\n",
    "PREFILL_POD=$(kubectl get pods -n $NAMESPACE | grep vllmprefillworker | awk '{print $1}' | head -1)\n",
    "DECODE_POD=$(kubectl get pods -n $NAMESPACE | grep vllmdecodeworker | awk '{print $1}' | head -1)\n",
    "\n",
    "if [ -n \"$PREFILL_POD\" ]; then\n",
    "    echo \"=== Prefill Worker Logs (GPU 0): $PREFILL_POD ===\"\n",
    "    echo \"Look for:\"\n",
    "    echo \"  - 'Loading model weights...' (downloading)\"\n",
    "    echo \"  - 'Model loading took X.XX GiB' (loaded)\"\n",
    "    echo \"\"\n",
    "    kubectl logs $PREFILL_POD -n $NAMESPACE --tail=30\n",
    "    echo \"\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$DECODE_POD\" ]; then\n",
    "    echo \"=== Decode Worker Logs (GPU 1): $DECODE_POD ===\"\n",
    "    echo \"Look for:\"\n",
    "    echo \"  - 'Loading model weights...' (downloading)\"\n",
    "    echo \"  - 'Model loading took X.XX GiB' (loaded)\"\n",
    "    echo \"\"\n",
    "    kubectl logs $DECODE_POD -n $NAMESPACE --tail=30\n",
    "fi\n",
    "\n",
    "if [ -z \"$PREFILL_POD\" ] && [ -z \"$DECODE_POD\" ]; then\n",
    "    echo \"Worker pods not found yet, please wait and try again\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444e894",
   "metadata": {},
   "source": [
    "## Section 4: Testing and Validation\n",
    "\n",
    "### Objectives\n",
    "- Expose the service locally using port forwarding\n",
    "- Send test requests to the deployment\n",
    "- Verify OpenAI API compatibility\n",
    "- Test streaming and non-streaming responses\n",
    "\n",
    "### Testing Strategy\n",
    "Once your deployment is running (`1/1 Ready`), you'll:\n",
    "1. Connect to the frontend via NodePort (already exposed on port 30100)\n",
    "2. Test with curl commands\n",
    "3. Verify response format and functionality\n",
    "\n",
    "### Step 1: Get Frontend URL\n",
    "\n",
    "The frontend is already exposed via NodePort on port 30100 (set up in Step 2b). Get the connection URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86bc8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get the node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Node IP: $NODE_IP\"\n",
    "echo \"\"\n",
    "echo \"Frontend URL: http://$NODE_IP:30100\"\n",
    "echo \"\"\n",
    "echo \"‚úì Access the frontend at: http://$NODE_IP:30100\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167130af",
   "metadata": {},
   "source": [
    "### Step 2: Test the `/v1/models` Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Test the /v1/models endpoint\n",
    "curl http://$NODE_IP:30100/v1/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b89e78",
   "metadata": {},
   "source": [
    "### Step 3: Simple Non-Streaming Chat Completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb10c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Test non-streaming chat completion\n",
    "curl http://$NODE_IP:30100/v1/chat/completions \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello! How are you?\"}],\n",
    "    \"stream\": false,\n",
    "    \"max_tokens\": 50\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8468b5e",
   "metadata": {},
   "source": [
    "### Step 4: Test Streaming Response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fde5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Test streaming chat completion\n",
    "curl http://$NODE_IP:30100/v1/chat/completions \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem about AI\"}],\n",
    "    \"stream\": true,\n",
    "    \"max_tokens\": 100\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6229a6",
   "metadata": {},
   "source": [
    "### Step 5: Test with Different Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Test with different parameters\n",
    "curl http://$NODE_IP:30100/v1/chat/completions \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "    \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in one sentence\"}],\n",
    "    \"stream\": false,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100,\n",
    "    \"top_p\": 0.9\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01443a9d",
   "metadata": {},
   "source": [
    "## Section 5: Benchmarking with AI-Perf\n",
    "\n",
    "### Objectives\n",
    "- Install and configure AI-Perf benchmarking tool\n",
    "- Run performance benchmarks against your Kubernetes deployment\n",
    "- Analyze throughput, latency, and token metrics\n",
    "- Compare performance across different configurations\n",
    "\n",
    "### Metrics to Measure\n",
    "- Throughput (requests/second, tokens/second)\n",
    "- Latency (TTFT - Time To First Token, TPOT - Time Per Output Token, end-to-end)\n",
    "- GPU utilization\n",
    "- KV cache efficiency\n",
    "\n",
    "### Benchmarking Setup\n",
    "You'll run AI-Perf from your local machine against the port-forwarded service, simulating:\n",
    "- Different concurrency levels (fixed concurrent requests)\n",
    "- Request rate patterns (requests per second)\n",
    "- Various workload characteristics\n",
    "\n",
    "### Step 1: Install AI-Perf (if not already installed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d63ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Ensure pip is available in the venv\n",
    "print(\"Setting up pip in venv...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--default-pip\"], \n",
    "               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# Install AI-Perf in the venv\n",
    "print(\"Installing AI-Perf...\")\n",
    "result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"aiperf\", \"-q\"])\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úì AI-Perf installed successfully\")\n",
    "    # Verify aiperf can be imported\n",
    "    verify = subprocess.run([sys.executable, \"-c\", \"import aiperf\"], capture_output=True)\n",
    "    if verify.returncode == 0:\n",
    "        print(\"  aiperf is ready to use\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Installation had issues, but may still work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902a96a",
   "metadata": {},
   "source": [
    "### Step 2: Run Baseline Benchmark (Low Concurrency)\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Run benchmarks in a TERMINAL, not in notebook cells (aiperf can crash the kernel).**\n",
    "\n",
    "**To run this benchmark:**\n",
    "\n",
    "1. Open a new terminal (File ‚Üí New ‚Üí Terminal in JupyterLab)\n",
    "2. Copy and paste this command:\n",
    "\n",
    "```\n",
    "cd ~/dynamo-grove-brev/resources && ./run-benchmark.sh baseline\n",
    "```\n",
    "\n",
    "This will run a low concurrency benchmark (1 concurrent request, 100 total requests) and display metrics including:\n",
    "- Time to First Token (TTFT)\n",
    "- Token throughput\n",
    "- Request latency\n",
    "- Percentile distributions (p50, p90, p99)\n",
    "\n",
    "### Step 3: Run Benchmark with Higher Concurrency\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Run benchmarks in a TERMINAL, not in notebook cells.**\n",
    "\n",
    "**To run this benchmark:**\n",
    "\n",
    "1. Open a new terminal (File ‚Üí New ‚Üí Terminal in JupyterLab)\n",
    "2. Copy and paste this command:\n",
    "\n",
    "```\n",
    "cd ~/dynamo-grove-brev/resources && ./run-benchmark.sh high\n",
    "```\n",
    "\n",
    "This will run a high concurrency benchmark (4 concurrent requests, 200 total requests) to stress test the system and see how it handles multiple simultaneous users.\n",
    "\n",
    "### Step 4: Run Benchmark with Request Rate\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Run benchmarks in a TERMINAL, not in notebook cells.**\n",
    "\n",
    "**To run this benchmark:**\n",
    "\n",
    "1. Open a new terminal (File ‚Üí New ‚Üí Terminal in JupyterLab)\n",
    "2. Copy and paste this command:\n",
    "\n",
    "```\n",
    "cd ~/dynamo-grove-brev/resources && ./run-benchmark.sh rate\n",
    "```\n",
    "\n",
    "This will run a request rate benchmark (10 requests per second, 200 total requests) to simulate a steady stream of users hitting the API at a controlled rate.\n",
    "\n",
    "### Step 5: Analyze Results\n",
    "\n",
    "Review the benchmark outputs above. Key metrics to look for:\n",
    "- **Throughput**: requests/second and tokens/second\n",
    "- **TTFT (Time To First Token)**: How quickly does the first token appear?\n",
    "- **TPOT (Time Per Output Token)**: Generation speed\n",
    "- **End-to-end latency**: Total request time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37da319",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- Continue to **Lab 2: Monitoring and Observability** to add dashboards and metrics\n",
    "- Or skip to **Lab 3: Distributed Serving** for multi-GPU deployments\n",
    "- See **Appendix** for cleanup commands (only if completely done with all labs)\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Check Pod Status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bf829",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Check all pods in your namespace\n",
    "kubectl get pods -n $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"# To describe a specific pod to see errors:\"\n",
    "echo \"# kubectl describe pod <pod-name> -n $NAMESPACE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc592a",
   "metadata": {},
   "source": [
    "### View Pod Logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# View logs from a specific component\n",
    "echo \"Frontend logs:\"\n",
    "kubectl logs -l component=Frontend -n $NAMESPACE --tail=50\n",
    "\n",
    "echo \"\"\n",
    "echo \"Worker logs:\"\n",
    "kubectl logs -l component=VllmDecodeWorker -n $NAMESPACE --tail=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265277f",
   "metadata": {},
   "source": [
    "### Check Deployment Status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Check DynamoGraphDeployment status\n",
    "echo \"DynamoGraphDeployment status:\"\n",
    "kubectl describe dynamographdeployment vllm-disagg-router -n $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"Operator logs:\"\n",
    "kubectl logs -l app.kubernetes.io/name=dynamo-operator -n $NAMESPACE --tail=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde037f",
   "metadata": {},
   "source": [
    "### Check Recent Events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# View recent events in your namespace\n",
    "kubectl get events -n $NAMESPACE --sort-by=.lastTimestamp | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4ec48",
   "metadata": {},
   "source": [
    "### Common Issues\n",
    "\n",
    "1. **ImagePullBackOff**: Check if you have access to NGC containers. Verify image version is correct.\n",
    "2. **Pods stuck in Pending**: Check if GPU resources are available: `kubectl describe pod <pod-name> -n $NAMESPACE`\n",
    "3. **Model download slow**: First run takes longer due to model download. Check worker logs for progress.\n",
    "4. **Port forward not working**: Make sure pods are `1/1 Ready` before forwarding. Kill existing port-forward processes: `pkill -f port-forward`\n",
    "\n",
    "---\n",
    "\n",
    "## Known Issues (v0.8.0)\n",
    "\n",
    "**‚ö†Ô∏è Important Notes for Dynamo v0.8.0:**\n",
    "\n",
    "1. **Validation Webhook Timing**: In rare cases, validation webhooks may reject valid configurations during high cluster load. If deployment fails with validation errors, wait 30 seconds and retry.\n",
    "\n",
    "2. **K8s-native Discovery Cold Start**: First request after deployment may take 5-10s longer as EndpointSlices propagate. Subsequent requests are fast.\n",
    "\n",
    "3. **TCP Transport Port Conflicts**: If using custom ports, ensure they don't conflict with existing services. Default ports (8000, 8001) are usually safe.\n",
    "\n",
    "4. **Model Loading on Multiple GPUs**: For multi-GPU setups, ensure sufficient shared memory (`/dev/shm`) is available. Add `--shm-size=2g` to worker pod spec if needed.\n",
    "\n",
    "**Workarounds:**\n",
    "- For webhook issues: Add `--wait --timeout=5m` to helm installs\n",
    "- For discovery delays: Add readiness probe with longer `initialDelaySeconds`\n",
    "- Check [GitHub Issues](https://github.com/ai-dynamo/dynamo/issues) for latest updates\n",
    "\n",
    "**Fixed in v0.8.1+:** Many of these issues are addressed in patch releases. Check release notes for updates.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "- ‚úÖ How to set up a namespace-scoped Dynamo deployment on Kubernetes\n",
    "- ‚úÖ Kubernetes-based disaggregated deployment architecture\n",
    "- ‚úÖ Creating and managing DynamoGraphDeployment resources\n",
    "- ‚úÖ Backend engine deployment (vLLM)\n",
    "- ‚úÖ Testing with OpenAI-compatible API\n",
    "- ‚úÖ Performance benchmarking with AI-Perf\n",
    "\n",
    "### Key Takeaways\n",
    "- Namespace-scoped operators enable safe multi-tenant deployments\n",
    "- Disaggregated serving separates prefill and decode for optimized resource utilization\n",
    "- KV-cache routing provides intelligent load balancing across replicas\n",
    "- DynamoGraphDeployment CRD simplifies complex inference deployments\n",
    "- AI-Perf provides comprehensive performance insights\n",
    "\n",
    "### Next Steps\n",
    "- **(Optional)** Complete the **Monitoring Extension** (`lab1-monitoring.md`) to set up Prometheus and Grafana for observability\n",
    "- In **Lab 2**, you'll explore advanced optimizations and use AIConfigurator to optimize configurations for larger models\n",
    "\n",
    "---\n",
    "\n",
    "## Appendix: Step-by-Step Commands\n",
    "\n",
    "This appendix provides complete commands for each section. Use these as a reference during the lab.\n",
    "\n",
    "**Note for MicroK8s users:** Replace `kubectl` with `microk8s kubectl` in all commands below, or set up an alias:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2156b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "alias kubectl='microk8s kubectl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89acf3b",
   "metadata": {},
   "source": [
    "### A0. Troubleshooting Pod Issues\n",
    "\n",
    "If your pods are in `Error` or `CrashLoopBackOff` state, use this comprehensive diagnostic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"=== Pod Status ===\"\n",
    "kubectl get pods -n $NAMESPACE | grep vllm\n",
    "echo \"\"\n",
    "\n",
    "echo \"=== GPU Availability ===\"\n",
    "kubectl get nodes -o custom-columns=NAME:.metadata.name,GPUs:.status.capacity.nvidia\\\\.com/gpu,GPU-Allocatable:.status.allocatable.nvidia\\\\.com/gpu\n",
    "echo \"\"\n",
    "\n",
    "echo \"=== Checking Secrets ===\"\n",
    "kubectl get secret hf-token-secret -n $NAMESPACE &>/dev/null && echo \"‚úì HF token secret exists\" || echo \"‚úó HF token secret missing!\"\n",
    "kubectl get secret ngc-secret -n $NAMESPACE &>/dev/null && echo \"‚úì NGC secret exists\" || echo \"‚úó NGC secret missing!\"\n",
    "echo \"\"\n",
    "\n",
    "echo \"=== Prefill Worker Logs (last 30 lines) ===\"\n",
    "PREFILL_POD=$(kubectl get pods -n $NAMESPACE | grep vllmprefillworker | awk '{print $1}' | head -1)\n",
    "if [ -n \"$PREFILL_POD\" ]; then\n",
    "    kubectl logs $PREFILL_POD -n $NAMESPACE --tail=30\n",
    "else\n",
    "    echo \"No prefill pod found\"\n",
    "fi\n",
    "echo \"\"\n",
    "\n",
    "echo \"=== Decode Worker Logs (last 30 lines) ===\"\n",
    "DECODE_POD=$(kubectl get pods -n $NAMESPACE | grep vllmdecodeworker | awk '{print $1}' | head -1)\n",
    "if [ -n \"$DECODE_POD\" ]; then\n",
    "    kubectl logs $DECODE_POD -n $NAMESPACE --tail=30\n",
    "else\n",
    "    echo \"No decode pod found\"\n",
    "fi\n",
    "echo \"\"\n",
    "\n",
    "echo \"=== Common Issues ===\"\n",
    "echo \"1. If 'insufficient gpu' error: You need 2 GPUs for disaggregated serving\"\n",
    "echo \"2. If 'HF_TOKEN' error: Make sure you created the hf-token-secret\"\n",
    "echo \"3. If 'ImagePullBackOff': Check NGC secret and credentials\"\n",
    "echo \"4. If model download errors: Check network connectivity to huggingface.co\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198722a",
   "metadata": {},
   "source": [
    "### A1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Verify kubectl is installed and configured\n",
    "kubectl version --client\n",
    "kubectl cluster-info\n",
    "\n",
    "# Set your configuration\n",
    "export NAMESPACE=\"dynamo\"\n",
    "export RELEASE_VERSION=\"0.8.0\"     # Dynamo version\n",
    "export HF_TOKEN=\"your_hf_token\"    # Your HuggingFace token\n",
    "export CACHE_PATH=\"/data/huggingface-cache\"  # Shared cache path\n",
    "\n",
    "# Create your personal namespace\n",
    "kubectl create namespace ${NAMESPACE}\n",
    "\n",
    "# Verify namespace was created\n",
    "kubectl get namespace ${NAMESPACE}\n",
    "\n",
    "# Check GPU nodes are available (optional)\n",
    "kubectl get nodes -o custom-columns=NAME:.metadata.name,GPUs:.status.capacity.nvidia\\\\.com/gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd77e40",
   "metadata": {},
   "source": [
    "### A2. Install Dynamo Platform (Namespace-Scoped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41931f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Step 1: Check if CRDs are already installed (cluster-wide)\n",
    "if kubectl get crd dynamographdeployments.nvidia.com &>/dev/null && \\\n",
    "   kubectl get crd dynamocomponentdeployments.nvidia.com &>/dev/null; then\n",
    "    echo \"‚úì CRDs already installed\"\n",
    "else\n",
    "    echo \"‚ö†Ô∏è  CRDs not found. Ask instructor to install them, or run:\"\n",
    "    echo \"helm fetch https://helm.ngc.nvidia.com/nvidia/ai-dynamo/charts/dynamo-crds-${RELEASE_VERSION}.tgz\"\n",
    "    echo \"helm install dynamo-crds dynamo-crds-${RELEASE_VERSION}.tgz --namespace default\"\n",
    "fi\n",
    "\n",
    "# Step 2: Download Dynamo platform helm chart\n",
    "helm fetch https://helm.ngc.nvidia.com/nvidia/ai-dynamo/charts/dynamo-platform-${RELEASE_VERSION}.tgz\n",
    "\n",
    "# Step 3: Install namespace-scoped Dynamo platform\n",
    "# IMPORTANT: --set dynamo-operator.namespaceRestriction.enabled=true restricts operator to this namespace\n",
    "helm install dynamo-platform dynamo-platform-${RELEASE_VERSION}.tgz \\\n",
    "  --namespace ${NAMESPACE} \\\n",
    "  --set dynamo-operator.namespaceRestriction.enabled=true\n",
    "\n",
    "# Step 4: Wait for platform pods to be ready (~2-3 minutes)\n",
    "echo \"Waiting for platform pods to be ready...\"\n",
    "kubectl wait --for=condition=ready pod \\\n",
    "  --all \\\n",
    "  --namespace ${NAMESPACE} \\\n",
    "  --timeout=300s\n",
    "\n",
    "# Step 5: Verify platform is running\n",
    "kubectl get pods -n ${NAMESPACE}\n",
    "# You should see: dynamo-operator, etcd, and nats pods in Running state\n",
    "\n",
    "# Step 6: Create HuggingFace token secret\n",
    "kubectl create secret generic hf-token-secret \\\n",
    "  --from-literal=HF_TOKEN=\"${HF_TOKEN}\" \\\n",
    "  --namespace ${NAMESPACE}\n",
    "\n",
    "# Verify secret was created\n",
    "kubectl get secret hf-token-secret -n ${NAMESPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fee81",
   "metadata": {},
   "source": [
    "### A3. Deploy Your First Model\n",
    "\n",
    "Create a deployment YAML file `disagg_router.yaml`:\n",
    "\n",
    "```yaml\n",
    "# disagg_router.yaml\n",
    "apiVersion: nvidia.com/v1alpha1\n",
    "kind: DynamoGraphDeployment\n",
    "metadata:\n",
    "  name: vllm-disagg-router\n",
    "spec:\n",
    "  services:\n",
    "    Frontend:\n",
    "      dynamoNamespace: vllm-disagg-router\n",
    "      componentType: frontend\n",
    "      replicas: 1\n",
    "      extraPodSpec:\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:${RELEASE_VERSION}\n",
    "      envs:\n",
    "        - name: DYN_ROUTER_MODE\n",
    "          value: disaggregated\n",
    "    VllmPrefillWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-disagg-router\n",
    "      componentType: worker\n",
    "      replicas: 1\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: \"info\"\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: ${CACHE_PATH}  # Defaults to /data/huggingface-cache\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:${RELEASE_VERSION}\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --worker-type prefill\n",
    "    VllmDecodeWorker:\n",
    "      envFromSecret: hf-token-secret\n",
    "      dynamoNamespace: vllm-disagg-router\n",
    "      componentType: worker\n",
    "      replicas: 1\n",
    "      resources:\n",
    "        limits:\n",
    "          gpu: \"1\"\n",
    "      envs:\n",
    "        - name: DYN_LOG\n",
    "          value: \"info\"\n",
    "      extraPodSpec:\n",
    "        volumes:\n",
    "        - name: local-model-cache\n",
    "          hostPath:\n",
    "            path: ${CACHE_PATH}  # Defaults to /data/huggingface-cache\n",
    "            type: DirectoryOrCreate\n",
    "        mainContainer:\n",
    "          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:${RELEASE_VERSION}\n",
    "          securityContext:\n",
    "            capabilities:\n",
    "              add:\n",
    "                - IPC_LOCK\n",
    "          volumeMounts:\n",
    "          - name: local-model-cache\n",
    "            mountPath: /root/.cache\n",
    "          workingDir: /workspace/components/backends/vllm\n",
    "          command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "          args:\n",
    "            - python3 -m dynamo.vllm --model Qwen/Qwen2.5-1.5B-Instruct --worker-type decode\n",
    "```\n",
    "\n",
    "Deploy the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Apply the deployment\n",
    "kubectl apply -f disagg_router.yaml --namespace ${NAMESPACE}\n",
    "\n",
    "# Monitor deployment progress\n",
    "kubectl get dynamographdeployment -n ${NAMESPACE}\n",
    "\n",
    "# Watch pods starting up (this takes 4-6 minutes for first run)\n",
    "kubectl get pods -n ${NAMESPACE} -w\n",
    "# Press Ctrl+C to stop watching\n",
    "\n",
    "# Check specific pod status\n",
    "kubectl get pods -n ${NAMESPACE} | grep vllm\n",
    "\n",
    "# View worker logs to see model loading progress\n",
    "WORKER_POD=$(kubectl get pods -n ${NAMESPACE} | grep vllmdecodeworker | head -1 | awk '{print $1}')\n",
    "kubectl logs ${WORKER_POD} -n ${NAMESPACE} --tail=50 --follow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358e16e",
   "metadata": {},
   "source": [
    "### A4. Test the Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The frontend is exposed via NodePort on port 30100\n",
    "# Get the node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Frontend URL: http://$NODE_IP:30100\"\n",
    "echo \"\"\n",
    "echo \"Quick test commands (run in terminal):\"\n",
    "echo \"\"\n",
    "echo \"# Test 1: Check available models\"\n",
    "echo \"curl http://$NODE_IP:30100/v1/models\"\n",
    "echo \"\"\n",
    "echo \"# Test 2: Simple chat completion\"\n",
    "echo \"curl http://$NODE_IP:30100/v1/chat/completions -H 'Content-Type: application/json' -d '{\\\"model\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\", \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello!\\\"}], \\\"stream\\\": false, \\\"max_tokens\\\": 50}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0734cbdc",
   "metadata": {},
   "source": [
    "### A5. Benchmark with AI-Perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "FRONTEND_URL=\"http://$NODE_IP:30100\"\n",
    "\n",
    "echo \"Benchmarking frontend at: $FRONTEND_URL\"\n",
    "echo \"\"\n",
    "\n",
    "# Install AI-Perf (if not already installed)\n",
    "pip install aiperf -q\n",
    "\n",
    "echo \"=== Running benchmarks ===\"\n",
    "echo \"\"\n",
    "\n",
    "# Run a simple benchmark (adjust parameters as needed)\n",
    "echo \"1. Low concurrency benchmark...\"\n",
    "aiperf profile     --log-level warning     --model Qwen/Qwen2.5-1.5B-Instruct     --url $FRONTEND_URL     --endpoint-type chat     --streaming     --concurrency 1     --request-count 100\n",
    "\n",
    "# Run with higher concurrency\n",
    "echo \"\"\n",
    "echo \"2. High concurrency benchmark...\"\n",
    "aiperf profile     --log-level warning     --model Qwen/Qwen2.5-1.5B-Instruct     --url $FRONTEND_URL     --endpoint-type chat     --streaming     --concurrency 4     --request-count 200\n",
    "\n",
    "# Run with request rate\n",
    "echo \"\"\n",
    "echo \"3. Request rate benchmark...\"\n",
    "aiperf profile     --log-level warning     --model Qwen/Qwen2.5-1.5B-Instruct     --url $FRONTEND_URL     --endpoint-type chat     --streaming     --request-rate 10     --request-count 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3419e",
   "metadata": {},
   "source": [
    "### A6. Scale Your Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Edit your disagg_router.yaml and change replicas from 1 to 2\n",
    "# Then reapply:\n",
    "kubectl apply -f disagg_router.yaml --namespace ${NAMESPACE}\n",
    "\n",
    "# Watch the new worker come online\n",
    "kubectl get pods -n ${NAMESPACE} -w\n",
    "\n",
    "# Test that load is distributed (KV-cache routing should work)\n",
    "# Run multiple requests and check logs from both workers\n",
    "kubectl logs -l component=VllmDecodeWorker -n ${NAMESPACE} --tail=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf685b",
   "metadata": {},
   "source": [
    "### A7. Cleanup (‚ö†Ô∏è ONLY AFTER COMPLETING ALL LABS)\n",
    "\n",
    "**‚ö†Ô∏è WARNING: DO NOT RUN THIS DURING THE WORKSHOP**\n",
    "\n",
    "This cleanup is **ONLY** for when you're completely done with:\n",
    "- ‚úÖ Lab 1: Deployment\n",
    "- ‚úÖ Lab 2: Monitoring (requires Lab 1 deployment running)\n",
    "- ‚úÖ Lab 3: Distributed Serving (requires platform installed)\n",
    "\n",
    "**If you're continuing to Lab 2 or Lab 3, DO NOT run these commands.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfef2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Step 1: Delete the Lab 1 deployment (only after Lab 2 is done)\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "kubectl delete dynamographdeployment vllm-disagg-router -n ${NAMESPACE}\n",
    "kubectl delete svc vllm-frontend-nodeport -n ${NAMESPACE}\n",
    "\n",
    "# Step 2: Verify pods are terminating\n",
    "kubectl get pods -n ${NAMESPACE}\n",
    "\n",
    "# Step 3: (Optional) Complete cleanup after ALL labs\n",
    "# This removes everything including the platform:\n",
    "# kubectl delete namespace ${NAMESPACE}\n",
    "# helm uninstall dynamo-platform -n ${NAMESPACE}\n",
    "# helm uninstall dynamo-crds -n default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a3b51",
   "metadata": {},
   "source": [
    "### A8. Troubleshooting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf53ee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check pod status\n",
    "kubectl get pods -n ${NAMESPACE}\n",
    "\n",
    "# Describe a pod to see errors\n",
    "kubectl describe pod <pod-name> -n ${NAMESPACE}\n",
    "\n",
    "# View logs from a specific pod\n",
    "kubectl logs <pod-name> -n ${NAMESPACE}\n",
    "\n",
    "# Check DynamoGraphDeployment status\n",
    "kubectl describe dynamographdeployment vllm-disagg-router -n ${NAMESPACE}\n",
    "\n",
    "# Check operator logs\n",
    "kubectl logs -l app.kubernetes.io/name=dynamo-operator -n ${NAMESPACE}\n",
    "\n",
    "# Check if image pull is working\n",
    "kubectl get events -n ${NAMESPACE} --sort-by='.lastTimestamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576dad0",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-kernelspec,-widgets,-language_info"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
