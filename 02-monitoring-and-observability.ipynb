{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514f5168",
   "metadata": {},
   "source": [
    "# Lab 1 Extension: Monitoring Dynamo with Prometheus and Grafana\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this extension to Lab 1, you will:\n",
    "- Access the cluster-wide Grafana and Prometheus installation\n",
    "- Configure metrics collection from your Dynamo deployment\n",
    "- Create and view the Dynamo inference dashboard in Grafana\n",
    "- Add Planner observability dashboard to monitor request routing\n",
    "- Explore unified tracing with OpenTelemetry for debugging\n",
    "- Understand key performance metrics\n",
    "\n",
    "**Prerequisites**: Complete Lab 1 (Introduction and Kubernetes-Based Deployment)\n",
    "\n",
    "**Note**: Prometheus and Grafana were installed cluster-wide during the initial setup. You'll verify they're running and configure them to monitor your Dynamo deployment.\n",
    "\n",
    "## Duration: ~30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Verify Cluster Monitoring Stack\n",
    "\n",
    "### Objectives\n",
    "- Verify cluster-wide Grafana and Prometheus are running\n",
    "- Get access information for Grafana dashboard\n",
    "- Understand how cluster-wide monitoring works\n",
    "\n",
    "### Important: Cluster-Wide Monitoring\n",
    "\n",
    "The Kubernetes cluster has a **cluster-wide monitoring stack** already deployed during initial setup:\n",
    "- Prometheus collects metrics from all namespaces\n",
    "- Grafana provides visualization dashboards\n",
    "- Services are exposed via NodePort for easy access\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "Cluster (monitoring namespace):\n",
    "  ‚îú‚îÄ‚îÄ Prometheus (cluster-wide metrics collection)\n",
    "  ‚îú‚îÄ‚îÄ Grafana (cluster-wide dashboards)\n",
    "  ‚îî‚îÄ‚îÄ Prometheus Operator (manages monitoring resources)\n",
    "\n",
    "Your Namespace (dynamo):\n",
    "  ‚îú‚îÄ‚îÄ Dynamo Deployment (Frontend + Workers)\n",
    "  ‚îî‚îÄ‚îÄ PodMonitors (tell Prometheus what to scrape)\n",
    "```\n",
    "\n",
    "### Step 1: Set Environment Variables\n",
    "\n",
    "Set up the environment variables (same as Lab 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e52e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables (use defaults if not already set)\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Get Grafana URL (extract ID from hostname: brev-xxxxx -> grafana0-xxxxx)\n",
    "BREV_ID=$(hostname | cut -d'-' -f2)\n",
    "GRAFANA_URL=\"https://grafana0-${BREV_ID}.brevlab.com/\"\n",
    "\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n",
    "echo \"üìä Lab 1 Extension: Monitoring Environment Configuration\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\"\n",
    "echo \"  Release Version:  $RELEASE_VERSION\"\n",
    "echo \"  Namespace:        $NAMESPACE\"\n",
    "echo \"  Cache Path:       $CACHE_PATH\"\n",
    "echo \"  Node IP:          $NODE_IP\"\n",
    "echo \"\"\n",
    "echo \"üìå Service URLs:\"\n",
    "echo \"  Frontend API:     http://$NODE_IP:30100\"\n",
    "echo \"  Grafana:          $GRAFANA_URL\"\n",
    "echo \"\"\n",
    "echo \"üí° Grafana is configured with anonymous access (no login required)\"\n",
    "echo \"\"\n",
    "echo \"‚úì Environment configured for monitoring\"\n",
    "echo \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659b41d",
   "metadata": {},
   "source": [
    "### Step 2: Verify Lab 1 Deployment is Running\n",
    "\n",
    "**IMPORTANT:** Lab 2 requires the deployment from Lab 1 to be running. Let's verify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19704ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"Checking Lab 1 deployment...\"\n",
    "echo \"\"\n",
    "kubectl get dynamographdeployment -n $NAMESPACE\n",
    "echo \"\"\n",
    "kubectl get pods -n $NAMESPACE | grep vllm\n",
    "echo \"\"\n",
    "\n",
    "# Check if deployment exists\n",
    "if kubectl get dynamographdeployment vllm-disagg-router -n $NAMESPACE &>/dev/null; then\n",
    "    echo \"‚úì Lab 1 deployment found\"\n",
    "    \n",
    "    # Check if pods are ready\n",
    "    READY_PODS=$(kubectl get pods -n $NAMESPACE | grep vllm | grep \"1/1\" | wc -l)\n",
    "    if [ \"$READY_PODS\" -ge 2 ]; then\n",
    "        echo \"‚úì Deployment is healthy and ready to monitor\"\n",
    "    else\n",
    "        echo \"‚ö†Ô∏è  Some pods are not ready yet. Wait for them to reach 1/1 Running status.\"\n",
    "        echo \"   Re-run this cell to check status again.\"\n",
    "    fi\n",
    "else\n",
    "    echo \"‚ùå Lab 1 deployment not found!\"\n",
    "    echo \"\"\n",
    "    echo \"Please complete Lab 1 first:\"\n",
    "    echo \"  1. Go back to Lab 1\"\n",
    "    echo \"  2. Complete Section 3: Deploy Distributed Model\"\n",
    "    echo \"  3. Wait for pods to be ready (1/1 Running)\"\n",
    "    echo \"  4. Return to this lab\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c5ee9",
   "metadata": {},
   "source": [
    "### Step 3: Verify Monitoring Stack is Running\n",
    "\n",
    "Check that Prometheus and Grafana pods are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check monitoring stack pods\n",
    "echo \"Checking cluster monitoring stack...\"\n",
    "echo \"\"\n",
    "kubectl get pods -n monitoring | grep -E \"(NAME|prometheus-|grafana-)\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì If you see Running pods, the monitoring stack is ready\"\n",
    "echo \"\"\n",
    "echo \"üîó Access Grafana at: $GRAFANA_URL\"\n",
    "echo \"   (Anonymous access enabled - no login required)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ed2ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Configure Metrics Collection\n",
    "\n",
    "### Objectives\n",
    "- Understand PodMonitor resources\n",
    "- Configure automatic metrics discovery\n",
    "- Verify metrics are being scraped by cluster Prometheus\n",
    "\n",
    "### How Dynamo Exposes Metrics\n",
    "\n",
    "Dynamo components expose metrics through:\n",
    "- **Frontend**: Exposes `/metrics` on its HTTP port (8000)\n",
    "  - Request rates, latencies, token metrics\n",
    "- **Workers**: Exposes `/metrics` on system port\n",
    "  - Worker-specific metrics, queue stats\n",
    "\n",
    "**Note**: The cluster-wide Prometheus automatically discovers PodMonitors in all namespaces, so once we create them, metrics will be collected automatically.\n",
    "\n",
    "### Step 1: Verify Dynamo Deployment Has Metrics Labels\n",
    "\n",
    "The Dynamo operator automatically adds metrics labels to pods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check if your Dynamo pods have metrics labels\n",
    "echo \"Checking Dynamo pod labels:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/metrics-enabled=true --show-labels\n",
    "\n",
    "echo \"\"\n",
    "echo \"Look for labels: nvidia.com/metrics-enabled=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ab488",
   "metadata": {},
   "source": [
    "### Step 2: Verify PodMonitors for Prometheus\n",
    "\n",
    "PodMonitors tell Prometheus which pods to scrape for metrics. The Dynamo operator creates them automatically, but they need a label for cluster Prometheus to discover them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f55aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"Checking PodMonitors...\"\n",
    "kubectl get podmonitor -n $NAMESPACE\n",
    "echo \"\"\n",
    "\n",
    "# Check if PodMonitors exist\n",
    "PODMONITOR_COUNT=$(kubectl get podmonitor -n $NAMESPACE 2>/dev/null | grep -c dynamo || echo \"0\")\n",
    "\n",
    "if [ \"$PODMONITOR_COUNT\" -gt 0 ]; then\n",
    "    # Show configuration for one PodMonitor\n",
    "    echo \"PodMonitor configuration (example: dynamo-frontend):\"\n",
    "    kubectl get podmonitor dynamo-frontend -n $NAMESPACE -o jsonpath='{.spec}' | python3 -m json.tool\n",
    "    echo \"\"\n",
    "    \n",
    "    # Ensure they have the required label for Prometheus discovery\n",
    "    echo \"Labeling for Prometheus discovery...\"\n",
    "    kubectl label podmonitor -n $NAMESPACE dynamo-frontend release=kube-prometheus-stack --overwrite 2>/dev/null || true\n",
    "    kubectl label podmonitor -n $NAMESPACE dynamo-planner release=kube-prometheus-stack --overwrite 2>/dev/null || true\n",
    "    kubectl label podmonitor -n $NAMESPACE dynamo-worker release=kube-prometheus-stack --overwrite 2>/dev/null || true\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"‚úì PodMonitors ready - Prometheus will scrape metrics within 1-2 minutes\"\n",
    "else\n",
    "    echo \"‚ö†Ô∏è  PodMonitors not found - deployment may still be starting\"\n",
    "    echo \"   Wait 30 seconds and re-run this cell\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ae6a5",
   "metadata": {},
   "source": [
    "### Step 3: Test Metrics Endpoint \n",
    "\n",
    "Let's verify metrics are accessible:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get the frontend pod name\n",
    "FRONTEND_POD=$(kubectl get pods -n $NAMESPACE | grep frontend | head -1 | awk '{print $1}')\n",
    "\n",
    "if [ -n \"$FRONTEND_POD\" ]; then\n",
    "    echo \"Testing metrics endpoint from frontend pod: $FRONTEND_POD\"\n",
    "    echo \"\"\n",
    "    kubectl exec -n $NAMESPACE $FRONTEND_POD -- curl -s localhost:8000/metrics | head -20\n",
    "    echo \"\"\n",
    "    echo \"‚úì Metrics endpoint is accessible\"\n",
    "else\n",
    "    echo \"‚ö†Ô∏è  Frontend pod not found. Make sure your deployment from Lab 1 is running.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb1ee08",
   "metadata": {},
   "source": [
    "### Step 4: Send Test Traffic to Generate Metrics\n",
    "\n",
    "Let's generate some traffic to populate metrics by sending requests to the Dynamo frontend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Sending test requests to http://$NODE_IP:30100...\"\n",
    "echo \"\"\n",
    "\n",
    "# Send a few test requests\n",
    "for i in {1..5}; do\n",
    "    echo \"Request $i/5...\"\n",
    "    curl -s http://$NODE_IP:30100/v1/chat/completions \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d '{\n",
    "        \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello! Tell me a short joke.\"}],\n",
    "        \"stream\": false,\n",
    "        \"max_tokens\": 30\n",
    "      }' > /dev/null\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Sent 5 test requests to generate metrics\"\n",
    "echo \"  Metrics should now be visible in Prometheus and Grafana\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf689e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Import Dynamo Inference Dashboard\n",
    "\n",
    "### Objectives\n",
    "- Import the Dynamo Inference dashboard to Grafana\n",
    "- Understand what metrics are displayed\n",
    "\n",
    "### Dashboard Overview\n",
    "\n",
    "The cluster's Grafana has a \"Dynamo Operator\" dashboard pre-installed, but it shows **operator metrics** (reconciliation loops, workqueues). For **inference metrics** (request rates, latency, tokens), we need to import a custom dashboard.\n",
    "\n",
    "The Dynamo Inference dashboard provides visibility into:\n",
    "- **Request Metrics**: Request rates, throughput, and counts\n",
    "- **Latency Metrics**: Time to first token (TTFT), inter-token latency\n",
    "- **Performance**: Request duration, inflight requests\n",
    "- **Model Metrics**: Input/output sequence lengths, token counts\n",
    "\n",
    "### Import the Inference Dashboard\n",
    "\n",
    "Deploy the dashboard as a ConfigMap that Grafana will automatically load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create ConfigMap with dashboard JSON\n",
    "echo \"Deploying Dynamo Inference Dashboard to $NAMESPACE...\"\n",
    "\n",
    "cat > /tmp/dynamo-inference-dashboard-configmap.yaml << EOF\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: grafana-dashboard-dynamo-inference\n",
    "  namespace: $NAMESPACE\n",
    "  labels:\n",
    "    grafana_dashboard: \"1\"\n",
    "data:\n",
    "  dynamo-inference.json: |\n",
    "EOF\n",
    "\n",
    "# Add dashboard JSON with proper indentation\n",
    "sed 's/^/    /' ~/dynamo-brev/resources/dynamo-inference-dashboard.json >> /tmp/dynamo-inference-dashboard-configmap.yaml\n",
    "\n",
    "# Apply ConfigMap\n",
    "kubectl apply -f /tmp/dynamo-inference-dashboard-configmap.yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Dashboard ConfigMap deployed to namespace: $NAMESPACE\"\n",
    "echo \"  Cluster-wide Grafana sidecar will auto-discover it within ~30 seconds\"\n",
    "echo \"  Access at: $GRAFANA_URL (look for 'Dynamo Inference Metrics' dashboard)\"\n",
    "echo \"\"\n",
    "echo \"Note: The ConfigMap is created in your namespace ($NAMESPACE), but the\"\n",
    "echo \"      cluster-wide Grafana searches all namespaces for dashboards.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7930517",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Access Grafana and View Metrics\n",
    "\n",
    "### Objectives\n",
    "- Access Grafana UI via Brev tunnel\n",
    "- Import the Dynamo dashboard\n",
    "- Query metrics in Prometheus\n",
    "- View Dynamo metrics in Grafana\n",
    "\n",
    "### Step 1: View Dynamo Inference Dashboard\n",
    "\n",
    "Once you've imported the dashboard (from Section 3):\n",
    "\n",
    "1. **Click on \"Dashboards\"** in the left sidebar\n",
    "2. **Search for \"Dynamo Inference\"** or look in the \"General\" folder\n",
    "3. **Open the dashboard**\n",
    "\n",
    "The dashboard displays:\n",
    "- **Request Rate**: Requests per second by model\n",
    "- **Time to First Token (TTFT)**: p50, p95, p99 percentiles\n",
    "- **Inter-Token Latency**: Token generation speed\n",
    "- **Request Duration**: Total time per request\n",
    "- **Token Metrics**: Input/output sequence lengths\n",
    "- **Inflight Requests**: Currently processing requests\n",
    "\n",
    "**Note**: The Grafana also has a \"Dynamo Operator\" dashboard showing operator metrics (reconciliation loops, workqueues), but the inference dashboard shows model serving metrics.\n",
    "\n",
    "### Step 2: Generate Load to See Metrics\n",
    "\n",
    "To see interesting metrics in the dashboard, generate some load using the benchmark script from Lab 1.\n",
    "\n",
    "**Run this in a terminal (not in the notebook):**\n",
    "\n",
    "```\n",
    "cd ~/dynamo-grove-brev/lab1\n",
    "./run-benchmark.sh baseline\n",
    "```\n",
    "\n",
    "Or send a few test requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7baed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Send test requests\n",
    "for i in {1..10}; do\n",
    "    echo \"Request $i/10...\"\n",
    "    curl -s http://$NODE_IP:30100/v1/chat/completions \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d '{\n",
    "        \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "        \"stream\": false,\n",
    "        \"max_tokens\": 30\n",
    "      }' > /dev/null\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Sent 10 test requests - check Grafana dashboard for updated metrics!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fad46c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Planner Observability Dashboard (New in v0.8.0)\n",
    "\n",
    "### Objectives\n",
    "- Understand the Planner component in Dynamo's architecture\n",
    "- Import and configure the Planner observability dashboard\n",
    "- Monitor request routing and KV cache decisions\n",
    "\n",
    "### What is the Planner?\n",
    "\n",
    "The **Planner** is Dynamo's intelligent request router that makes critical decisions:\n",
    "- Which worker should handle each request (prefill vs. decode)\n",
    "- KV cache placement and reuse strategies\n",
    "- Load balancing across workers\n",
    "- Request batching decisions\n",
    "\n",
    "In v0.8.0, the Planner has enhanced observability with dedicated metrics for production debugging.\n",
    "\n",
    "### Import Planner Dashboard\n",
    "\n",
    "The Planner dashboard helps you understand:\n",
    "- Request routing decisions\n",
    "- KV cache hit/miss rates\n",
    "- Worker selection logic\n",
    "- Queue depths and backpressure\n",
    "\n",
    "**Note:** This dashboard is available starting with Dynamo v0.8.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550aaa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NAMESPACE=${NAMESPACE:-dynamo}\n",
    "GRAFANA_URL=\"http://$(hostname -I | awk '{print $1}'):30300\"\n",
    "\n",
    "# Create Planner dashboard ConfigMap\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: grafana-dashboard-dynamo-planner\n",
    "  namespace: $NAMESPACE\n",
    "  labels:\n",
    "    grafana_dashboard: \"1\"\n",
    "data:\n",
    "  dynamo-planner-dashboard.json: |\n",
    "    {\n",
    "      \"dashboard\": {\n",
    "        \"title\": \"Dynamo Planner Observability\",\n",
    "        \"tags\": [\"dynamo\", \"planner\", \"v0.8.0\"],\n",
    "        \"timezone\": \"browser\",\n",
    "        \"panels\": [\n",
    "          {\n",
    "            \"title\": \"Request Routing Decisions\",\n",
    "            \"targets\": [\n",
    "              {\n",
    "                \"expr\": \"rate(dynamo_planner_routing_decisions_total[5m])\",\n",
    "                \"legendFormat\": \"{{decision_type}}\"\n",
    "              }\n",
    "            ],\n",
    "            \"type\": \"graph\"\n",
    "          },\n",
    "          {\n",
    "            \"title\": \"KV Cache Hit Rate\",\n",
    "            \"targets\": [\n",
    "              {\n",
    "                \"expr\": \"rate(dynamo_planner_kv_cache_hits_total[5m]) / rate(dynamo_planner_kv_cache_lookups_total[5m])\",\n",
    "                \"legendFormat\": \"Cache Hit Rate\"\n",
    "              }\n",
    "            ],\n",
    "            \"type\": \"graph\"\n",
    "          },\n",
    "          {\n",
    "            \"title\": \"Worker Queue Depths\",\n",
    "            \"targets\": [\n",
    "              {\n",
    "                \"expr\": \"dynamo_planner_worker_queue_depth\",\n",
    "                \"legendFormat\": \"{{worker_id}}\"\n",
    "              }\n",
    "            ],\n",
    "            \"type\": \"graph\"\n",
    "          },\n",
    "          {\n",
    "            \"title\": \"Planning Latency (p95)\",\n",
    "            \"targets\": [\n",
    "              {\n",
    "                \"expr\": \"histogram_quantile(0.95, rate(dynamo_planner_decision_duration_seconds_bucket[5m]))\",\n",
    "                \"legendFormat\": \"p95 Planning Latency\"\n",
    "              }\n",
    "            ],\n",
    "            \"type\": \"graph\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "EOF\n",
    "\n",
    "echo \"‚úì Planner dashboard ConfigMap created\"\n",
    "echo \"  Dashboard will auto-import to Grafana\"\n",
    "echo \"  View at: $GRAFANA_URL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c6b931",
   "metadata": {},
   "source": [
    "### Access Planner Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7788c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"Grafana URL: $GRAFANA_URL\"\n",
    "echo \"\"\n",
    "echo \"Navigate to: Dashboards ‚Üí Dynamo Planner Observability\"\n",
    "echo \"\"\n",
    "echo \"(Anonymous access enabled - no login required)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3282da",
   "metadata": {},
   "source": [
    "### Key Planner Metrics\n",
    "\n",
    "| Metric | Description | What to Watch |\n",
    "|--------|-------------|---------------|\n",
    "| `dynamo_planner_routing_decisions_total` | Routing decisions made | Decision type distribution |\n",
    "| `dynamo_planner_kv_cache_hits_total` | KV cache hits | High hit rate = efficient reuse |\n",
    "| `dynamo_planner_kv_cache_lookups_total` | Total cache lookups | Cache utilization |\n",
    "| `dynamo_planner_worker_queue_depth` | Requests queued per worker | Backpressure indicators |\n",
    "| `dynamo_planner_decision_duration_seconds` | Time to make routing decision | Planning overhead |\n",
    "\n",
    "**Optimization Tip:** High KV cache hit rates (>70%) indicate effective prompt caching and can significantly reduce latency and costs.\n",
    "\n",
    "---\n",
    "\n",
    "## Section 6: Unified Tracing with OpenTelemetry (New in v0.8.0)\n",
    "\n",
    "### Objectives\n",
    "- Understand distributed tracing in Dynamo\n",
    "- Enable OpenTelemetry tracing\n",
    "- Visualize end-to-end request flows\n",
    "\n",
    "### What is Unified Tracing?\n",
    "\n",
    "Dynamo v0.8.0 introduces **OpenTelemetry-based distributed tracing** that tracks requests across:\n",
    "- Frontend API layer\n",
    "- Planner routing decisions\n",
    "- Prefill worker execution\n",
    "- KV cache transfers\n",
    "- Decode worker execution\n",
    "\n",
    "This gives you **end-to-end visibility** into where time is spent in complex requests.\n",
    "\n",
    "### Enable Tracing (Optional)\n",
    "\n",
    "**Note:** This requires a tracing backend like Jaeger or Tempo. For this lab, we'll show the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0551f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Example: Enable OpenTelemetry tracing in Dynamo deployment\n",
    "# Add these annotations to your DynamoGraphDeployment:\n",
    "\n",
    "cat <<EOF\n",
    "spec:\n",
    "  frontend:\n",
    "    annotations:\n",
    "      opentelemetry.io/enabled: \"true\"\n",
    "      opentelemetry.io/exporter: \"otlp\"\n",
    "      opentelemetry.io/endpoint: \"http://jaeger-collector:4317\"\n",
    "  workers:\n",
    "    annotations:\n",
    "      opentelemetry.io/enabled: \"true\"\n",
    "EOF\n",
    "\n",
    "echo \"Note: Tracing requires a backend like Jaeger or Tempo\"\n",
    "echo \"For production deployments, integrate with your observability stack\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe144c",
   "metadata": {},
   "source": [
    "### What Tracing Shows You\n",
    "\n",
    "With tracing enabled, you can see:\n",
    "\n",
    "1. **Request Flow Timeline:**\n",
    "   - Frontend receives request: 0ms\n",
    "   - Planner makes routing decision: 2ms\n",
    "   - Prefill worker starts: 5ms\n",
    "   - KV cache transfer: 150ms\n",
    "   - Decode worker generates: 300ms\n",
    "   - Response returned: 800ms\n",
    "\n",
    "2. **Bottleneck Identification:**\n",
    "   - Slow prefill? ‚Üí Model loading issue\n",
    "   - Slow KV transfer? ‚Üí Network/NIXL issue\n",
    "   - Slow decode? ‚Üí Batch size or GPU utilization\n",
    "\n",
    "3. **Cache Effectiveness:**\n",
    "   - Trace shows \"KV cache hit\" span = prompt was cached\n",
    "   - No cache hit = full prefill required\n",
    "\n",
    "**Production Tip:** Combine tracing with metrics for powerful debugging. Use metrics for aggregate patterns, traces for individual request debugging.\n",
    "\n",
    "---\n",
    "\n",
    "## Section 7: Understanding Key Metrics\n",
    "\n",
    "### Frontend Metrics\n",
    "\n",
    "The Dynamo frontend exposes these key metrics:\n",
    "\n",
    "| Metric | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| `dynamo_frontend_requests_total` | Total number of requests | Track request volume |\n",
    "| `dynamo_frontend_time_to_first_token_seconds` | Time until first token appears | User experience, responsiveness |\n",
    "| `dynamo_frontend_inter_token_latency_seconds` | Time between consecutive tokens | Generation speed, smoothness |\n",
    "| `dynamo_frontend_request_duration_seconds` | Total request duration | Overall latency |\n",
    "| `dynamo_frontend_input_tokens_total` | Input tokens processed | Input size distribution |\n",
    "| `dynamo_frontend_output_tokens_total` | Output tokens generated | Output size, throughput |\n",
    "\n",
    "### Worker Metrics\n",
    "\n",
    "Workers expose additional metrics:\n",
    "\n",
    "| Metric | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| `dynamo_worker_queue_size` | Requests waiting in queue | Identify backpressure |\n",
    "| `dynamo_worker_active_requests` | Currently processing requests | Worker utilization |\n",
    "| `dynamo_worker_kv_cache_usage` | KV cache memory usage | Memory optimization |\n",
    "\n",
    "### Exploring Metrics in Prometheus\n",
    "\n",
    "### Exploring Advanced Queries\n",
    "\n",
    "You can run advanced Prometheus queries directly in Grafana's Explore view:\n",
    "\n",
    "1. **Open Grafana** at `$GRAFANA_URL`\n",
    "2. **Click \"Explore\"** in the left sidebar (compass icon)\n",
    "3. **Select \"Prometheus\"** as the data source\n",
    "4. **Enter queries** in the query editor\n",
    "\n",
    "Try these advanced queries:\n",
    "\n",
    "**Total Requests:**\n",
    "```\n",
    "sum(dynamo_frontend_requests_total)\n",
    "```\n",
    "\n",
    "**Average Request Rate (last 5 minutes):**\n",
    "```\n",
    "avg(rate(dynamo_frontend_requests_total[5m]))\n",
    "```\n",
    "\n",
    "**95th Percentile TTFT over time:**\n",
    "```\n",
    "histogram_quantile(0.95, rate(dynamo_frontend_time_to_first_token_seconds_bucket[5m]))\n",
    "```\n",
    "\n",
    "**Tokens per second:**\n",
    "```\n",
    "rate(dynamo_frontend_output_sequence_tokens_sum[5m]) / rate(dynamo_frontend_output_sequence_tokens_count[5m])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Section 8: Exercises and Exploration\n",
    "\n",
    "### Exercise 1: Correlate Load with Latency\n",
    "\n",
    "1. Run different concurrency levels with aiperf\n",
    "2. Observe how TTFT and ITL change in Grafana\n",
    "3. Find the optimal concurrency for your deployment\n",
    "\n",
    "**Run these commands in a terminal (not in the notebook):**\n",
    "\n",
    "```\n",
    "# Test with low concurrency\n",
    "cd ~/dynamo-brev/resources\n",
    "./run-benchmark.sh baseline\n",
    "\n",
    "# Check Grafana - note the TTFT values\n",
    "# Then test with higher concurrency:\n",
    "\n",
    "# Get NODE_IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Test with high concurrency\n",
    "python3 -m aiperf profile \\\n",
    "  --model Qwen/Qwen2.5-1.5B-Instruct \\\n",
    "  --url http://$NODE_IP:30100 \\\n",
    "  --endpoint-type chat \\\n",
    "  --streaming \\\n",
    "  --concurrency 8 \\\n",
    "  --request-count 30\n",
    "\n",
    "# Compare TTFT between low and high concurrency in Grafana\n",
    "```\n",
    "\n",
    "### Exercise 2: Create Custom Prometheus Queries\n",
    "\n",
    "Try creating your own queries:\n",
    "\n",
    "1. **Average TTFT over time:**\n",
    "   ```\n",
    "   avg(rate(dynamo_frontend_time_to_first_token_seconds_sum[1m]))\n",
    "   ```\n",
    "\n",
    "2. **Request success rate:**\n",
    "   ```\n",
    "   rate(dynamo_frontend_requests_total{status=\"success\"}[1m])\n",
    "   ```\n",
    "\n",
    "3. **Tokens per second:**\n",
    "   ```\n",
    "   rate(dynamo_frontend_output_tokens_total[1m])\n",
    "   ```\n",
    "\n",
    "### Exercise 3: Set Up Alerts (Optional)\n",
    "\n",
    "Create a PrometheusRule for high latency alerts. Here's an example configuration:\n",
    "\n",
    "```yaml\n",
    "# Example: high-latency-alert.yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PrometheusRule\n",
    "metadata:\n",
    "  name: dynamo-alerts\n",
    "  namespace: dynamo\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  groups:\n",
    "  - name: dynamo\n",
    "    interval: 30s\n",
    "    rules:\n",
    "    - alert: HighTimeToFirstToken\n",
    "      expr: histogram_quantile(0.95, rate(dynamo_frontend_time_to_first_token_seconds_bucket[5m])) > 1.0\n",
    "      for: 2m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: \"High Time to First Token\"\n",
    "        description: \"95th percentile TTFT is above 1 second\"\n",
    "```\n",
    "\n",
    "To create and apply this alert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create the alert file\n",
    "cat > /tmp/high-latency-alert.yaml << EOF\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PrometheusRule\n",
    "metadata:\n",
    "  name: dynamo-alerts\n",
    "  namespace: $NAMESPACE\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  groups:\n",
    "  - name: dynamo\n",
    "    interval: 30s\n",
    "    rules:\n",
    "    - alert: HighTimeToFirstToken\n",
    "      expr: histogram_quantile(0.95, rate(dynamo_frontend_time_to_first_token_seconds_bucket[5m])) > 1.0\n",
    "      for: 2m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: \"High Time to First Token\"\n",
    "        description: \"95th percentile TTFT is above 1 second\"\n",
    "EOF\n",
    "\n",
    "# Apply the alert\n",
    "kubectl apply -f /tmp/high-latency-alert.yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Alert rule created\"\n",
    "echo \"  View alerts in Grafana: Alerting section\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812d06a",
   "metadata": {},
   "source": [
    "### Exercise 4: Cleanup Your Monitoring Resources\n",
    "\n",
    "When you're done exploring, you can remove the monitoring resources you created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efffaae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove PodMonitors and Dashboard ConfigMap from your namespace\n",
    "echo \"Cleaning up monitoring resources from namespace: $NAMESPACE...\"\n",
    "\n",
    "kubectl delete podmonitor -n $NAMESPACE --all\n",
    "kubectl delete configmap grafana-dashboard-dynamo-inference -n $NAMESPACE 2>/dev/null || true\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Monitoring resources removed from your namespace\"\n",
    "echo \"\"\n",
    "echo \"Note: The cluster-wide Prometheus and Grafana remain active.\"\n",
    "echo \"      Only your PodMonitors and dashboard ConfigMap were removed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ebc940",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Known Issues (v0.8.0 Observability)\n",
    "\n",
    "**‚ö†Ô∏è Monitoring-Specific Issues:**\n",
    "\n",
    "1. **Planner Dashboard Initial Load**: The Planner dashboard (new in v0.8.0) may show \"No data\" for the first 2-3 minutes after deployment. Metrics start appearing once requests flow through the system.\n",
    "\n",
    "2. **OpenTelemetry Trace Sampling**: Default trace sampling is 10% to reduce overhead. For debugging, increase sampling rate in deployment annotations: `opentelemetry.io/sample-rate: \"1.0\"`.\n",
    "\n",
    "3. **Grafana Dashboard Auto-Import Delay**: ConfigMaps with `grafana_dashboard: \"1\"` label may take 30-60 seconds to appear in Grafana. Refresh the dashboards list if not immediately visible.\n",
    "\n",
    "4. **PodMonitor Label Selectors**: Ensure your PodMonitor `matchLabels` exactly match the pod labels. Case-sensitive. Use `kubectl describe podmonitor` to debug.\n",
    "\n",
    "**Workarounds:**\n",
    "- For missing metrics: Check PodMonitor status: `kubectl get podmonitors -n $NAMESPACE -o yaml`\n",
    "- For tracing issues: Verify OTLP endpoint is reachable: `kubectl logs <pod> | grep -i otel`\n",
    "- Dashboard not loading: Manually import JSON from ConfigMap\n",
    "\n",
    "**New in v0.8.0:** Unified tracing and Planner observability are production-ready but may have edge cases. Report issues to the team.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "- ‚úÖ How to access cluster-wide Prometheus and Grafana\n",
    "- ‚úÖ Understanding Prometheus Operator and PodMonitors\n",
    "- ‚úÖ Configuring automatic metrics collection from Dynamo\n",
    "- ‚úÖ Creating and deploying Grafana dashboards via ConfigMaps\n",
    "- ‚úÖ Key Dynamo performance metrics\n",
    "- ‚úÖ Using Prometheus queries for analysis\n",
    "- ‚úÖ Correlating load with performance metrics\n",
    "\n",
    "### Key Takeaways\n",
    "- **Cluster-wide monitoring** enables shared observability infrastructure\n",
    "- **PodMonitors** automatically discover and scrape Dynamo metrics\n",
    "- **Prometheus** provides powerful query language for metric analysis\n",
    "- **Grafana** offers rich visualizations for real-time monitoring\n",
    "- **Key metrics** like TTFT and ITL are critical for LLM performance\n",
    "- **Dashboard ConfigMaps** with `grafana_dashboard: \"1\"` label are auto-discovered by Grafana sidecar\n",
    "\n",
    "### Next Steps\n",
    "- In **Lab 2**, you'll explore disaggregated serving and monitor the separate prefill/decode workers\n",
    "- Advanced monitoring: Set up alerting rules and long-term metric storage\n",
    "- Integrate with your CI/CD: Automated performance regression testing\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Prometheus Not Scraping Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check Prometheus targets\n",
    "echo \"Checking if Prometheus is scraping Dynamo pods...\"\n",
    "echo \"\"\n",
    "echo \"You can also view targets in Grafana:\"\n",
    "echo \"  1. Go to $GRAFANA_URL\"\n",
    "echo \"  2. Navigate to Status > Targets (in Prometheus section)\"\n",
    "echo \"\"\n",
    "echo \"Look for Dynamo pods in the targets list\"\n",
    "echo \"If pods are missing, check PodMonitor configuration:\"\n",
    "kubectl get podmonitor -n $NAMESPACE -o yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e1ad87",
   "metadata": {},
   "source": [
    "### Grafana Dashboard Not Appearing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a324fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check if dashboard ConfigMap has correct labels\n",
    "kubectl get configmap -n $NAMESPACE grafana-dashboard-dynamo-inference -o yaml | grep -A 5 labels\n",
    "\n",
    "echo \"\"\n",
    "echo \"The ConfigMap should have label: grafana_dashboard: '1'\"\n",
    "echo \"\"\n",
    "echo \"If the ConfigMap exists but dashboard doesn't appear:\"\n",
    "echo \"  1. Wait 30-60 seconds for Grafana sidecar to scan\"\n",
    "echo \"  2. Check Grafana logs for any import errors\"\n",
    "echo \"  3. Verify cluster-wide Grafana has sidecar.dashboards.enabled=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41420aa0",
   "metadata": {},
   "source": [
    "### Can't Access Grafana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check Grafana pod status\n",
    "kubectl get pods -n $NAMESPACE | grep grafana\n",
    "\n",
    "# Check Grafana logs\n",
    "GRAFANA_POD=$(kubectl get pods -n $NAMESPACE | grep grafana | awk '{print $1}')\n",
    "kubectl logs -n $NAMESPACE $GRAFANA_POD --tail=30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75d3ca",
   "metadata": {},
   "source": [
    "### Port Forwards Not Working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e223f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Kill all existing port-forwards and restart\n",
    "pkill -f 'kubectl port-forward' || true\n",
    "\n",
    "echo \"‚úì Killed existing port-forwards\"\n",
    "echo \"\"\n",
    "echo \"Re-run the port-forward commands from Section 4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440cffa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- üìñ [Dynamo Metrics Documentation](../../dynamo/docs/observability/metrics.md)\n",
    "- üìä [Prometheus Query Examples](https://prometheus.io/docs/prometheus/latest/querying/examples/)\n",
    "- üé® [Grafana Dashboard Best Practices](https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/best-practices/)\n",
    "- üîî [Prometheus Alerting](https://prometheus.io/docs/alerting/latest/overview/)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-kernelspec,-widgets,-language_info"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
