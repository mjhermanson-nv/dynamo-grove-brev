{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3d2fc2",
   "metadata": {},
   "source": [
    "# Lab 1 Extension: Monitoring Dynamo with Prometheus and Grafana\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this extension to Lab 1, you will:\n",
    "- Access the cluster-wide Grafana and Prometheus installation\n",
    "- Configure metrics collection from your Dynamo deployment\n",
    "- Create and view the Dynamo dashboard in Grafana\n",
    "- Explore metrics in Prometheus\n",
    "- Understand key performance metrics\n",
    "\n",
    "**Prerequisites**: Complete Lab 1 (Introduction and Kubernetes-Based Deployment)\n",
    "\n",
    "**Note**: Prometheus and Grafana were installed cluster-wide during the initial setup. You'll verify they're running and configure them to monitor your Dynamo deployment.\n",
    "\n",
    "## Duration: ~20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Verify Cluster Monitoring Stack\n",
    "\n",
    "### Objectives\n",
    "- Verify cluster-wide Grafana and Prometheus are running\n",
    "- Get access information for Grafana dashboard\n",
    "- Understand how cluster-wide monitoring works\n",
    "\n",
    "### Important: Cluster-Wide Monitoring\n",
    "\n",
    "The Kubernetes cluster has a **cluster-wide monitoring stack** already deployed during initial setup:\n",
    "- Prometheus collects metrics from all namespaces\n",
    "- Grafana provides visualization dashboards\n",
    "- Services are exposed via NodePort for easy access\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "Cluster (monitoring namespace):\n",
    "  â”œâ”€â”€ Prometheus (cluster-wide metrics collection)\n",
    "  â”œâ”€â”€ Grafana (cluster-wide dashboards)\n",
    "  â””â”€â”€ Prometheus Operator (manages monitoring resources)\n",
    "\n",
    "Your Namespace (dynamo):\n",
    "  â”œâ”€â”€ Dynamo Deployment (Frontend + Workers)\n",
    "  â””â”€â”€ PodMonitors (tell Prometheus what to scrape)\n",
    "```\n",
    "\n",
    "### Step 1: Set Environment Variables\n",
    "\n",
    "Set up the environment variables (same as Lab 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa65d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables (use defaults if not already set)\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.7.1}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Get Grafana URL (extract ID from hostname: brev-xxxxx -> grafana0-xxxxx)\n",
    "BREV_ID=$(hostname | cut -d'-' -f2)\n",
    "GRAFANA_URL=\"https://grafana0-${BREV_ID}.brevlab.com/\"\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ðŸ“Š Lab 1 Extension: Monitoring Environment Configuration\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Release Version:  $RELEASE_VERSION\"\n",
    "echo \"  Namespace:        $NAMESPACE\"\n",
    "echo \"  Cache Path:       $CACHE_PATH\"\n",
    "echo \"  Node IP:          $NODE_IP\"\n",
    "echo \"\"\n",
    "echo \"ðŸ“Œ Service URLs:\"\n",
    "echo \"  Frontend API:     http://$NODE_IP:30100\"\n",
    "echo \"  Grafana:          $GRAFANA_URL\"\n",
    "echo \"\"\n",
    "echo \"ðŸ’¡ Grafana is configured with anonymous access (no login required)\"\n",
    "echo \"\"\n",
    "echo \"âœ“ Environment configured for monitoring\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e9371",
   "metadata": {},
   "source": [
    "### Step 2: Verify Monitoring Stack is Running\n",
    "\n",
    "Check that Prometheus and Grafana pods are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d447b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check monitoring stack pods\n",
    "echo \"Checking cluster monitoring stack...\"\n",
    "echo \"\"\n",
    "kubectl get pods -n monitoring | grep -E \"(NAME|prometheus-|grafana-)\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ If you see Running pods, the monitoring stack is ready\"\n",
    "echo \"\"\n",
    "echo \"ðŸ”— Access Grafana at: $GRAFANA_URL\"\n",
    "echo \"   (Anonymous access enabled - no login required)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61ab6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Configure Metrics Collection\n",
    "\n",
    "### Objectives\n",
    "- Understand PodMonitor resources\n",
    "- Configure automatic metrics discovery\n",
    "- Verify metrics are being scraped by cluster Prometheus\n",
    "\n",
    "### How Dynamo Exposes Metrics\n",
    "\n",
    "Dynamo components expose metrics through:\n",
    "- **Frontend**: Exposes `/metrics` on its HTTP port (8000)\n",
    "  - Request rates, latencies, token metrics\n",
    "- **Workers**: Exposes `/metrics` on system port\n",
    "  - Worker-specific metrics, queue stats\n",
    "\n",
    "**Note**: The cluster-wide Prometheus automatically discovers PodMonitors in all namespaces, so once we create them, metrics will be collected automatically.\n",
    "\n",
    "### Step 1: Verify Dynamo Deployment Has Metrics Labels\n",
    "\n",
    "The Dynamo operator automatically adds metrics labels to pods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check if your Dynamo pods have metrics labels\n",
    "echo \"Checking Dynamo pod labels:\"\n",
    "kubectl get pods -n $NAMESPACE -l nvidia.com/metrics-enabled=true --show-labels\n",
    "\n",
    "echo \"\"\n",
    "echo \"Look for labels: nvidia.com/metrics-enabled=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffefad8e",
   "metadata": {},
   "source": [
    "### Step 2: Check if PodMonitors Were Created\n",
    "\n",
    "The Dynamo operator should automatically create PodMonitor resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d49a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List PodMonitors in your namespace\n",
    "echo \"PodMonitors in namespace $NAMESPACE:\"\n",
    "kubectl get podmonitor -n $NAMESPACE\n",
    "\n",
    "echo \"\"\n",
    "echo \"You should see PodMonitors for frontend and worker components\"\n",
    "echo \"These are automatically discovered by the cluster Prometheus\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e753a",
   "metadata": {},
   "source": [
    "### Step 3: Label PodMonitors for Prometheus Discovery\n",
    "\n",
    "The cluster Prometheus requires PodMonitors to have a specific label. Let's add it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36cb2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Add the required label to Dynamo PodMonitors\n",
    "echo \"Labeling PodMonitors for Prometheus discovery...\"\n",
    "\n",
    "kubectl label podmonitor -n $NAMESPACE dynamo-frontend release=kube-prometheus-stack --overwrite\n",
    "kubectl label podmonitor -n $NAMESPACE dynamo-planner release=kube-prometheus-stack --overwrite\n",
    "kubectl label podmonitor -n $NAMESPACE dynamo-worker release=kube-prometheus-stack --overwrite\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ PodMonitors labeled - Prometheus will now discover and scrape metrics\"\n",
    "echo \"  It may take 1-2 minutes for metrics to appear in Grafana\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be198561",
   "metadata": {},
   "source": [
    "### Step 4: Manually Test Metrics Endpoint\n",
    "\n",
    "Let's verify metrics are accessible:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get the frontend pod name\n",
    "FRONTEND_POD=$(kubectl get pods -n $NAMESPACE | grep frontend | head -1 | awk '{print $1}')\n",
    "\n",
    "if [ -n \"$FRONTEND_POD\" ]; then\n",
    "    echo \"Testing metrics endpoint from frontend pod: $FRONTEND_POD\"\n",
    "    echo \"\"\n",
    "    kubectl exec -n $NAMESPACE $FRONTEND_POD -- curl -s localhost:8000/metrics | head -20\n",
    "    echo \"\"\n",
    "    echo \"âœ“ Metrics endpoint is accessible\"\n",
    "else\n",
    "    echo \"âš ï¸  Frontend pod not found. Make sure your deployment from Lab 1 is running.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e4f23",
   "metadata": {},
   "source": [
    "### Step 5: Send Test Traffic to Generate Metrics\n",
    "\n",
    "Let's generate some traffic to populate metrics by sending requests to the Dynamo frontend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Sending test requests to http://$NODE_IP:30100...\"\n",
    "echo \"\"\n",
    "\n",
    "# Send a few test requests\n",
    "for i in {1..5}; do\n",
    "    echo \"Request $i/5...\"\n",
    "    curl -s http://$NODE_IP:30100/v1/chat/completions \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d '{\n",
    "        \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello! Tell me a short joke.\"}],\n",
    "        \"stream\": false,\n",
    "        \"max_tokens\": 30\n",
    "      }' > /dev/null\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Sent 5 test requests to generate metrics\"\n",
    "echo \"  Metrics should now be visible in Prometheus and Grafana\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d0aaf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Import Dynamo Inference Dashboard\n",
    "\n",
    "### Objectives\n",
    "- Import the Dynamo Inference dashboard to Grafana\n",
    "- Understand what metrics are displayed\n",
    "\n",
    "### Dashboard Overview\n",
    "\n",
    "The cluster's Grafana has a \"Dynamo Operator\" dashboard pre-installed, but it shows **operator metrics** (reconciliation loops, workqueues). For **inference metrics** (request rates, latency, tokens), we need to import a custom dashboard.\n",
    "\n",
    "The Dynamo Inference dashboard provides visibility into:\n",
    "- **Request Metrics**: Request rates, throughput, and counts\n",
    "- **Latency Metrics**: Time to first token (TTFT), inter-token latency\n",
    "- **Performance**: Request duration, inflight requests\n",
    "- **Model Metrics**: Input/output sequence lengths, token counts\n",
    "\n",
    "### Import the Inference Dashboard\n",
    "\n",
    "Deploy the dashboard as a ConfigMap that Grafana will automatically load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create ConfigMap with dashboard JSON\n",
    "echo \"Deploying Dynamo Inference Dashboard to $NAMESPACE...\"\n",
    "\n",
    "cat > /tmp/dynamo-inference-dashboard-configmap.yaml << EOF\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: grafana-dashboard-dynamo-inference\n",
    "  namespace: $NAMESPACE\n",
    "  labels:\n",
    "    grafana_dashboard: \"1\"\n",
    "data:\n",
    "  dynamo-inference.json: |\n",
    "EOF\n",
    "\n",
    "# Add dashboard JSON with proper indentation\n",
    "sed 's/^/    /' ~/dynamo-brev/resources/dynamo-inference-dashboard.json >> /tmp/dynamo-inference-dashboard-configmap.yaml\n",
    "\n",
    "# Apply ConfigMap\n",
    "kubectl apply -f /tmp/dynamo-inference-dashboard-configmap.yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Dashboard ConfigMap deployed to namespace: $NAMESPACE\"\n",
    "echo \"  Cluster-wide Grafana sidecar will auto-discover it within ~30 seconds\"\n",
    "echo \"  Access at: $GRAFANA_URL (look for 'Dynamo Inference Metrics' dashboard)\"\n",
    "echo \"\"\n",
    "echo \"Note: The ConfigMap is created in your namespace ($NAMESPACE), but the\"\n",
    "echo \"      cluster-wide Grafana searches all namespaces for dashboards.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c451e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Access Grafana and View Metrics\n",
    "\n",
    "### Objectives\n",
    "- Access Grafana UI via Brev tunnel\n",
    "- Import the Dynamo dashboard\n",
    "- Query metrics in Prometheus\n",
    "- View Dynamo metrics in Grafana\n",
    "\n",
    "### Step 1: View Dynamo Inference Dashboard\n",
    "\n",
    "Once you've imported the dashboard (from Section 3):\n",
    "\n",
    "1. **Click on \"Dashboards\"** in the left sidebar\n",
    "2. **Search for \"Dynamo Inference\"** or look in the \"General\" folder\n",
    "3. **Open the dashboard**\n",
    "\n",
    "The dashboard displays:\n",
    "- **Request Rate**: Requests per second by model\n",
    "- **Time to First Token (TTFT)**: p50, p95, p99 percentiles\n",
    "- **Inter-Token Latency**: Token generation speed\n",
    "- **Request Duration**: Total time per request\n",
    "- **Token Metrics**: Input/output sequence lengths\n",
    "- **Inflight Requests**: Currently processing requests\n",
    "\n",
    "**Note**: The Grafana also has a \"Dynamo Operator\" dashboard showing operator metrics (reconciliation loops, workqueues), but the inference dashboard shows model serving metrics.\n",
    "\n",
    "### Step 2: Generate Load to See Metrics\n",
    "\n",
    "To see interesting metrics in the dashboard, generate some load using the benchmark script from Lab 1.\n",
    "\n",
    "**Run this in a terminal (not in the notebook):**\n",
    "\n",
    "```\n",
    "cd ~/dynamo-grove-brev/lab1\n",
    "./run-benchmark.sh baseline\n",
    "```\n",
    "\n",
    "Or send a few test requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Send test requests\n",
    "for i in {1..10}; do\n",
    "    echo \"Request $i/10...\"\n",
    "    curl -s http://$NODE_IP:30100/v1/chat/completions \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d '{\n",
    "        \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "        \"stream\": false,\n",
    "        \"max_tokens\": 30\n",
    "      }' > /dev/null\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Sent 10 test requests - check Grafana dashboard for updated metrics!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b36c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Understanding Key Metrics\n",
    "\n",
    "### Frontend Metrics\n",
    "\n",
    "The Dynamo frontend exposes these key metrics:\n",
    "\n",
    "| Metric | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| `dynamo_frontend_requests_total` | Total number of requests | Track request volume |\n",
    "| `dynamo_frontend_time_to_first_token_seconds` | Time until first token appears | User experience, responsiveness |\n",
    "| `dynamo_frontend_inter_token_latency_seconds` | Time between consecutive tokens | Generation speed, smoothness |\n",
    "| `dynamo_frontend_request_duration_seconds` | Total request duration | Overall latency |\n",
    "| `dynamo_frontend_input_tokens_total` | Input tokens processed | Input size distribution |\n",
    "| `dynamo_frontend_output_tokens_total` | Output tokens generated | Output size, throughput |\n",
    "\n",
    "### Worker Metrics\n",
    "\n",
    "Workers expose additional metrics:\n",
    "\n",
    "| Metric | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| `dynamo_worker_queue_size` | Requests waiting in queue | Identify backpressure |\n",
    "| `dynamo_worker_active_requests` | Currently processing requests | Worker utilization |\n",
    "| `dynamo_worker_kv_cache_usage` | KV cache memory usage | Memory optimization |\n",
    "\n",
    "### Exploring Metrics in Prometheus\n",
    "\n",
    "### Exploring Advanced Queries\n",
    "\n",
    "You can run advanced Prometheus queries directly in Grafana's Explore view:\n",
    "\n",
    "1. **Open Grafana** at `$GRAFANA_URL`\n",
    "2. **Click \"Explore\"** in the left sidebar (compass icon)\n",
    "3. **Select \"Prometheus\"** as the data source\n",
    "4. **Enter queries** in the query editor\n",
    "\n",
    "Try these advanced queries:\n",
    "\n",
    "**Total Requests:**\n",
    "```\n",
    "sum(dynamo_frontend_requests_total)\n",
    "```\n",
    "\n",
    "**Average Request Rate (last 5 minutes):**\n",
    "```\n",
    "avg(rate(dynamo_frontend_requests_total[5m]))\n",
    "```\n",
    "\n",
    "**95th Percentile TTFT over time:**\n",
    "```\n",
    "histogram_quantile(0.95, rate(dynamo_frontend_time_to_first_token_seconds_bucket[5m]))\n",
    "```\n",
    "\n",
    "**Tokens per second:**\n",
    "```\n",
    "rate(dynamo_frontend_output_sequence_tokens_sum[5m]) / rate(dynamo_frontend_output_sequence_tokens_count[5m])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Section 6: Exercises and Exploration\n",
    "\n",
    "### Exercise 1: Correlate Load with Latency\n",
    "\n",
    "1. Run different concurrency levels with aiperf\n",
    "2. Observe how TTFT and ITL change in Grafana\n",
    "3. Find the optimal concurrency for your deployment\n",
    "\n",
    "**Run these commands in a terminal (not in the notebook):**\n",
    "\n",
    "```\n",
    "# Test with low concurrency\n",
    "cd ~/dynamo-brev/resources\n",
    "./run-benchmark.sh baseline\n",
    "\n",
    "# Check Grafana - note the TTFT values\n",
    "# Then test with higher concurrency:\n",
    "\n",
    "# Get NODE_IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Test with high concurrency\n",
    "python3 -m aiperf profile \\\n",
    "  --model Qwen/Qwen2.5-1.5B-Instruct \\\n",
    "  --url http://$NODE_IP:30100 \\\n",
    "  --endpoint-type chat \\\n",
    "  --streaming \\\n",
    "  --concurrency 8 \\\n",
    "  --request-count 30\n",
    "\n",
    "# Compare TTFT between low and high concurrency in Grafana\n",
    "```\n",
    "\n",
    "### Exercise 2: Create Custom Prometheus Queries\n",
    "\n",
    "Try creating your own queries:\n",
    "\n",
    "1. **Average TTFT over time:**\n",
    "   ```\n",
    "   avg(rate(dynamo_frontend_time_to_first_token_seconds_sum[1m]))\n",
    "   ```\n",
    "\n",
    "2. **Request success rate:**\n",
    "   ```\n",
    "   rate(dynamo_frontend_requests_total{status=\"success\"}[1m])\n",
    "   ```\n",
    "\n",
    "3. **Tokens per second:**\n",
    "   ```\n",
    "   rate(dynamo_frontend_output_tokens_total[1m])\n",
    "   ```\n",
    "\n",
    "### Exercise 3: Set Up Alerts (Optional)\n",
    "\n",
    "Create a PrometheusRule for high latency alerts. Here's an example configuration:\n",
    "\n",
    "```yaml\n",
    "# Example: high-latency-alert.yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PrometheusRule\n",
    "metadata:\n",
    "  name: dynamo-alerts\n",
    "  namespace: dynamo\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  groups:\n",
    "  - name: dynamo\n",
    "    interval: 30s\n",
    "    rules:\n",
    "    - alert: HighTimeToFirstToken\n",
    "      expr: histogram_quantile(0.95, rate(dynamo_frontend_time_to_first_token_seconds_bucket[5m])) > 1.0\n",
    "      for: 2m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: \"High Time to First Token\"\n",
    "        description: \"95th percentile TTFT is above 1 second\"\n",
    "```\n",
    "\n",
    "To create and apply this alert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create the alert file\n",
    "cat > /tmp/high-latency-alert.yaml << EOF\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PrometheusRule\n",
    "metadata:\n",
    "  name: dynamo-alerts\n",
    "  namespace: $NAMESPACE\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  groups:\n",
    "  - name: dynamo\n",
    "    interval: 30s\n",
    "    rules:\n",
    "    - alert: HighTimeToFirstToken\n",
    "      expr: histogram_quantile(0.95, rate(dynamo_frontend_time_to_first_token_seconds_bucket[5m])) > 1.0\n",
    "      for: 2m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: \"High Time to First Token\"\n",
    "        description: \"95th percentile TTFT is above 1 second\"\n",
    "EOF\n",
    "\n",
    "# Apply the alert\n",
    "kubectl apply -f /tmp/high-latency-alert.yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Alert rule created\"\n",
    "echo \"  View alerts in Grafana: Alerting section\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847612b4",
   "metadata": {},
   "source": [
    "### Exercise 4: Cleanup Your Monitoring Resources\n",
    "\n",
    "When you're done exploring, you can remove the monitoring resources you created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove PodMonitors and Dashboard ConfigMap from your namespace\n",
    "echo \"Cleaning up monitoring resources from namespace: $NAMESPACE...\"\n",
    "\n",
    "kubectl delete podmonitor -n $NAMESPACE --all\n",
    "kubectl delete configmap grafana-dashboard-dynamo-inference -n $NAMESPACE 2>/dev/null || true\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Monitoring resources removed from your namespace\"\n",
    "echo \"\"\n",
    "echo \"Note: The cluster-wide Prometheus and Grafana remain active.\"\n",
    "echo \"      Only your PodMonitors and dashboard ConfigMap were removed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c194ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "- âœ… How to access cluster-wide Prometheus and Grafana\n",
    "- âœ… Understanding Prometheus Operator and PodMonitors\n",
    "- âœ… Configuring automatic metrics collection from Dynamo\n",
    "- âœ… Creating and deploying Grafana dashboards via ConfigMaps\n",
    "- âœ… Key Dynamo performance metrics\n",
    "- âœ… Using Prometheus queries for analysis\n",
    "- âœ… Correlating load with performance metrics\n",
    "\n",
    "### Key Takeaways\n",
    "- **Cluster-wide monitoring** enables shared observability infrastructure\n",
    "- **PodMonitors** automatically discover and scrape Dynamo metrics\n",
    "- **Prometheus** provides powerful query language for metric analysis\n",
    "- **Grafana** offers rich visualizations for real-time monitoring\n",
    "- **Key metrics** like TTFT and ITL are critical for LLM performance\n",
    "- **Dashboard ConfigMaps** with `grafana_dashboard: \"1\"` label are auto-discovered by Grafana sidecar\n",
    "\n",
    "### Next Steps\n",
    "- In **Lab 2**, you'll explore disaggregated serving and monitor the separate prefill/decode workers\n",
    "- Advanced monitoring: Set up alerting rules and long-term metric storage\n",
    "- Integrate with your CI/CD: Automated performance regression testing\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Prometheus Not Scraping Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688050b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check Prometheus targets\n",
    "echo \"Checking if Prometheus is scraping Dynamo pods...\"\n",
    "echo \"\"\n",
    "echo \"You can also view targets in Grafana:\"\n",
    "echo \"  1. Go to $GRAFANA_URL\"\n",
    "echo \"  2. Navigate to Status > Targets (in Prometheus section)\"\n",
    "echo \"\"\n",
    "echo \"Look for Dynamo pods in the targets list\"\n",
    "echo \"If pods are missing, check PodMonitor configuration:\"\n",
    "kubectl get podmonitor -n $NAMESPACE -o yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d61d75",
   "metadata": {},
   "source": [
    "### Grafana Dashboard Not Appearing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab19286",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check if dashboard ConfigMap has correct labels\n",
    "kubectl get configmap -n $NAMESPACE grafana-dashboard-dynamo-inference -o yaml | grep -A 5 labels\n",
    "\n",
    "echo \"\"\n",
    "echo \"The ConfigMap should have label: grafana_dashboard: '1'\"\n",
    "echo \"\"\n",
    "echo \"If the ConfigMap exists but dashboard doesn't appear:\"\n",
    "echo \"  1. Wait 30-60 seconds for Grafana sidecar to scan\"\n",
    "echo \"  2. Check Grafana logs for any import errors\"\n",
    "echo \"  3. Verify cluster-wide Grafana has sidecar.dashboards.enabled=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95988d",
   "metadata": {},
   "source": [
    "### Can't Access Grafana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba96b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check Grafana pod status\n",
    "kubectl get pods -n $NAMESPACE | grep grafana\n",
    "\n",
    "# Check Grafana logs\n",
    "GRAFANA_POD=$(kubectl get pods -n $NAMESPACE | grep grafana | awk '{print $1}')\n",
    "kubectl logs -n $NAMESPACE $GRAFANA_POD --tail=30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf622c3f",
   "metadata": {},
   "source": [
    "### Port Forwards Not Working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Kill all existing port-forwards and restart\n",
    "pkill -f 'kubectl port-forward' || true\n",
    "\n",
    "echo \"âœ“ Killed existing port-forwards\"\n",
    "echo \"\"\n",
    "echo \"Re-run the port-forward commands from Section 4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997df9e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- ðŸ“– [Dynamo Metrics Documentation](../../dynamo/docs/observability/metrics.md)\n",
    "- ðŸ“Š [Prometheus Query Examples](https://prometheus.io/docs/prometheus/latest/querying/examples/)\n",
    "- ðŸŽ¨ [Grafana Dashboard Best Practices](https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/best-practices/)\n",
    "- ðŸ”” [Prometheus Alerting](https://prometheus.io/docs/alerting/latest/overview/)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-kernelspec,-widgets,-language_info"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
