{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af42b89",
   "metadata": {},
   "source": [
    "# Lab 1 Extension: Monitoring Dynamo with Prometheus and Grafana\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this extension to Lab 1, you will:\n",
    "- Access the cluster-wide Grafana and Prometheus installation\n",
    "- Configure metrics collection from your Dynamo deployment\n",
    "- Create and view the Dynamo inference dashboard in Grafana\n",
    "- Add Planner observability dashboard to monitor request routing\n",
    "- Explore unified tracing with OpenTelemetry for debugging\n",
    "- Understand key performance metrics\n",
    "\n",
    "**Prerequisites**: Complete Lab 1 (Introduction and Kubernetes-Based Deployment)\n",
    "\n",
    "**Note**: Prometheus and Grafana were installed cluster-wide during the initial setup. You'll verify they're running and configure them to monitor your Dynamo deployment.\n",
    "\n",
    "## Duration: ~30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Verify Cluster Monitoring Stack\n",
    "\n",
    "### Objectives\n",
    "- Verify cluster-wide Grafana and Prometheus are running\n",
    "- Get access information for Grafana dashboard\n",
    "- Understand how cluster-wide monitoring works\n",
    "\n",
    "### Important: Cluster-Wide Monitoring\n",
    "\n",
    "The Kubernetes cluster has a **cluster-wide monitoring stack** already deployed during initial setup:\n",
    "- Prometheus collects metrics from all namespaces\n",
    "- Grafana provides visualization dashboards\n",
    "- Services are exposed via NodePort for easy access\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "Cluster (monitoring namespace):\n",
    "  â”œâ”€â”€ Prometheus (cluster-wide metrics collection)\n",
    "  â”œâ”€â”€ Grafana (cluster-wide dashboards)\n",
    "  â””â”€â”€ Prometheus Operator (manages monitoring resources)\n",
    "\n",
    "Your Namespace (dynamo):\n",
    "  â”œâ”€â”€ Dynamo Deployment (Frontend + Workers)\n",
    "  â””â”€â”€ PodMonitors (tell Prometheus what to scrape)\n",
    "```\n",
    "\n",
    "### Step 1: Set Environment Variables\n",
    "\n",
    "Set up the environment variables (same as Lab 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d686d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables (use defaults if not already set)\n",
    "export RELEASE_VERSION=${RELEASE_VERSION:-0.8.0}\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export CACHE_PATH=${CACHE_PATH:-/data/huggingface-cache}\n",
    "\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Get Grafana URL (extract ID from hostname: brev-xxxxx -> grafana0-xxxxx)\n",
    "BREV_ID=$(hostname | cut -d'-' -f2)\n",
    "GRAFANA_URL=\"https://grafana0-${BREV_ID}.brevlab.com/\"\n",
    "\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"ðŸ“Š Lab 1 Extension: Monitoring Environment Configuration\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n",
    "echo \"  Release Version:  $RELEASE_VERSION\"\n",
    "echo \"  Namespace:        $NAMESPACE\"\n",
    "echo \"  Cache Path:       $CACHE_PATH\"\n",
    "echo \"  Node IP:          $NODE_IP\"\n",
    "echo \"\"\n",
    "echo \"ðŸ“Œ Service URLs:\"\n",
    "echo \"  Frontend API:     http://$NODE_IP:30100\"\n",
    "echo \"  Grafana:          $GRAFANA_URL\"\n",
    "echo \"\"\n",
    "echo \"ðŸ’¡ Grafana is configured with anonymous access (no login required)\"\n",
    "echo \"\"\n",
    "echo \"âœ“ Environment configured for monitoring\"\n",
    "echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d233fbd8",
   "metadata": {},
   "source": [
    "### Step 2: Verify Lab 1 Deployment is Running\n",
    "\n",
    "**IMPORTANT:** Lab 2 requires the deployment from Lab 1 to be running. Let's verify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f684da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"Checking Lab 1 deployment...\"\n",
    "echo \"\"\n",
    "kubectl get dynamographdeployment -n $NAMESPACE\n",
    "echo \"\"\n",
    "kubectl get pods -n $NAMESPACE | grep vllm\n",
    "echo \"\"\n",
    "\n",
    "# Check if deployment exists\n",
    "if kubectl get dynamographdeployment vllm-disagg-router -n $NAMESPACE &>/dev/null; then\n",
    "    echo \"âœ“ Lab 1 deployment found\"\n",
    "    \n",
    "    # Check if pods are ready\n",
    "    READY_PODS=$(kubectl get pods -n $NAMESPACE | grep vllm | grep \"1/1\" | wc -l)\n",
    "    if [ \"$READY_PODS\" -ge 2 ]; then\n",
    "        echo \"âœ“ Deployment is healthy and ready to monitor\"\n",
    "    else\n",
    "        echo \"âš ï¸  Some pods are not ready yet. Wait for them to reach 1/1 Running status.\"\n",
    "        echo \"   Re-run this cell to check status again.\"\n",
    "    fi\n",
    "else\n",
    "    echo \"âŒ Lab 1 deployment not found!\"\n",
    "    echo \"\"\n",
    "    echo \"Please complete Lab 1 first:\"\n",
    "    echo \"  1. Go back to Lab 1\"\n",
    "    echo \"  2. Complete Section 3: Deploy Distributed Model\"\n",
    "    echo \"  3. Wait for pods to be ready (1/1 Running)\"\n",
    "    echo \"  4. Return to this lab\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f66953",
   "metadata": {},
   "source": [
    "### Step 3: Verify Monitoring Stack is Running\n",
    "\n",
    "Check that Prometheus and Grafana pods are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check monitoring stack pods\n",
    "echo \"Checking cluster monitoring stack...\"\n",
    "echo \"\"\n",
    "kubectl get pods -n monitoring | grep -E \"(NAME|prometheus-|grafana-)\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ If you see Running pods, the monitoring stack is ready\"\n",
    "echo \"\"\n",
    "echo \"ðŸ”— Access Grafana at: $GRAFANA_URL\"\n",
    "echo \"   (Anonymous access enabled - no login required)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee26a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Configure Metrics Collection\n",
    "\n",
    "### Objectives\n",
    "- Understand PodMonitor resources\n",
    "- Configure automatic metrics discovery\n",
    "- Verify metrics are being scraped by cluster Prometheus\n",
    "\n",
    "### How Dynamo Exposes Metrics\n",
    "\n",
    "Dynamo components expose metrics through:\n",
    "- **Frontend**: Exposes `/metrics` on its HTTP port (8000)\n",
    "  - Request rates, latencies, token metrics\n",
    "- **Workers**: Exposes `/metrics` on system port\n",
    "  - Worker-specific metrics, queue stats\n",
    "\n",
    "**Note**: The cluster-wide Prometheus automatically discovers PodMonitors in all namespaces, so once we create them, metrics will be collected automatically.\n",
    "\n",
    "### Step 1: Verify Dynamo Deployment Has Metrics Labels\n",
    "\n",
    "The Dynamo operator automatically adds metrics labels to pods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Check if your Dynamo pods have metrics labels\n",
    "echo \"Checking Dynamo pod labels:\"\n",
    "kubectl get pods -n $NAMESPACE -l \"nvidia.com/metrics-enabled=true\" --show-labels\n",
    "\n",
    "echo \"\"\n",
    "echo \"Look for labels: nvidia.com/metrics-enabled=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e110d",
   "metadata": {},
   "source": [
    "### Step 2: Verify PodMonitors for Prometheus\n",
    "\n",
    "PodMonitors tell Prometheus which pods to scrape for metrics. The Dynamo operator creates them automatically, but they need a label for cluster Prometheus to discover them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "echo \"Checking PodMonitors...\"\n",
    "kubectl get podmonitor -n $NAMESPACE\n",
    "echo \"\"\n",
    "\n",
    "# Check if PodMonitors exist\n",
    "PODMONITOR_COUNT=$(kubectl get podmonitor -n $NAMESPACE 2>/dev/null | grep -c dynamo || echo \"0\")\n",
    "\n",
    "if [ \"$PODMONITOR_COUNT\" -gt 0 ]; then\n",
    "    # Show configuration for one PodMonitor\n",
    "    echo \"PodMonitor configuration (example: dynamo-frontend):\"\n",
    "    kubectl get podmonitor dynamo-frontend -n $NAMESPACE -o jsonpath='{.spec}' | python3 -m json.tool\n",
    "    echo \"\"\n",
    "    \n",
    "    # Ensure they have the required label for Prometheus discovery\n",
    "    echo \"Labeling for Prometheus discovery...\"\n",
    "    kubectl label podmonitor dynamo-frontend -n $NAMESPACE release=kube-prometheus-stack --overwrite 2>/dev/null || true\n",
    "    kubectl label podmonitor dynamo-planner -n $NAMESPACE release=kube-prometheus-stack --overwrite 2>/dev/null || true\n",
    "    kubectl label podmonitor dynamo-worker -n $NAMESPACE release=kube-prometheus-stack --overwrite 2>/dev/null || true\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"âœ“ PodMonitors ready - Prometheus will scrape metrics within 1-2 minutes\"\n",
    "else\n",
    "    echo \"âš ï¸  PodMonitors not found - deployment may still be starting\"\n",
    "    echo \"   Wait 30 seconds and re-run this cell\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675355de",
   "metadata": {},
   "source": [
    "### Step 3: Test Metrics Endpoint \n",
    "\n",
    "Let's verify metrics are accessible:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a97c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "\n",
    "# Get the frontend pod name\n",
    "FRONTEND_POD=$(kubectl get pods -n $NAMESPACE | grep frontend | head -1 | awk '{print $1}')\n",
    "\n",
    "if [ -n \"$FRONTEND_POD\" ]; then\n",
    "    echo \"Testing metrics endpoint from frontend pod: $FRONTEND_POD\"\n",
    "    echo \"\"\n",
    "    kubectl exec -n $NAMESPACE $FRONTEND_POD -- curl -s localhost:8000/metrics | head -20\n",
    "    echo \"\"\n",
    "    echo \"âœ“ Metrics endpoint is accessible\"\n",
    "else\n",
    "    echo \"âš ï¸  Frontend pod not found. Make sure your deployment from Lab 1 is running.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83951936",
   "metadata": {},
   "source": [
    "### Step 4: Send Test Traffic to Generate Metrics\n",
    "\n",
    "Let's generate some traffic to populate metrics by sending requests to the Dynamo frontend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b133d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "echo \"Sending test requests to http://$NODE_IP:30100...\"\n",
    "echo \"\"\n",
    "\n",
    "# Send a few test requests\n",
    "for i in {1..5}; do\n",
    "    echo \"Request $i/5...\"\n",
    "    curl -s http://$NODE_IP:30100/v1/chat/completions \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d '{\n",
    "        \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello! Tell me a short joke.\"}],\n",
    "        \"stream\": false,\n",
    "        \"max_tokens\": 30\n",
    "      }' > /dev/null\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Sent 5 test requests to generate metrics\"\n",
    "echo \"  Metrics should now be visible in Prometheus and Grafana\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1776a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Import Dynamo Inference Dashboard\n",
    "\n",
    "### Objectives\n",
    "- Import the Dynamo Inference dashboard to Grafana\n",
    "- Understand what metrics are displayed\n",
    "\n",
    "### Dashboard Overview\n",
    "\n",
    "The cluster's Grafana has a \"Dynamo Operator\" dashboard pre-installed, but it shows **operator metrics** (reconciliation loops, workqueues). For **inference metrics** (request rates, latency, tokens), we need to import a custom dashboard.\n",
    "\n",
    "The Dynamo Inference dashboard provides visibility into:\n",
    "- **Request Metrics**: Request rates, throughput, and counts\n",
    "- **Latency Metrics**: Time to first token (TTFT), inter-token latency\n",
    "- **Performance**: Request duration, inflight requests\n",
    "- **Model Metrics**: Input/output sequence lengths, token counts\n",
    "\n",
    "### Import the Inference Dashboard\n",
    "\n",
    "Deploy the dashboard as a ConfigMap that Grafana will automatically load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4da0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export NAMESPACE=${NAMESPACE:-dynamo}\n",
    "export GRAFANA_URL=${GRAFANA_URL:-\"http://$(hostname -I | awk '{print $1}'):30080\"}\n",
    "\n",
    "# Create ConfigMap with dashboard JSON in monitoring namespace (where Grafana looks)\n",
    "echo \"Deploying Dynamo Inference Dashboard to monitoring namespace...\"\n",
    "\n",
    "cat > /tmp/dynamo-inference-dashboard-configmap.yaml << EOF\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: grafana-dashboard-dynamo-inference\n",
    "  namespace: monitoring\n",
    "  labels:\n",
    "    grafana_dashboard: \"1\"\n",
    "data:\n",
    "  dynamo-inference.json: |\n",
    "EOF\n",
    "\n",
    "# Add dashboard JSON with proper indentation\n",
    "sed 's/^/    /' resources/dynamo-inference-dashboard.json >> /tmp/dynamo-inference-dashboard-configmap.yaml\n",
    "\n",
    "# Apply ConfigMap\n",
    "kubectl apply -f /tmp/dynamo-inference-dashboard-configmap.yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Dashboard ConfigMap deployed to monitoring namespace\"\n",
    "echo \"  Cluster-wide Grafana sidecar will auto-discover it within ~30 seconds\"\n",
    "echo \"  Access at: $GRAFANA_URL (look for 'Dynamo Inference Metrics' dashboard)\"\n",
    "echo \"\"\n",
    "echo \"Note: The ConfigMap is created in the monitoring namespace where\"\n",
    "echo \"      the cluster-wide Grafana sidecar searches for dashboards.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf67b99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Access Grafana and View Metrics\n",
    "\n",
    "### Objectives\n",
    "- Access Grafana UI via Brev tunnel\n",
    "- Import the Dynamo dashboard\n",
    "- Query metrics in Prometheus\n",
    "- View Dynamo metrics in Grafana\n",
    "\n",
    "### Step 1: View Dynamo Inference Dashboard\n",
    "\n",
    "Once you've imported the dashboard (from Section 3):\n",
    "\n",
    "1. **Click on \"Dashboards\"** in the left sidebar\n",
    "2. **Search for \"Dynamo Inference\"** or look in the \"General\" folder\n",
    "3. **Open the dashboard**\n",
    "\n",
    "The dashboard displays:\n",
    "- **Request Rate**: Requests per second by model\n",
    "- **Time to First Token (TTFT)**: p50, p95, p99 percentiles\n",
    "- **Inter-Token Latency**: Token generation speed\n",
    "- **Request Duration**: Total time per request\n",
    "- **Token Metrics**: Input/output sequence lengths\n",
    "- **Inflight Requests**: Currently processing requests\n",
    "\n",
    "**Note**: The Grafana also has a \"Dynamo Operator\" dashboard showing operator metrics (reconciliation loops, workqueues), but the inference dashboard shows model serving metrics.\n",
    "\n",
    "### Step 2: Generate Load to See Metrics\n",
    "\n",
    "To see interesting metrics in the dashboard, generate some load using the benchmark script from Lab 1.\n",
    "\n",
    "**Run this in a terminal (not in the notebook):**\n",
    "\n",
    "```\n",
    "cd ~/dynamo-grove-brev/lab1\n",
    "./run-benchmark.sh baseline\n",
    "```\n",
    "\n",
    "Or send a few test requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc67a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Get node IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Send test requests\n",
    "for i in {1..10}; do\n",
    "    echo \"Request $i/10...\"\n",
    "    curl -s http://$NODE_IP:30100/v1/chat/completions \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d '{\n",
    "        \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "        \"stream\": false,\n",
    "        \"max_tokens\": 30\n",
    "      }' > /dev/null\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Sent 10 test requests - check Grafana dashboard for updated metrics!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b308fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Unified Tracing with OpenTelemetry (New in v0.8.0)\n",
    "\n",
    "### Objectives\n",
    "- Understand distributed tracing in Dynamo\n",
    "- Enable OpenTelemetry tracing\n",
    "- Visualize end-to-end request flows\n",
    "\n",
    "### What is Unified Tracing?\n",
    "\n",
    "Dynamo v0.8.0 introduces **OpenTelemetry-based distributed tracing** that tracks requests across:\n",
    "- Frontend API layer\n",
    "- Planner routing decisions\n",
    "- Prefill worker execution\n",
    "- KV cache transfers\n",
    "- Decode worker execution\n",
    "\n",
    "This gives you **end-to-end visibility** into where time is spent in complex requests.\n",
    "\n",
    "### Enable Tracing (Optional)\n",
    "\n",
    "**Note:** This requires a tracing backend like Jaeger or Tempo. For this lab, we'll show the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Example: Enable OpenTelemetry tracing in Dynamo deployment\n",
    "# Add these annotations to your DynamoGraphDeployment:\n",
    "\n",
    "cat <<EOF\n",
    "spec:\n",
    "  frontend:\n",
    "    annotations:\n",
    "      opentelemetry.io/enabled: \"true\"\n",
    "      opentelemetry.io/exporter: \"otlp\"\n",
    "      opentelemetry.io/endpoint: \"http://jaeger-collector:4317\"\n",
    "  workers:\n",
    "    annotations:\n",
    "      opentelemetry.io/enabled: \"true\"\n",
    "EOF\n",
    "\n",
    "echo \"Note: Tracing requires a backend like Jaeger or Tempo\"\n",
    "echo \"For production deployments, integrate with your observability stack\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deecf590",
   "metadata": {},
   "source": [
    "### What Tracing Shows You\n",
    "\n",
    "With tracing enabled, you can see:\n",
    "\n",
    "1. **Request Flow Timeline:**\n",
    "   - Frontend receives request: 0ms\n",
    "   - Planner makes routing decision: 2ms\n",
    "   - Prefill worker starts: 5ms\n",
    "   - KV cache transfer: 150ms\n",
    "   - Decode worker generates: 300ms\n",
    "   - Response returned: 800ms\n",
    "\n",
    "2. **Bottleneck Identification:**\n",
    "   - Slow prefill? â†’ Model loading issue\n",
    "   - Slow KV transfer? â†’ Network/NIXL issue\n",
    "   - Slow decode? â†’ Batch size or GPU utilization\n",
    "\n",
    "3. **Cache Effectiveness:**\n",
    "   - Trace shows \"KV cache hit\" span = prompt was cached\n",
    "   - No cache hit = full prefill required\n",
    "\n",
    "**Production Tip:** Combine tracing with metrics for powerful debugging. Use metrics for aggregate patterns, traces for individual request debugging.\n",
    "\n",
    "---\n",
    "\n",
    "## Section 6: Understanding Key Metrics\n",
    "\n",
    "### Frontend Metrics\n",
    "\n",
    "The Dynamo frontend exposes these key metrics:\n",
    "\n",
    "| Metric | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| `dynamo_frontend_requests_total` | Total number of requests | Track request volume |\n",
    "| `dynamo_frontend_time_to_first_token_seconds` | Time until first token appears | User experience, responsiveness |\n",
    "| `dynamo_frontend_inter_token_latency_seconds` | Time between consecutive tokens | Generation speed, smoothness |\n",
    "| `dynamo_frontend_request_duration_seconds` | Total request duration | Overall latency |\n",
    "| `dynamo_frontend_input_tokens_total` | Input tokens processed | Input size distribution |\n",
    "| `dynamo_frontend_output_tokens_total` | Output tokens generated | Output size, throughput |\n",
    "\n",
    "### Worker Metrics\n",
    "\n",
    "Workers expose additional metrics:\n",
    "\n",
    "| Metric | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| `dynamo_worker_queue_size` | Requests waiting in queue | Identify backpressure |\n",
    "| `dynamo_worker_active_requests` | Currently processing requests | Worker utilization |\n",
    "| `dynamo_worker_kv_cache_usage` | KV cache memory usage | Memory optimization |\n",
    "\n",
    "### Exploring Metrics in Prometheus\n",
    "\n",
    "### Exploring Advanced Queries\n",
    "\n",
    "You can run advanced Prometheus queries directly in Grafana's Explore view:\n",
    "\n",
    "1. **Open Grafana** at `$GRAFANA_URL`\n",
    "2. **Click \"Explore\"** in the left sidebar (compass icon)\n",
    "3. **Select \"Prometheus\"** as the data source\n",
    "4. **Enter queries** in the query editor\n",
    "\n",
    "Try these advanced queries:\n",
    "\n",
    "**Total Requests:**\n",
    "```\n",
    "sum(dynamo_frontend_requests_total)\n",
    "```\n",
    "\n",
    "**Average Request Rate (last 5 minutes):**\n",
    "```\n",
    "avg(rate(dynamo_frontend_requests_total[5m]))\n",
    "```\n",
    "\n",
    "**95th Percentile TTFT over time:**\n",
    "```\n",
    "histogram_quantile(0.95, rate(dynamo_frontend_time_to_first_token_seconds_bucket[5m]))\n",
    "```\n",
    "\n",
    "**Tokens per second:**\n",
    "```\n",
    "rate(dynamo_frontend_output_sequence_tokens_sum[5m]) / rate(dynamo_frontend_output_sequence_tokens_count[5m])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Section 7: Exercises and Exploration\n",
    "\n",
    "### Exercise 1: Correlate Load with Latency\n",
    "\n",
    "1. Run different concurrency levels with aiperf\n",
    "2. Observe how TTFT and ITL change in Grafana\n",
    "3. Find the optimal concurrency for your deployment\n",
    "\n",
    "**Run these commands in a terminal (not in the notebook):**\n",
    "\n",
    "```\n",
    "# Test with low concurrency\n",
    "cd ~/dynamo-brev/resources\n",
    "./run-benchmark.sh baseline\n",
    "\n",
    "# Check Grafana - note the TTFT values\n",
    "# Then test with higher concurrency:\n",
    "\n",
    "# Get NODE_IP\n",
    "NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n",
    "\n",
    "# Test with high concurrency\n",
    "python3 -m aiperf profile \\\n",
    "  --model Qwen/Qwen2.5-1.5B-Instruct \\\n",
    "  --url http://$NODE_IP:30100 \\\n",
    "  --endpoint-type chat \\\n",
    "  --streaming \\\n",
    "  --concurrency 8 \\\n",
    "  --request-count 30\n",
    "\n",
    "# Compare TTFT between low and high concurrency in Grafana\n",
    "```\n",
    "\n",
    "### Exercise 2: Create Custom Prometheus Queries\n",
    "\n",
    "Try creating your own queries:\n",
    "\n",
    "1. **Average TTFT over time:**\n",
    "   ```\n",
    "   avg(rate(dynamo_frontend_time_to_first_token_seconds_sum[1m]))\n",
    "   ```\n",
    "\n",
    "2. **Request success rate:**\n",
    "   ```\n",
    "   rate(dynamo_frontend_requests_total{status=\"success\"}[1m])\n",
    "   ```\n",
    "\n",
    "3. **Tokens per second:**\n",
    "   ```\n",
    "   rate(dynamo_frontend_output_tokens_total[1m])\n",
    "   ```\n",
    "\n",
    "### Exercise 3: Set Up Alerts (Optional)\n",
    "\n",
    "Create a PrometheusRule for high latency alerts. Here's an example configuration:\n",
    "\n",
    "```yaml\n",
    "# Example: high-latency-alert.yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PrometheusRule\n",
    "metadata:\n",
    "  name: dynamo-alerts\n",
    "  namespace: dynamo\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  groups:\n",
    "  - name: dynamo\n",
    "    interval: 30s\n",
    "    rules:\n",
    "    - alert: HighTimeToFirstToken\n",
    "      expr: histogram_quantile(0.95, rate(dynamo_frontend_time_to_first_token_seconds_bucket[5m])) > 1.0\n",
    "      for: 2m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: \"High Time to First Token\"\n",
    "        description: \"95th percentile TTFT is above 1 second\"\n",
    "```\n",
    "\n",
    "To create and apply this alert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9129de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create the alert file\n",
    "cat > /tmp/high-latency-alert.yaml << EOF\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PrometheusRule\n",
    "metadata:\n",
    "  name: dynamo-alerts\n",
    "  namespace: $NAMESPACE\n",
    "  labels:\n",
    "    release: kube-prometheus-stack\n",
    "spec:\n",
    "  groups:\n",
    "  - name: dynamo\n",
    "    interval: 30s\n",
    "    rules:\n",
    "    - alert: HighTimeToFirstToken\n",
    "      expr: histogram_quantile(0.95, rate(dynamo_frontend_time_to_first_token_seconds_bucket[5m])) > 1.0\n",
    "      for: 2m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: \"High Time to First Token\"\n",
    "        description: \"95th percentile TTFT is above 1 second\"\n",
    "EOF\n",
    "\n",
    "# Apply the alert\n",
    "kubectl apply -f /tmp/high-latency-alert.yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Alert rule created\"\n",
    "echo \"  View alerts in Grafana: Alerting section\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc8ada2",
   "metadata": {},
   "source": [
    "### Exercise 4: Cleanup Your Monitoring Resources\n",
    "\n",
    "When you're done exploring, you can remove the monitoring resources you created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d73f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove PodMonitors and Dashboard ConfigMap from your namespace\n",
    "echo \"Cleaning up monitoring resources from namespace: $NAMESPACE...\"\n",
    "\n",
    "kubectl delete podmonitor -n $NAMESPACE --all\n",
    "kubectl delete configmap grafana-dashboard-dynamo-inference -n $NAMESPACE 2>/dev/null || true\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Monitoring resources removed from your namespace\"\n",
    "echo \"\"\n",
    "echo \"Note: The cluster-wide Prometheus and Grafana remain active.\"\n",
    "echo \"      Only your PodMonitors and dashboard ConfigMap were removed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865afaa9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Known Issues\n",
    "\n",
    "For known issues related to Dynamo v0.8.0 observability features, see the [Known Issues section in the main repository](https://github.com/ai-dynamo/dynamo/blob/main/KNOWN_ISSUES.md).\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "- âœ… How to access cluster-wide Prometheus and Grafana\n",
    "- âœ… Understanding Prometheus Operator and PodMonitors\n",
    "- âœ… Configuring automatic metrics collection from Dynamo\n",
    "- âœ… Creating and deploying Grafana dashboards via ConfigMaps\n",
    "- âœ… Key Dynamo performance metrics\n",
    "- âœ… Using Prometheus queries for analysis\n",
    "- âœ… Correlating load with performance metrics\n",
    "\n",
    "### Key Takeaways\n",
    "- **Cluster-wide monitoring** enables shared observability infrastructure\n",
    "- **PodMonitors** automatically discover and scrape Dynamo metrics\n",
    "- **Prometheus** provides powerful query language for metric analysis\n",
    "- **Grafana** offers rich visualizations for real-time monitoring\n",
    "- **Key metrics** like TTFT and ITL are critical for LLM performance\n",
    "- **Dashboard ConfigMaps** with `grafana_dashboard: \"1\"` label are auto-discovered by Grafana sidecar\n",
    "\n",
    "### Next Steps\n",
    "- In **Lab 2**, you'll explore disaggregated serving and monitor the separate prefill/decode workers\n",
    "- Advanced monitoring: Set up alerting rules and long-term metric storage\n",
    "- Integrate with your CI/CD: Automated performance regression testing\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Prometheus Not Scraping Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check Prometheus targets\n",
    "echo \"Checking if Prometheus is scraping Dynamo pods...\"\n",
    "echo \"\"\n",
    "echo \"You can also view targets in Grafana:\"\n",
    "echo \"  1. Go to $GRAFANA_URL\"\n",
    "echo \"  2. Navigate to Status > Targets (in Prometheus section)\"\n",
    "echo \"\"\n",
    "echo \"Look for Dynamo pods in the targets list\"\n",
    "echo \"If pods are missing, check PodMonitor configuration:\"\n",
    "kubectl get podmonitor -n $NAMESPACE -o yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f063ae",
   "metadata": {},
   "source": [
    "### Grafana Dashboard Not Appearing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a52a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check if dashboard ConfigMap has correct labels\n",
    "kubectl get configmap -n $NAMESPACE grafana-dashboard-dynamo-inference -o yaml | grep -A 5 labels\n",
    "\n",
    "echo \"\"\n",
    "echo \"The ConfigMap should have label: grafana_dashboard: '1'\"\n",
    "echo \"\"\n",
    "echo \"If the ConfigMap exists but dashboard doesn't appear:\"\n",
    "echo \"  1. Wait 30-60 seconds for Grafana sidecar to scan\"\n",
    "echo \"  2. Check Grafana logs for any import errors\"\n",
    "echo \"  3. Verify cluster-wide Grafana has sidecar.dashboards.enabled=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eed34f",
   "metadata": {},
   "source": [
    "### Can't Access Grafana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d65ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check Grafana pod status\n",
    "kubectl get pods -n $NAMESPACE | grep grafana\n",
    "\n",
    "# Check Grafana logs\n",
    "GRAFANA_POD=$(kubectl get pods -n $NAMESPACE | grep grafana | awk '{print $1}')\n",
    "kubectl logs -n $NAMESPACE $GRAFANA_POD --tail=30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03836d16",
   "metadata": {},
   "source": [
    "### Port Forwards Not Working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aca1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Kill all existing port-forwards and restart\n",
    "pkill -f 'kubectl port-forward' || true\n",
    "\n",
    "echo \"âœ“ Killed existing port-forwards\"\n",
    "echo \"\"\n",
    "echo \"Re-run the port-forward commands from Section 4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d3e5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- ðŸ“– [Dynamo Metrics Documentation](../../dynamo/docs/observability/metrics.md)\n",
    "- ðŸ“Š [Prometheus Query Examples](https://prometheus.io/docs/prometheus/latest/querying/examples/)\n",
    "- ðŸŽ¨ [Grafana Dashboard Best Practices](https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/best-practices/)\n",
    "- ðŸ”” [Prometheus Alerting](https://prometheus.io/docs/alerting/latest/overview/)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "jupytext,-kernelspec,-widgets,-language_info"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
